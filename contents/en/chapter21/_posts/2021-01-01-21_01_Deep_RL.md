---
layout: post
title: 21-01 Deep Reinforcement Learning
chapter: '21'
order: 2
owner: Deep Learning Course
lang: en
categories:
- chapter21
lesson_type: required
---

# Deep Reinforcement Learning: Neural Networks Meet Sequential Decision Making

## 1. Concept Overview

Deep Reinforcement Learning combines the representation learning power of deep neural networks with the sequential decision-making framework of reinforcement learning, enabling agents to learn complex behaviors directly from high-dimensional sensory inputs like images or raw sensor data. While classical RL required hand-crafted feature representations and could only handle low-dimensional state spaces, deep RL learns both the representation and the policy end-to-end, scaling to problems previously intractable: playing video games from pixels, controlling robots from camera inputs, mastering chess and Go at superhuman levels, and learning complex manipulation skills through trial and error.

The key insight making deep RL work is using neural networks as function approximators for value functions or policies. Instead of maintaining explicit tables $$Q(s,a)$$ for every state-action pair (impossible for high-dimensional states like 84×84×4 Atari frames with $$256^{84 \times 84 \times 4} \approx 10^{67000}$$ possible states), we approximate $$Q(s,a) \approx Q(s,a;\theta)$$ with a neural network parameterized by $$\theta$$. The network learns to generalize across similar states—having learned to recognize enemies in one game location, it applies this knowledge to other locations—enabling learning from feasible amounts of experience rather than requiring exhaustive exploration.

However, combining deep learning with RL introduces unique challenges absent in either field individually. RL generates its own training data through interaction, creating correlations between sequential samples that violate the i.i.d. assumption underlying standard neural network training. The target values in Q-learning are non-stationary—they depend on the current Q-network which is constantly updating, creating a moving target that can cause instability. The exploration-exploitation tradeoff becomes more critical when actions affect what data the agent sees, potentially causing the agent to get stuck in sub-optimal behaviors that prevent discovering better alternatives.

Deep Q-Networks (DQN), introduced by DeepMind in 2013, solved these challenges through two key innovations: experience replay and target networks. Experience replay stores past experiences in a buffer and samples uniformly for training, breaking temporal correlations and allowing reuse of rare experiences. Target networks provide stable targets by updating slowly, preventing the oscillations that occur when chasing a constantly moving target. These techniques, combined with careful architecture design and training procedures, enabled learning to play Atari games directly from pixels at human or superhuman levels—a watershed moment demonstrating deep learning could tackle sequential decision-making at scale.

The success of deep RL extends far beyond games. AlphaGo combined deep neural networks with Monte Carlo tree search to defeat world champions at Go, a game previously thought to require decades more progress. Robotic systems learn manipulation skills through deep RL, discovering strategies humans never explicitly programmed. Language models are fine-tuned through reinforcement learning from human feedback (RLHF), aligning them with human preferences for helpfulness and safety. Autonomous vehicles learn driving policies through simulation and real-world interaction. Understanding deep RL opens this vast application space while providing insights into how systems can learn complex behaviors through interaction rather than passive observation.

## 2. Mathematical Foundation

Deep Q-Networks extend Q-learning to continuous state spaces through neural function approximation. The Q-function is approximated as $$Q(s,a;\theta)$$ where $$\theta$$ are neural network parameters. Given transition $$(s_t, a_t, r_t, s_{t+1})$$, standard Q-learning updates:

$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha[r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)]$$

With neural networks, this becomes a gradient descent update minimizing squared TD error:

$$\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}}\left[\left(r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s,a;\theta)\right)^2\right]$$

where $$\mathcal{D}$$ is experience replay buffer and $$\theta^-$$ are target network parameters (updated periodically from $$\theta$$).

The gradient with respect to $$\theta$$ is:

$$\nabla_\theta \mathcal{L}(\theta) = \mathbb{E}\left[\left(r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s,a;\theta)\right) \nabla_\theta Q(s,a;\theta)\right]$$

Notice the target $$r + \gamma \max_{a'} Q(s', a'; \theta^-)$$ is treated as constant (no gradient through $$\theta^-$$), preventing unstable feedback loops that would occur if targets depended on parameters being optimized.

### Experience Replay

The replay buffer $$\mathcal{D}$$ stores recent transitions: $$\mathcal{D} = \{(s_i, a_i, r_i, s_i')\}_{i=1}^N$$ with capacity $$N$$ (typically 100K-1M). Each training iteration:
1. Agent takes action, observes transition, adds to $$\mathcal{D}$$
2. Sample mini-batch uniformly from $$\mathcal{D}$$
3. Compute loss and gradient on mini-batch
4. Update $$\theta$$ via gradient descent

Uniform sampling breaks temporal correlations (adjacent experiences are correlated, but random samples from buffer aren't), stabilizing training. Reusing experiences improves sample efficiency—each experience can be used in multiple updates rather than once.

### Target Network

Maintain two networks: online network $$Q(s,a;\theta)$$ updated every step, and target network $$Q(s,a;\theta^-)$$ updated periodically (every $$C$$ steps):

$$\theta^- \leftarrow \theta \quad \text{every } C \text{ steps}$$

This provides stable targets for $$C$$ steps, preventing the moving target problem where both the prediction and target change simultaneously, causing oscillations. Typical $$C = 10,000$$ steps.

### Policy Gradient Methods

An alternative to value-based RL directly parameterizes the policy $$\pi(a|s;\theta)$$. The objective is expected return:

$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^T \gamma^t r_t\right]$$

The policy gradient theorem gives:

$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t\right]$$

where $$G_t = \sum_{k=t}^T \gamma^{k-t} r_k$$ is return from time $$t$$. This remarkable result says we can estimate the gradient by sampling trajectories, computing returns, and weighting log-probabilities of actions by returns. High-return actions get positive weight (increase probability), low-return actions get negative weight (decrease probability).

Actor-Critic methods combine value and policy learning. The actor (policy $$\pi(a|s;\theta)$$) selects actions, the critic (value function $$V(s;\phi)$$) evaluates them. The actor updates using policy gradient with advantage:

$$\nabla_\theta J(\theta) \propto \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot A(s_t, a_t)$$

where $$A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$$ is advantage (how much better than average this action is). The critic updates to better estimate values, providing better advantage estimates for the actor.

## 3. Example / Intuition

Imagine teaching an agent to play Pong from pixel inputs. The state is an 84×84 grayscale image showing paddles and ball. Actions are {up, down, stay}. The agent must learn to move the paddle to hit the ball—a simple task for humans but challenging for RL.

Initially, the agent takes random actions. Occasionally it hits the ball by chance, receiving +1 reward. The DQN updates Q-values for states/actions leading to this reward. For the state "ball approaching my paddle from upper-left," action "move up" gets higher Q-value. Through thousands of games, patterns emerge:

- Ball high, paddle low → move up has high Q-value
- Ball low, paddle high → move down has high Q-value
- Ball middle, paddle middle → stay has high Q-value

The neural network learns these patterns not through explicit rules but by associating visual patterns (ball position relative to paddle) with action values. Crucially, it learns the dynamics: where the ball will be, not just where it is now, enabling anticipatory control.

The experience replay buffer contains diverse situations: paddle at top with ball approaching, paddle at bottom with ball far away, etc. Uniformly sampling this buffer prevents the network from forgetting how to handle situations it hasn't seen recently—a problem called catastrophic forgetting that would occur if training only on the current game state.

## 4. Code Snippet

Complete DQN implementation:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque
import random

class DQN(nn.Module):
    """
    Deep Q-Network for Atari-style games.
    
    Architecture: Conv layers extract features from frames,
    fully connected layers output Q-values for each action.
    """
    
    def __init__(self, input_shape, num_actions):
        super(DQN, self).__init__()
        
        # Input: (batch, 4, 84, 84) - 4 stacked frames
        self.conv = nn.Sequential(
            nn.Conv2d(4, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )
        
        # Calculate conv output size
        conv_output_size = 64 * 7 * 7
        
        # Fully connected layers
        self.fc = nn.Sequential(
            nn.Linear(conv_output_size, 512),
            nn.ReLU(),
            nn.Linear(512, num_actions)  # Q-value per action
        )
    
    def forward(self, x):
        """x: (batch, 4, 84, 84) stacked frames"""
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        return self.fc(x)

class ReplayBuffer:
    """Experience replay buffer"""
    
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = zip(*batch)
        return (np.array(state), np.array(action), np.array(reward),
                np.array(next_state), np.array(done))
    
    def __len__(self):
        return len(self.buffer)

class DQNAgent:
    """Complete DQN agent with experience replay and target network"""
    
    def __init__(self, state_shape, num_actions):
        self.num_actions = num_actions
        
        # Online and target networks
        self.policy_net = DQN(state_shape, num_actions)
        self.target_net = DQN(state_shape, num_actions)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.00025)
        self.memory = ReplayBuffer(capacity=100000)
        
        self.batch_size = 32
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_min = 0.1
        self.epsilon_decay = 0.995
        self.target_update_freq = 1000
        self.steps = 0
    
    def select_action(self, state):
        """ε-greedy action selection"""
        if random.random() < self.epsilon:
            return random.randrange(self.num_actions)
        else:
            with torch.no_grad():
                state_t = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.policy_net(state_t)
                return q_values.argmax().item()
    
    def train_step(self):
        """Single training step on mini-batch from replay buffer"""
        if len(self.memory) < self.batch_size:
            return
        
        # Sample mini-batch
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        # Convert to tensors
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones)
        
        # Compute current Q-values
        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()
        
        # Compute target Q-values using target network
        with torch.no_grad():
            next_q = self.target_net(next_states).max(1)[0]
            target_q = rewards + (1 - dones) * self.gamma * next_q
        
        # Compute loss
        loss = F.mse_loss(current_q, target_q)
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)
        self.optimizer.step()
        
        # Update target network periodically
        self.steps += 1
        if self.steps % self.target_update_freq == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())
        
        # Decay epsilon
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        return loss.item()

print("="*70)
print("Deep Q-Network (DQN) Agent")
print("="*70)

# Demonstrate DQN components
state_shape = (4, 84, 84)  # 4 stacked frames
num_actions = 4

agent = DQNAgent(state_shape, num_actions)

print(f"DQN Components:")
print(f"  Policy network: {sum(p.numel() for p in agent.policy_net.parameters()):,} params")
print(f"  Target network: Same architecture, updated every {agent.target_update_freq} steps")
print(f"  Replay buffer: Capacity {agent.memory.buffer.maxlen:,}")
print(f"  Batch size: {agent.batch_size}")
print(f"  Discount γ: {agent.gamma}")
print(f"  Initial ε: {agent.epsilon} (decays to {agent.epsilon_min})")

# Simulate training loop structure
print("\nDQN Training Loop:")
print("  1. Select action using ε-greedy")
print("  2. Execute in environment, observe reward and next state")
print("  3. Store transition in replay buffer")
print("  4. Sample mini-batch from buffer")
print("  5. Compute Q-learning loss with target network")
print("  6. Update policy network via gradient descent")
print("  7. Periodically copy policy → target network")
print("\nThis combination of techniques enables stable deep RL training!")
```

## 5. Related Concepts

Deep RL connects to supervised learning through imitation learning. Instead of learning from rewards, agents can learn from expert demonstrations—supervised learning on (state, action) pairs. This often provides better initialization than random policies, with subsequent RL fine-tuning adapting to the specific environment.

The relationship to evolutionary strategies shows alternative optimization approaches. Instead of gradient-based policy improvement, evolve population of policies through selection and mutation. Deep neuroevolution has achieved competitive results, particularly for tasks where gradients are unreliable.

## 6. Fundamental Papers

**["Playing Atari with Deep Reinforcement Learning" (2013)](https://arxiv.org/abs/1312.5602)**  
*Authors*: Volodymyr Mnih et al. (DeepMind)  
DQN paper showed deep networks could learn control policies from pixels, combining Q-learning with deep CNN function approximation, experience replay, and target networks. Achieved human-level performance on multiple Atari games, launching deep RL revolution.

**["Human-level control through deep reinforcement learning" (2015)](https://www.nature.com/articles/nature14236)**  
*Authors*: Volodymyr Mnih et al. (DeepMind)  
Nature publication demonstrating DQN achieving human-level performance across 49 Atari games with single architecture and hyperparameters. Established deep RL as viable approach for complex control from high-dimensional inputs.

**["Asynchronous Methods for Deep Reinforcement Learning" (2016)](https://arxiv.org/abs/1602.01783)**  
*Authors*: Volodymyr Mnih et al.  
A3C parallelized RL across multiple agents, removing need for experience replay through asynchronous updates. Achieved better performance with less training time, establishing actor-critic methods as alternative to value-based approaches.

**["Proximal Policy Optimization Algorithms" (2017)](https://arxiv.org/abs/1707.06347)**  
*Authors*: John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov  
PPO simplified policy gradient methods while maintaining performance through clipped surrogate objective preventing excessively large policy updates. Became de facto standard for policy gradient RL due to simplicity and robustness.

**["Mastering the game of Go without human knowledge" (2017)](https://www.nature.com/articles/nature24270)**  
*Authors*: David Silver et al. (DeepMind)  
AlphaGo Zero achieved superhuman Go performance through pure self-play without human data, using deep RL with MCTS. Demonstrated RL's potential for discovering novel strategies surpassing human knowledge.

## Common Pitfalls and Tricks

Reward shaping can help or hurt. Providing intermediate rewards for subgoals speeds learning but might create unintended behaviors (agent learns to maximize shaped rewards instead of true objective). Use carefully and validate final behavior on true rewards.

Hyperparameter sensitivity in deep RL exceeds supervised learning. Learning rate, replay buffer size, update frequency, exploration schedule—all significantly affect performance. Extensive tuning often necessary. Start with published hyperparameters for similar tasks.

## Key Takeaways

Deep reinforcement learning combines neural networks with RL, using networks as function approximators for value functions or policies, enabling learning from high-dimensional inputs like images. DQN uses experience replay to break temporal correlations and target networks for stable Q-learning targets, achieving human-level Atari game performance. Policy gradient methods directly optimize policy networks through REINFORCE or actor-critic approaches, often more stable than value methods for continuous actions. Deep RL has achieved remarkable successes including mastering Go, robotic control, and game playing, while remaining challenging due to sample inefficiency, hyperparameter sensitivity, and training instability. Understanding deep RL requires appreciating both RL foundations (MDPs, value functions, exploration) and deep learning techniques (CNNs for vision, gradient descent, architecture design), combining both fields' insights to create agents that learn complex behaviors through interaction.

