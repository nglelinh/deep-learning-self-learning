<!DOCTYPE html>
<html lang="en">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      18-01 Word Embeddings and Language Representation &middot; Deep learning in Data Science
    
  </title>

  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/poole.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/syntax.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/lanyon.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/github-markdown.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/multilang.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/search.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/content-boxes.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap">
  
  <!-- Lunr.js for search functionality -->
  <script src="https://unpkg.com/lunr/lunr.js"></script>

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="http://localhost:4000/deep-learning-self-learning/public/logo.png">
  <link rel="shortcut icon" href="http://localhost:4000/deep-learning-self-learning/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/deep-learning-self-learning/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Deep learning in Data Science</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/">Home</a>

    

    
    
    
    <!-- Hiển thị các chương có sẵn cho ngôn ngữ hiện tại -->
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter00/">
              00. Basic Mathematical Concepts
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter01/">
              01. Introduction to Deep Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter02/">
              02. Neural Networks Fundamentals
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter03/">
              03. Training Neural Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter04/">
              04. Convolutional Neural Networks (CNNs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter05/">
              05. Recurrent Neural Networks (RNNs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter06/">
              06. LSTM and GRU Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter07/">
              07. Attention Mechanisms
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter08/">
              08. Transformers
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter09/">
              09. Regularization Techniques
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter10/">
              10. Deep Learning Algorithms
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter11/">
              11. Generative Models
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter12/">
              12. Autoencoders
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter13/">
              13. Variational Autoencoders (VAE)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter14/">
              14. Generative Adversarial Networks (GANs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter15/">
              15. Transfer Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter16/">
              16. Self-Supervised Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter17/">
              17. Computer Vision Applications
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter18/">
              18. Natural Language Processing
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter19/">
              19. Speech and Audio Processing
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter20/">
              20. Reinforcement Learning Basics
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter21/">
              21. Deep Reinforcement Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter22/">
              22. Graph Neural Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter23/">
              23. Efficient Deep Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter24/">
              24. Interpretability and Explainability
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter25/">
              25. Advanced Topics and Future Directions
              
            </a>
          
        
      
    
    
    <!-- Nếu không có nội dung cho ngôn ngữ hiện tại, hiển thị thông báo -->
    
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2025. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/deep-learning-self-learning/" title="Home">Deep learning in Data Science</a>
            <small></small>
          </h3>
          <!-- Header Actions: Language Toggle and GitHub Link -->
          <div class="header-actions">
            <div class="language-toggle">
              <a href="/deep-learning-self-learning/contents/vi/chapter00/" class="language-switch" title="Switch to Vietnamese">Switch to Vietnamese</a>
            </div>
            <a class="github-logo__wrapper" target="_blank" href="https://github.com/nglelinh/deep-learning-self-learning" title="Github">
             <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
            </a>
          </div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">
    18-01 Word Embeddings and Language Representation
    
      
        <span class="lesson-badge required large">Required</span>
      
    
  </h1>
  <h1 id="word-embeddings-representing-language-in-vector-space">Word Embeddings: Representing Language in Vector Space</h1>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Word2vec.png/800px-Word2vec.png" alt="Word2Vec Visualization" />
<em>Hình ảnh: Minh họa Word2Vec embeddings trong không gian vector. Nguồn: Wikimedia Commons</em></p>

<h2 id="1-concept-overview">1. Concept Overview</h2>

<p>Word embeddings represent one of the most fundamental innovations in natural language processing, transforming how we represent and process text in machine learning systems. The core idea is elegantly simple yet profoundly impactful: represent each word as a dense vector of real numbers (typically 100-300 dimensions) such that semantically similar words have similar vectors. This continuous vector representation replaces older sparse representations like one-hot encoding (vectors with thousands of dimensions, all zeros except one) with compact, meaningful embeddings that capture semantic relationships through geometric properties in vector space.</p>

<p>Understanding why embeddings revolutionized NLP requires appreciating the limitations of discrete word representations. Traditional approaches treated words as atomic symbols—”king,” “queen,” and “car” were equally distant from each other, sharing no structure. One-hot encoding represents a 50,000-word vocabulary with 50,000-dimensional vectors that are all orthogonal, providing no notion of similarity. This makes learning difficult because the model cannot generalize from seeing “king lives in palace” to understanding “queen lives in palace”—it must learn facts about “queen” independently despite semantic similarity to “king.”</p>

<p>Word embeddings solve this by learning continuous representations where similar words cluster together. The distance between “king” and “queen” vectors is small (they’re both royalty), while “king” and “car” are distant (semantically unrelated). More remarkably, embedding spaces exhibit analogical relationships through vector arithmetic: the vector from “man” to “woman” is similar to the vector from “king” to “queen,” capturing the gender relationship. We can solve analogies through simple vector math: king - man + woman ≈ queen. This emergent structure wasn’t explicitly programmed but arose from training on text, demonstrating that embeddings capture deep semantic regularities.</p>

<p>The training objective for learning embeddings is elegantly formulated through the distributional hypothesis from linguistics: words appearing in similar contexts have similar meanings. This simple principle enables unsupervised learning from massive text corpora. Models like Word2Vec and GloVe learn embeddings by predicting context words from target words or vice versa, or by factorizing co-occurrence statistics. The resulting vectors encode lexical semantics, syntactic patterns, and even some world knowledge, all discovered purely from word co-occurrence patterns in text without any labeled data.</p>

<p>The impact of word embeddings on NLP cannot be overstated. They provided the foundation for the deep learning revolution in language processing, enabling neural networks to leverage vast unlabeled text for learning representations that then transfer to downstream tasks. Pre-trained embeddings like Word2Vec and GloVe became standard components in virtually all NLP systems from 2013-2018. While modern contextual embeddings from BERT and GPT have largely superseded static word embeddings for many tasks, understanding static embeddings remains crucial for appreciating how representation learning in NLP evolved and for applications where their simplicity and efficiency remain advantages.</p>

<h2 id="2-mathematical-foundation">2. Mathematical Foundation</h2>

<table>
  <tbody>
    <tr>
      <td>Word embeddings map discrete symbols (words) to continuous vectors in a way that captures semantic similarity. Formally, we have a vocabulary \(V\) of size $$</td>
      <td>V</td>
      <td>\(and learn an embedding matrix\)\mathbf{E} \in \mathbb{R}^{d \times</td>
      <td>V</td>
      <td>}\(where column\)\mathbf{e}_w \in \mathbb{R}^d\(is the embedding for word\)w\(. Typical embedding dimension\)d = 100\text{-}300$$, much smaller than vocabulary size (10,000-100,000).</td>
    </tr>
  </tbody>
</table>

<h3 id="word2vec-skip-gram-model">Word2Vec: Skip-gram Model</h3>

<p>The skip-gram model predicts context words given a target word, based on the distributional hypothesis. For a corpus with words \(w_1, w_2, \ldots, w_T\), the objective is:</p>

\[\max_\theta \frac{1}{T}\sum_{t=1}^T \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j} | w_t; \theta)\]

<p>where \(c\) is context window size (typically 5), and \(\theta\) includes the embedding matrix and output weights. The conditional probability uses softmax:</p>

\[p(w_O | w_I) = \frac{\exp(\mathbf{v}_{w_O}^T \mathbf{v}_{w_I})}{\sum_{w=1}^{|V|} \exp(\mathbf{v}_w^T \mathbf{v}_{w_I})}\]

<p>where \(\mathbf{v}_{w_I}\) is the input embedding of word \(w_I\) and \(\mathbf{v}_{w_O}\) is the output embedding of \(w_O\). Computing this softmax requires summing over the entire vocabulary (expensive!), motivating approximations.</p>

<p><strong>Negative sampling</strong> approximates the softmax by sampling a few negative examples instead of summing over all words:</p>

\[\log \sigma(\mathbf{v}_{w_O}^T \mathbf{v}_{w_I}) + \sum_{i=1}^k \mathbb{E}_{w_i \sim P_n(w)}[\log \sigma(-\mathbf{v}_{w_i}^T \mathbf{v}_{w_I})]\]

<p>where \(\sigma\) is sigmoid, \(k\) is number of negative samples (typically 5-20), and \(P_n(w)\) is noise distribution (often unigram raised to 3/4 power to oversample rare words). This transforms the multi-class problem into \(k+1\) binary classifications, tractable even for large vocabularies.</p>

<p>The remarkable property of Word2Vec embeddings is that semantic relationships are encoded as linear translations in vector space:</p>

\[\mathbf{e}_{\text{queen}} \approx \mathbf{e}_{\text{king}} - \mathbf{e}_{\text{man}} + \mathbf{e}_{\text{woman}}\]

\[\mathbf{e}_{\text{Paris}} \approx \mathbf{e}_{\text{France}} - \mathbf{e}_{\text{Germany}} + \mathbf{e}_{\text{Berlin}}\]

<p>These analogies weren’t explicitly trained but emerge from the distributional hypothesis: “king” and “queen” appear in similar contexts (royal, throne, palace), as do “king” and “man” (gendered contexts), creating vector geometry that reflects these semantic patterns.</p>

<h3 id="glove-global-vectors">GloVe: Global Vectors</h3>

<p>GloVe takes a different approach, directly factorizing word co-occurrence statistics. Let \(X_{ij}\) be the number of times word \(j\) appears in word \(i\)’s context. GloVe minimizes:</p>

\[J = \sum_{i,j=1}^{|V|} f(X_{ij})(\mathbf{w}_i^T \tilde{\mathbf{w}}_j + b_i + \tilde{b}_j - \log X_{ij})^2\]

<p>where \(\mathbf{w}_i\) and \(\tilde{\mathbf{w}}_j\) are word and context embeddings, \(b_i, \tilde{b}_j\) are biases, and \(f(X_{ij})\) is a weighting function:</p>

\[f(x) = \begin{cases} (x/x_{\max})^\alpha &amp; \text{if } x &lt; x_{\max} \\ 1 &amp; \text{otherwise} \end{cases}\]

<p>This weights frequent co-occurrences less heavily (they’re already well-represented) and caps influence of very frequent pairs. GloVe combines the benefits of global matrix factorization methods (leveraging entire corpus statistics) with local context window methods (Word2Vec), often producing embeddings competitive or superior to Word2Vec.</p>

<h2 id="3-example--intuition">3. Example / Intuition</h2>

<p>Imagine learning embeddings for a small vocabulary: {cat, dog, car, truck, animal, vehicle}. Initially, vectors are random. As we process text:</p>

<p>“The cat is an animal” → “cat” and “animal” co-occur<br />
“The dog is an animal” → “dog” and “animal” co-occur<br />
“The car is a vehicle” → “car” and “vehicle” co-occur<br />
“The truck is a vehicle” → “truck” and “vehicle” co-occur</p>

<p>The model adjusts vectors so:</p>
<ul>
  <li>“cat” and “dog” become close (both appear with “animal”)</li>
  <li>“car” and “truck” become close (both appear with “vehicle”)</li>
  <li>“cat” and “car” stay distant (appear in different contexts)</li>
</ul>

<p>After seeing enough text, the 2D embedding space might organize as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     animal
        ↑
    dog • cat
        |
    ----+---- 
        |
  truck • car
        ↓
     vehicle
</code></pre></div></div>

<p>Semantic categories (animals vs vehicles) cluster, and within categories, similar items are nearby. We can compute:</p>

<p>“cat” - “animal” ≈ “dog” - “animal” (both point from category to instance)<br />
“car” + “vehicle” ≈ “truck” (category + similarity gives similar item)</p>

<p>This geometric structure enables generalization: if the model learns facts about “cat,” it can transfer to “dog” through their vector similarity.</p>

<h2 id="4-code-snippet">4. Code Snippet</h2>

<p>Complete Word2Vec implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="k">class</span> <span class="nc">Word2VecSkipGram</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Skip-gram Word2Vec with negative sampling.
    
    Learns word embeddings by predicting context words from target words.
    Uses negative sampling to make training efficient.
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># Input and output embeddings
</span>        <span class="c1"># Input: embeddings used when word is target
</span>        <span class="c1"># Output: embeddings used when word is context
</span>        <span class="n">self</span><span class="p">.</span><span class="n">input_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        
        <span class="c1"># Initialize with small random values
</span>        <span class="n">self</span><span class="p">.</span><span class="n">input_embeddings</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">/</span><span class="n">embedding_dim</span><span class="p">,</span> 
                                                   <span class="mf">0.5</span><span class="o">/</span><span class="n">embedding_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_embeddings</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">target_words</span><span class="p">,</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">negative_words</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        target_words: (batch,) target word indices
        context_words: (batch,) context word indices (positive examples)
        negative_words: (batch, k) negative samples
        
        Returns negative log-likelihood
        </span><span class="sh">"""</span>
        <span class="c1"># Get embeddings
</span>        <span class="n">target_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">input_embeddings</span><span class="p">(</span><span class="n">target_words</span><span class="p">)</span>  <span class="c1"># (batch, emb_dim)
</span>        <span class="n">context_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_embeddings</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>  <span class="c1"># (batch, emb_dim)
</span>        <span class="n">neg_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_embeddings</span><span class="p">(</span><span class="n">negative_words</span><span class="p">)</span>  <span class="c1"># (batch, k, emb_dim)
</span>        
        <span class="c1"># Positive scores (target-context similarity)
</span>        <span class="n">pos_scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">target_embeds</span> <span class="o">*</span> <span class="n">context_embeds</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch,)
</span>        <span class="n">pos_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">pos_scores</span><span class="p">)).</span><span class="nf">mean</span><span class="p">()</span>
        
        <span class="c1"># Negative scores (target-negative dissimilarity)
</span>        <span class="n">neg_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">neg_embeds</span><span class="p">,</span> <span class="n">target_embeds</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">()</span>  <span class="c1"># (batch, k)
</span>        <span class="n">neg_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="o">-</span><span class="n">neg_scores</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">pos_loss</span> <span class="o">+</span> <span class="n">neg_loss</span>

<span class="c1"># Prepare training data
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training Word2Vec Embeddings</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>

<span class="c1"># Simple corpus for demonstration
</span><span class="n">corpus</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
the cat sat on the mat .
the dog sat on the rug .
the cat and the dog are animals .
the car is a vehicle .
the truck is a vehicle .
cats and dogs are pets .
cars and trucks are vehicles .
</span><span class="sh">"""</span><span class="p">.</span><span class="nf">lower</span><span class="p">().</span><span class="nf">split</span><span class="p">()</span>

<span class="c1"># Build vocabulary
</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">corpus</span><span class="p">))</span>
<span class="n">word_to_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
<span class="n">idx_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">w</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Corpus: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Vocabulary: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s"> unique words</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sample vocab: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="si">:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Generate training pairs
</span><span class="k">def</span> <span class="nf">generate_training_data</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">word_to_idx</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Generate (target, context) pairs</span><span class="sh">"""</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
        <span class="n">target_idx</span> <span class="o">=</span> <span class="n">word_to_idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        
        <span class="c1"># Get context (words within window)
</span>        <span class="n">context_start</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">)</span>
        <span class="n">context_end</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">context_start</span><span class="p">,</span> <span class="n">context_end</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>  <span class="c1"># Don't pair with self
</span>                <span class="n">context_idx</span> <span class="o">=</span> <span class="n">word_to_idx</span><span class="p">[</span><span class="n">corpus</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span>
                <span class="n">pairs</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">target_idx</span><span class="p">,</span> <span class="n">context_idx</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">pairs</span>

<span class="n">pairs</span> <span class="o">=</span> <span class="nf">generate_training_data</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">word_to_idx</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Generated </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span><span class="si">}</span><span class="s"> training pairs</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sample pairs: </span><span class="si">{</span><span class="n">pairs</span><span class="p">[</span><span class="si">:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Train Word2Vec
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Word2VecSkipGram</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># Small dim for demo
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Training embeddings...</span><span class="sh">"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">target_idx</span><span class="p">,</span> <span class="n">context_idx</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="c1"># Sample negatives
</span>        <span class="n">neg_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        
        <span class="c1"># To tensors
</span>        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">([</span><span class="n">target_idx</span><span class="p">])</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">([</span><span class="n">context_idx</span><span class="p">])</span>
        <span class="n">negatives</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">(</span><span class="n">neg_indices</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Forward
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">negatives</span><span class="p">)</span>
        
        <span class="c1"># Backward
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: Loss = </span><span class="si">{</span><span class="n">epoch_loss</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Analyze learned embeddings
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Analyzing Learned Embeddings</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">input_embeddings</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

<span class="c1"># Find nearest neighbors
</span><span class="k">def</span> <span class="nf">find_nearest</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">word_to_idx</span><span class="p">,</span> <span class="n">idx_to_word</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Find k nearest words to query word</span><span class="sh">"""</span>
    <span class="n">word_idx</span> <span class="o">=</span> <span class="n">word_to_idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="n">word_vec</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">word_idx</span><span class="p">]</span>
    
    <span class="c1"># Compute cosine similarities
</span>    <span class="n">similarities</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">@</span> <span class="n">word_vec</span> <span class="o">/</span> <span class="p">(</span>
        <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">word_vec</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span>
    <span class="p">)</span>
    
    <span class="c1"># Get top k (excluding self)
</span>    <span class="n">top_k</span> <span class="o">=</span> <span class="n">similarities</span><span class="p">.</span><span class="nf">argsort</span><span class="p">()[</span><span class="o">-</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="p">[(</span><span class="n">idx_to_word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">similarities</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_k</span><span class="p">]</span>

<span class="c1"># Test semantic similarity
</span><span class="n">test_words</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">cat</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">dog</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">car</span><span class="sh">'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">test_words</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_to_idx</span><span class="p">:</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="nf">find_nearest</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">word_to_idx</span><span class="p">,</span> <span class="n">idx_to_word</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Nearest to </span><span class="sh">'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="sh">'</span><span class="s">: </span><span class="si">{</span><span class="n">neighbors</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Embeddings learned semantic relationships from co-occurrence patterns!</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Similar words (cat/dog, car/truck) have similar embeddings.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="5-related-concepts">5. Related Concepts</h2>

<p>Word embeddings connect to distributional semantics, the linguistic theory that word meaning is determined by context. The computational implementation—learning vectors such that words in similar contexts have similar representations—directly operationalizes this theory. Understanding this connection helps appreciate why embeddings work: they’re not arbitrary feature engineering but implementations of fundamental linguistic principles.</p>

<p>Embeddings relate to dimensionality reduction techniques like PCA or autoencoders. We’re compressing high-dimensional one-hot vectors (vocab size) to low-dimensional dense vectors (embedding size) while preserving semantic information. The learned compression discovers that semantic relationships can be captured in far fewer dimensions than explicit symbol identity, revealing the intrinsic dimensionality of word semantics is much lower than vocabulary size.</p>

<p>The evolution from static embeddings (Word2Vec, GloVe) to contextual embeddings (ELMo, BERT) reflects increasing sophistication. Static embeddings assign one vector per word type, so “bank” (financial) and “bank” (river) have identical representations despite different meanings. Contextual embeddings produce different vectors based on context, resolving polysemy. This evolution shows the field progressing from learning word-level representations to modeling language’s context-dependent nature.</p>

<h2 id="6-fundamental-papers">6. Fundamental Papers</h2>

<p><strong><a href="https://arxiv.org/abs/1301.3781">“Efficient Estimation of Word Representations in Vector Space” (2013)</a></strong><br />
<em>Authors</em>: Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean<br />
Word2Vec introduced efficient methods (skip-gram and CBOW) for learning word embeddings at scale. The negative sampling training procedure enabled processing billions of words, making embeddings practical for large vocabularies. The paper demonstrated remarkable semantic properties—analogies solved through vector arithmetic—showing embeddings capture sophisticated language patterns. Word2Vec’s simplicity, efficiency, and quality made it widely adopted, establishing embeddings as fundamental to NLP.</p>

<p><strong><a href="https://aclanthology.org/D14-1162/">“GloVe: Global Vectors for Word Representation” (2014)</a></strong><br />
<em>Authors</em>: Jeffrey Pennington, Richard Socher, Christopher Manning<br />
GloVe combined global matrix factorization with local context, factorizing word co-occurrence matrices to learn embeddings. The method achieved competitive or superior performance to Word2Vec while providing intuitive interpretation through co-occurrence statistics. GloVe demonstrated that different training objectives could produce similar high-quality embeddings, suggesting the representation itself matters more than specific training procedure.</p>

<p><strong><a href="https://arxiv.org/abs/1802.05365">“Deep contextualized word representations” (2018)</a></strong><br />
<em>Authors</em>: Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer<br />
ELMo introduced contextual embeddings from deep bidirectional LSTMs, representing each word differently based on sentence context. This addressed static embeddings’ inability to handle polysemy and context-dependent meaning. ELMo showed that deep language models learn different types of information at different layers (syntax in lower, semantics in higher), and combining layers improves downstream tasks. ELMo represented transition from static to contextual representations, paving way for BERT and Transformers.</p>

<h2 id="common-pitfalls-and-tricks">Common Pitfalls and Tricks</h2>

<p>Using pre-trained embeddings without proper vocabulary alignment causes out-of-vocabulary issues. If your task vocabulary contains words not in the pre-trained embeddings, you need strategies: use subword embeddings (BPE, WordPiece), initialize missing words from similar words (if embeddings for “coronavirus” missing, average “virus” and “corona”), or fine-tune embeddings on domain-specific text.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<p>Word embeddings represent words as dense continuous vectors where semantic similarity corresponds to geometric proximity, enabling neural networks to generalize across semantically related words through shared vector representations. Skip-gram Word2Vec predicts context from targets using negative sampling for efficiency, while GloVe factorizes co-occurrence matrices, both learning from unlabeled text through distributional hypothesis. The resulting embeddings exhibit remarkable properties including analogical reasoning through vector arithmetic (king - man + woman ≈ queen) and semantic clustering (synonyms have similar vectors), all emerging from co-occurrence patterns without explicit supervision. Pre-trained embeddings like Word2Vec and GloVe transfer to downstream tasks, providing semantic representations that improve performance across NLP applications from sentiment analysis to machine translation. Modern contextual embeddings from BERT provide context-dependent representations addressing polysemy, though static embeddings remain useful for efficiency and interpretability. Understanding word embeddings provides foundation for all representation learning in NLP, demonstrating how neural networks can discover semantic structure purely from text patterns.</p>


</div>

<!-- Back to Chapter Home Link -->

  
  
  <div style="margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #007bff;">
    <a href="/deep-learning-self-learning/contents/en/chapter18/" style="text-decoration: none; color: #007bff; font-weight: bold;">
      ← Back to Chapter Home
    </a>
  </div>













<div class="related">
  <ul class="related-posts">
    
      
        <li>
          <h2>Previous Post</h2>
          <h3>
            <a href="/deep-learning-self-learning/contents/en/chapter18/18_00_Introduction/">
              18 Natural Language Processing
            </a>
          </h3>
        </li>
      
    
      
    
    
    
  
    
  
  </ul>
</div>



<script src="https://utteranc.es/client.js"
        repo="convex-deep-learning-for-all/convex-deep-learning-for-all.github.io"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/deep-learning-self-learning/public/js/script.js'></script>
    <script src='/deep-learning-self-learning/public/js/multilang.js'></script>
    <script src='/deep-learning-self-learning/public/js/search.js'></script>
  </body>
</html>
