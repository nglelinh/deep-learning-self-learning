<!DOCTYPE html>
<html lang="en">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      02-02 Neural Network Architecture &middot; Deep learning in Data Science
    
  </title>

  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/poole.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/syntax.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/lanyon.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/github-markdown.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/multilang.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/search.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/content-boxes.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap">
  
  <!-- Lunr.js for search functionality -->
  <script src="https://unpkg.com/lunr/lunr.js"></script>

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="http://localhost:4000/deep-learning-self-learning/public/logo.png">
  <link rel="shortcut icon" href="http://localhost:4000/deep-learning-self-learning/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/deep-learning-self-learning/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Deep learning in Data Science</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/">Home</a>

    

    
    
    
    <!-- Hiển thị các chương có sẵn cho ngôn ngữ hiện tại -->
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter00/">
              00. Basic Mathematical Concepts
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter01/">
              01. Introduction to Deep Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter02/">
              02. Neural Networks Fundamentals
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter03/">
              03. Training Neural Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter04/">
              04. Convolutional Neural Networks (CNNs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter05/">
              05. Recurrent Neural Networks (RNNs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter06/">
              06. LSTM and GRU Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter07/">
              07. Attention Mechanisms
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter08/">
              08. Transformers
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter09/">
              09. Regularization Techniques
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter10/">
              10. Deep Learning Algorithms
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter11/">
              11. Generative Models
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter12/">
              12. Autoencoders
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter13/">
              13. Variational Autoencoders (VAE)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter14/">
              14. Generative Adversarial Networks (GANs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter15/">
              15. Transfer Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter16/">
              16. Self-Supervised Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter17/">
              17. Computer Vision Applications
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter18/">
              18. Natural Language Processing
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter19/">
              19. Speech and Audio Processing
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter20/">
              20. Reinforcement Learning Basics
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter21/">
              21. Deep Reinforcement Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter22/">
              22. Graph Neural Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter23/">
              23. Efficient Deep Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter24/">
              24. Interpretability and Explainability
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter25/">
              25. Advanced Topics and Future Directions
              
            </a>
          
        
      
    
    
    <!-- Nếu không có nội dung cho ngôn ngữ hiện tại, hiển thị thông báo -->
    
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2025. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/deep-learning-self-learning/" title="Home">Deep learning in Data Science</a>
            <small></small>
          </h3>
          <!-- Header Actions: Language Toggle and GitHub Link -->
          <div class="header-actions">
            <div class="language-toggle">
              <a href="/deep-learning-self-learning/contents/vi/chapter00/" class="language-switch" title="Switch to Vietnamese">Switch to Vietnamese</a>
            </div>
            <a class="github-logo__wrapper" target="_blank" href="https://github.com/nglelinh/deep-learning-self-learning" title="Github">
             <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
            </a>
          </div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">
    02-02 Neural Network Architecture
    
      
        <span class="lesson-badge required large">Required</span>
      
    
  </h1>
  <h1 id="neural-network-architecture-from-neurons-to-deep-systems">Neural Network Architecture: From Neurons to Deep Systems</h1>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/500px-Colored_neural_network.svg.png" alt="Neural Network Layers" />
<em>Hình ảnh: Kiến trúc neural network với input, hidden và output layers. Nguồn: Wikimedia Commons</em></p>

<h2 id="1-concept-overview">1. Concept Overview</h2>

<p>Neural network architecture is the blueprint that defines how individual neurons are organized, connected, and structured to solve complex problems. While a single neuron can only learn linear decision boundaries (as we saw with the perceptron), the true power of deep learning emerges when we compose many neurons into layers and stack these layers into deep architectures. This compositional structure is not merely a engineering convenience—it reflects a profound insight about how complex intelligence can emerge from simple computational units working in concert.</p>

<p>Understanding architecture is crucial because the way we organize neurons fundamentally determines what a network can learn and how efficiently it learns. A poorly designed architecture might fail to learn even simple patterns, while a well-designed one can discover intricate relationships in data with remarkable efficiency. The architecture embodies our inductive biases—our assumptions about the problem structure—allowing the network to learn more effectively than treating all problems as completely general function approximation tasks.</p>

<h3 id="the-layered-paradigm">The Layered Paradigm</h3>

<p>The fundamental organizing principle of neural networks is <strong>layers</strong>—groups of neurons that perform transformations at the same stage of computation. This layered structure naturally implements compositional computation: each layer transforms its input into a new representation, and subsequent layers build on these representations to create increasingly abstract features. When recognizing a face, early layers might detect edges, middle layers combine edges into facial features (eyes, nose, mouth), and deep layers recognize complete identities.</p>

<p>This hierarchical processing mirrors both biological neural systems and the compositional nature of many real-world concepts. A “car” is composed of wheels, windows, and doors; these components are composed of shapes and textures; these are composed of edges and colors. Neural network layers naturally capture this hierarchy through learned transformations, with each layer learning the appropriate level of abstraction for its position in the processing pipeline.</p>

<h3 id="why-architecture-matters-the-depth-vs-width-tradeoff">Why Architecture Matters: The Depth vs Width Tradeoff</h3>

<p>A critical insight from both theory and practice is that <strong>depth</strong> (number of layers) and <strong>width</strong> (neurons per layer) have fundamentally different effects on network capacity and learning. The Universal Approximation Theorem tells us that a single hidden layer with sufficiently many neurons can approximate any continuous function. Yet in practice, deep networks with relatively few neurons per layer dramatically outperform shallow wide networks on complex tasks.</p>

<p>This isn’t just about parameter efficiency, though deep networks often achieve the same representational power with exponentially fewer parameters than shallow ones. Deep networks learn hierarchical features naturally—you don’t need to tell them to detect edges first, then shapes, then objects; this emerges automatically from the training process. They also exhibit better generalization: the intermediate representations learned by deep networks transfer across tasks, enabling powerful techniques like transfer learning and pre-training that shallow networks don’t support nearly as well.</p>

<p>Understanding the tradeoff between depth and width, and how architecture choices affect training dynamics, generalization, and computational efficiency, is essential for designing effective neural networks. This lesson provides the foundational understanding of how networks are structured, why these structures work, and how to make informed architectural decisions for your own applications.</p>

<h2 id="2-mathematical-foundation">2. Mathematical Foundation</h2>

<h3 id="feedforward-neural-networks-formal-definition">Feedforward Neural Networks: Formal Definition</h3>

<p>A <strong>feedforward neural network</strong> (also called <strong>Multilayer Perceptron</strong> or <strong>MLP</strong>) is a function \(f: \mathbb{R}^{n_0} \to \mathbb{R}^{n_L}\) defined by composition of layer transformations. For a network with \(L\) layers, the function is:</p>

\[f(\mathbf{x}) = f^{[L]} \circ f^{[L-1]} \circ \cdots \circ f^{[1]}(\mathbf{x})\]

<p>where each layer function \(f^{[l]}\) is an affine transformation followed by an element-wise nonlinearity:</p>

\[f^{[l]}(\mathbf{a}^{[l-1]}) = \sigma^{[l]}(\mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]})\]

<p>Let’s carefully unpack each component and understand why this seemingly simple formulation is so powerful.</p>

<h3 id="layer-by-layer-computation">Layer-by-Layer Computation</h3>

<p>For a network with \(L\) layers (not counting the input), layer \(l \in \{1, 2, \ldots, L\}\) computes:</p>

<p><strong>Pre-activation (linear transformation)</strong>:
\(\mathbf{z}^{[l]} = \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}\)</p>

<p><strong>Activation (nonlinear transformation)</strong>:
\(\mathbf{a}^{[l]} = \sigma^{[l]}(\mathbf{z}^{[l]})\)</p>

<p>The dimensions are:</p>
<ul>
  <li>\(\mathbf{a}^{[l-1]} \in \mathbb{R}^{n_{l-1}}\): input to layer \(l\) (output from previous layer)</li>
  <li>\(\mathbf{W}^{[l]} \in \mathbb{R}^{n_l \times n_{l-1}}\): weight matrix</li>
  <li>\(\mathbf{b}^{[l]} \in \mathbb{R}^{n_l}\): bias vector</li>
  <li>\(\mathbf{z}^{[l]} \in \mathbb{R}^{n_l}\): pre-activation values</li>
  <li>\(\mathbf{a}^{[l]} \in \mathbb{R}^{n_l}\): post-activation values (layer output)</li>
  <li>\(n_l\): number of neurons in layer \(l\)</li>
</ul>

<h3 id="the-role-of-each-component">The Role of Each Component</h3>

<p><strong>Weight Matrix \(\mathbf{W}^{[l]}\)</strong>: Each row \(\mathbf{w}_i^{[l]}\) defines one neuron’s linear combination of inputs. The matrix multiplication \(\mathbf{W}^{[l]} \mathbf{a}^{[l-1]}\) computes all neurons’ pre-activations in parallel. The weights are the learnable parameters that adapt during training to capture patterns in data.</p>

<p><strong>Bias Vector \(\mathbf{b}^{[l]}\)</strong>: Shifts the activation function left or right, allowing neurons to activate even when inputs are near zero. Without bias, a ReLU neuron with all-zero inputs would always output zero, limiting expressiveness. Bias is crucial for learning appropriate thresholds.</p>

<p><strong>Activation Function \(\sigma^{[l]}\)</strong>: Introduces nonlinearity, enabling the network to learn non-linear decision boundaries. Without activation functions, stacking layers would be pointless—multiple linear transformations compose into a single linear transformation. Common choices include:</p>
<ul>
  <li>ReLU: \(\sigma(z) = \max(0, z)\)</li>
  <li>Sigmoid: \(\sigma(z) = \frac{1}{1+e^{-z}}\)</li>
  <li>Tanh: \(\sigma(z) = \tanh(z)\)</li>
</ul>

<h3 id="output-layer-design">Output Layer Design</h3>

<p>The output layer’s structure depends fundamentally on the task, as it must produce outputs in the appropriate format for the loss function.</p>

<p><strong>Binary Classification</strong> (\(y \in \{0, 1\}\)):</p>
<ul>
  <li>Single output neuron with sigmoid activation</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Interpretation: $$\hat{y} = P(y=1</td>
          <td>\mathbf{x})$$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Output: \(\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}} \in (0,1)\)</li>
  <li>Loss: Binary cross-entropy \(\mathcal{L} = -[y \log \hat{y} + (1-y) \log(1-\hat{y})]\)</li>
</ul>

<p><strong>Multi-class Classification</strong> (\(y \in \{1,2,\ldots,K\}\)):</p>
<ul>
  <li>\(K\) output neurons with softmax activation</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Interpretation: $$\hat{y}_k = P(y=k</td>
          <td>\mathbf{x})$$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Output: \(\hat{y}_k = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}\) where \(\sum_{k=1}^K \hat{y}_k = 1\)</li>
  <li>Loss: Categorical cross-entropy \(\mathcal{L} = -\sum_{k=1}^K y_k \log \hat{y}_k\)</li>
</ul>

<p>The softmax function has elegant properties: it’s differentiable, outputs form a probability distribution, and it “softens” the argmax operation (hence the name), allowing gradient-based learning.</p>

<p><strong>Regression</strong> (\(y \in \mathbb{R}\)):</p>
<ul>
  <li>One or more output neurons with linear (identity) activation</li>
  <li>Output: \(\hat{y} = z\) (no activation function)</li>
  <li>Loss: Mean Squared Error \(\mathcal{L} = \frac{1}{2}(y - \hat{y})^2\)</li>
</ul>

<h3 id="forward-propagation-the-complete-picture">Forward Propagation: The Complete Picture</h3>

<p>Given input \(\mathbf{x} \in \mathbb{R}^{n_0}\), forward propagation computes:</p>

\[\begin{align}
\mathbf{a}^{[0]} &amp;= \mathbf{x} \quad \text{(initialize with input)} \\
\\
\text{For } l &amp;= 1 \text{ to } L: \\
\mathbf{z}^{[l]} &amp;= \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]} \quad \text{(affine transformation)} \\
\mathbf{a}^{[l]} &amp;= \sigma^{[l]}(\mathbf{z}^{[l]}) \quad \text{(nonlinear activation)} \\
\\
\hat{\mathbf{y}} &amp;= \mathbf{a}^{[L]} \quad \text{(final output)}
\end{align}\]

<p>This sequential computation builds increasingly complex representations. Each layer learns features at a different level of abstraction, with the composition of layers enabling the network to represent highly complex functions.</p>

<h3 id="parameter-count-and-complexity">Parameter Count and Complexity</h3>

<p>The total number of learnable parameters is:</p>

\[\text{Parameters} = \sum_{l=1}^{L} (n_l \times n_{l-1} + n_l) = \sum_{l=1}^{L} n_l(n_{l-1} + 1)\]

<p>For a concrete example with architecture [784, 128, 64, 10]:</p>
<ul>
  <li>Layer 1: \(128 \times 784 + 128 = 100,480\) parameters</li>
  <li>Layer 2: \(64 \times 128 + 64 = 8,256\) parameters</li>
  <li>Layer 3: \(10 \times 64 + 10 = 650\) parameters</li>
  <li><strong>Total</strong>: \(109,386\) parameters</li>
</ul>

<p>This parameter count grows quadratically with layer width but only linearly with depth, explaining why deep narrow networks are often more parameter-efficient than shallow wide ones for similar representational capacity.</p>

<h3 id="the-universal-approximation-theorem">The Universal Approximation Theorem</h3>

<p><strong>Theorem</strong> (Cybenko 1989, Hornik et al. 1989): Let \(\sigma\) be a non-constant, bounded, monotonically-increasing continuous function (e.g., sigmoid). Then for any continuous function \(g\) on a compact subset \(K \subset \mathbb{R}^n\), any \(\epsilon &gt; 0\), and any probability measure \(\mu\) on \(K\), there exists a one-hidden-layer neural network \(f\) such that:</p>

\[\int_K |f(\mathbf{x}) - g(\mathbf{x})| d\mu(\mathbf{x}) &lt; \epsilon\]

<p><strong>What This Means</strong>: Neural networks can approximate any continuous function arbitrarily well. This is remarkable—it means neural networks are universal function approximators, capable of representing any relationship we might want to learn.</p>

<p><strong>Important Caveats</strong>:</p>
<ol>
  <li><strong>Existence ≠ Learnability</strong>: The theorem guarantees a solution exists but doesn’t tell us how to find it via gradient descent</li>
  <li><strong>Width Requirements</strong>: May need exponentially many neurons (in input dimension or precision \(1/\epsilon\))</li>
  <li><strong>Depth Efficiency</strong>: Deeper networks can often achieve the same approximation with exponentially fewer parameters</li>
  <li><strong>No Guidance on Architecture</strong>: Doesn’t tell us what activation functions, initializations, or learning rates to use</li>
</ol>

<p>The theorem explains <em>why</em> neural networks work in principle, but practical deep learning success comes from additional insights about depth, architecture, optimization, and regularization that the theorem doesn’t address.</p>

<h2 id="3-example--intuition">3. Example / Intuition</h2>

<p>To solidify understanding of neural network architecture, let’s trace through a concrete example step by step, watching how information transforms as it flows through layers.</p>

<h3 id="example-3-layer-network-for-mnist">Example: 3-Layer Network for MNIST</h3>

<p>Consider a network designed to classify handwritten digits (28×28 grayscale images into 10 classes):</p>

<p><strong>Architecture</strong>: [784 → 128 → 64 → 10]</p>
<ul>
  <li><strong>Input</strong>: 784 pixels (28×28 flattened)</li>
  <li><strong>Hidden Layer 1</strong>: 128 neurons with ReLU</li>
  <li><strong>Hidden Layer 2</strong>: 64 neurons with ReLU</li>
  <li><strong>Output Layer</strong>: 10 neurons with softmax</li>
</ul>

<h3 id="information-flow-a-detailed-walkthrough">Information Flow: A Detailed Walkthrough</h3>

<p><strong>Step 1: Input (Layer 0)</strong></p>

<p>We receive a 28×28 image of the digit “3”. Flattened into a vector:
\(\mathbf{a}^{[0]} = [0.2, 0.1, 0.0, 0.8, 0.9, \ldots] \in \mathbb{R}^{784}\)</p>

<p>Each value represents a pixel intensity (0=black, 1=white). The network sees this as a point in 784-dimensional space.</p>

<p><strong>Step 2: First Hidden Layer (Layer 1)</strong></p>

<p>This layer has 128 neurons, each looking for different patterns:</p>

\[\mathbf{z}^{[1]} = \mathbf{W}^{[1]} \mathbf{a}^{[0]} + \mathbf{b}^{[1]}\]

<p>Where \(\mathbf{W}^{[1]} \in \mathbb{R}^{128 \times 784}\) and \(\mathbf{b}^{[1]} \in \mathbb{R}^{128}\).</p>

<p>Each of the 128 neurons computes:</p>
<ul>
  <li>Neuron 1 might activate for vertical edges in the top-left</li>
  <li>Neuron 2 might activate for circular curves</li>
  <li>Neuron 3 might activate for diagonal strokes</li>
  <li>… and so on</li>
</ul>

<p>After ReLU activation: \(\mathbf{a}^{[1]} = \max(0, \mathbf{z}^{[1]}) \in \mathbb{R}^{128}\)</p>

<p>Some neurons fire strongly (values close to their maximum), others don’t fire at all (zeroed out by ReLU). The network has transformed the raw pixel representation into a feature representation: “this image has strong vertical edges, moderate curves, weak horizontal strokes.”</p>

<p><strong>Step 3: Second Hidden Layer (Layer 2)</strong></p>

<p>This layer combines the low-level features from Layer 1 into higher-level concepts:</p>

\[\mathbf{z}^{[2]} = \mathbf{W}^{[2]} \mathbf{a}^{[1]} + \mathbf{b}^{[2]}\]

<p>Where \(\mathbf{W}^{[2]} \in \mathbb{R}^{64 \times 128}\).</p>

<p>These 64 neurons might recognize:</p>
<ul>
  <li>Neuron 1: “top loop” (combining curves and top-positioned edges)</li>
  <li>Neuron 2: “bottom loop” (different curve combinations)</li>
  <li>Neuron 3: “vertical stroke” (combining vertical edges)</li>
</ul>

<p>After ReLU: \(\mathbf{a}^{[2]} = \max(0, \mathbf{z}^{[2]}) \in \mathbb{R}^{64}\)</p>

<p>The representation is now even more abstract: “this image has a top loop and a bottom loop, characteristic of digits like 3, 8, or possibly 0.”</p>

<p><strong>Step 4: Output Layer (Layer 3)</strong></p>

<p>The final layer makes the classification decision:</p>

\[\mathbf{z}^{[3]} = \mathbf{W}^{[3]} \mathbf{a}^{[2]} + \mathbf{b}^{[3]} \in \mathbb{R}^{10}\]

<p>where \(\mathbf{W}^{[3]} \in \mathbb{R}^{10 \times 64}\).</p>

<p>This gives raw scores (logits) for each digit. To convert to probabilities:</p>

\[\hat{\mathbf{y}} = \text{softmax}(\mathbf{z}^{[3]})\]

<p>Resulting in something like:
\(\hat{\mathbf{y}} = [0.01, 0.02, 0.05, 0.82, 0.03, 0.01, 0.02, 0.02, 0.01, 0.01]\)</p>

<p>The network is 82% confident this is a “3” (index 3), with some probability mass on other digits that share similar features.</p>

<h3 id="why-this-layered-structure-works">Why This Layered Structure Works</h3>

<p><strong>Hierarchical Feature Learning</strong>: Layer 1 learned edges and curves. Layer 2 combined these into digit parts. Layer 3 combined parts into complete digit predictions. This hierarchy emerges automatically from training—we never explicitly told the network to detect edges first!</p>

<p><strong>Distributed Representation</strong>: The digit “3” isn’t represented by a single neuron but by a pattern of activation across all 64 neurons in Layer 2. This makes the representation robust (losing a few neurons doesn’t destroy the concept) and efficient (the same features help recognize multiple digits).</p>

<p><strong>Dimensionality Reduction</strong>: We started with 784 dimensions and compressed through 128 → 64 → 10. Each reduction forced the network to extract increasingly essential information, discarding noise and irrelevant details while preserving discriminative features.</p>

<h3 id="intuition-why-depth-beats-width">Intuition: Why Depth Beats Width</h3>

<p>Consider two alternative networks for the same task:</p>

<p><strong>Shallow-Wide</strong>: [784 → 4096 → 10]</p>
<ul>
  <li>Single massive hidden layer with 4096 neurons</li>
  <li>Total parameters: ~3.2M</li>
  <li>Each neuron must learn complete pattern from raw pixels</li>
  <li>No explicit feature hierarchy</li>
</ul>

<p><strong>Deep-Narrow</strong>: [784 → 128 → 64 → 32 → 10]</p>
<ul>
  <li>Four smaller hidden layers</li>
  <li>Total parameters: ~110K (29× fewer!)</li>
  <li>Natural feature hierarchy emerges</li>
  <li>Better generalization, easier to train</li>
</ul>

<p>The deep network wins because most real-world patterns are compositional. Faces are composed of eyes, noses, mouths (not random pixel patterns). Sentences are composed of phrases, composed of words, composed of letters. Deep networks naturally capture this compositional structure through their layered architecture.</p>

<h2 id="network-depth-and-width">Network Depth and Width</h2>

<h3 id="width">Width</h3>

<p>The <strong>width</strong> of a layer refers to the number of neurons it contains.</p>

<ul>
  <li><strong>Wider networks</strong>: More neurons per layer
    <ul>
      <li>Greater capacity to learn complex patterns within a single layer</li>
      <li>More parameters (can lead to overfitting)</li>
      <li>More computational cost per layer</li>
    </ul>
  </li>
</ul>

<h3 id="depth">Depth</h3>

<p>The <strong>depth</strong> of a network refers to the number of layers.</p>

<ul>
  <li><strong>Deeper networks</strong>: More layers
    <ul>
      <li>Can learn hierarchical representations</li>
      <li>More expressive (can represent more complex functions)</li>
      <li>Can be harder to train (vanishing/exploding gradients)</li>
      <li>The term “deep learning” comes from using deep networks</li>
    </ul>
  </li>
</ul>

<h3 id="the-universal-approximation-theorem-1">The Universal Approximation Theorem</h3>

<p><strong>Theorem</strong>: A feedforward neural network with:</p>
<ul>
  <li>A single hidden layer</li>
  <li>Finite number of neurons</li>
  <li>Appropriate activation function (e.g., sigmoid, ReLU)</li>
</ul>

<p>can approximate any continuous function on a compact subset of \(\mathbb{R}^n\) to arbitrary accuracy.</p>

<p><strong>Important Notes:</strong></p>
<ul>
  <li>This is an <strong>existence theorem</strong>, not a practical guideline</li>
  <li>It doesn’t specify how many neurons are needed (could be exponentially many)</li>
  <li>Deeper networks can often approximate functions with far fewer parameters</li>
  <li>Deeper networks tend to learn hierarchical features naturally</li>
</ul>

<h2 id="common-design-patterns">Common Design Patterns</h2>

<h3 id="decreasing-width">Decreasing Width</h3>

<p>A common pattern is to gradually decrease the layer width:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input (784) → 512 → 256 → 128 → 64 → Output (10)
</code></pre></div></div>

<p><strong>Rationale</strong>: Progressively compress information into higher-level abstractions.</p>

<h3 id="hourglassbottleneck-architecture">Hourglass/Bottleneck Architecture</h3>

<p>Decrease then increase width:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input (784) → 256 → 64 → 256 → Output (784)
</code></pre></div></div>

<p><strong>Use case</strong>: Autoencoders for dimensionality reduction and reconstruction.</p>

<h3 id="uniform-width">Uniform Width</h3>

<p>Keep all hidden layers the same size:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input (784) → 256 → 256 → 256 → Output (10)
</code></pre></div></div>

<p><strong>Rationale</strong>: Simplicity and easier hyperparameter tuning.</p>

<h2 id="activation-functions-per-layer">Activation Functions Per Layer</h2>

<p>Different layers can use different activation functions:</p>

<p><strong>Typical configuration:</strong></p>
<ul>
  <li><strong>Hidden layers</strong>: ReLU (or variants like Leaky ReLU, ELU)
    <ul>
      <li>Computational efficiency</li>
      <li>Mitigates vanishing gradient</li>
    </ul>
  </li>
  <li><strong>Output layer</strong>: Task-dependent
    <ul>
      <li>Binary classification: Sigmoid</li>
      <li>Multi-class classification: Softmax</li>
      <li>Regression: Linear (identity function)</li>
    </ul>
  </li>
</ul>

<h2 id="fully-connected-vs-other-architectures">Fully Connected vs. Other Architectures</h2>

<h3 id="fully-connected-dense-layers">Fully Connected (Dense) Layers</h3>

<p>Every neuron in layer \(l\) is connected to every neuron in layer \(l-1\).</p>

<p><strong>Advantages:</strong></p>
<ul>
  <li>Maximum flexibility</li>
  <li>Can learn any pattern (given enough neurons)</li>
</ul>

<p><strong>Disadvantages:</strong></p>
<ul>
  <li>Many parameters (\(n^{[l]} \times n^{[l-1]}\))</li>
  <li>No built-in assumption about input structure</li>
  <li>Not efficient for structured data (images, sequences)</li>
</ul>

<h3 id="specialized-architectures">Specialized Architectures</h3>

<p>For specific data types, specialized architectures are more efficient:</p>

<ul>
  <li><strong>Convolutional layers</strong>: For images (spatial structure)</li>
  <li><strong>Recurrent layers</strong>: For sequences (temporal structure)</li>
  <li><strong>Attention mechanisms</strong>: For handling long-range dependencies</li>
</ul>

<p>We’ll cover these in later chapters.</p>

<h2 id="network-representation">Network Representation</h2>

<h3 id="graphical-representation">Graphical Representation</h3>

<p>Networks are often visualized as directed acyclic graphs (DAGs):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Input Layer    Hidden Layer 1   Hidden Layer 2   Output Layer
         (3)             (4)              (4)              (2)
    
    x₁  ○────────────────●──────────────────●──────────────────●  ŷ₁
                        ╱│╲              ╱│╲              ╱│
    x₂  ○──────────────●─●─●────────────●─●─●────────────●─●
                        ╲│╱              ╲│╱              ╲│
    x₃  ○────────────────●──────────────────●──────────────────●  ŷ₂
</code></pre></div></div>

<h3 id="matrix-representation">Matrix Representation</h3>

<p>For computational efficiency, we represent operations as matrix multiplications:</p>

\[\mathbf{Z}^{[l]} = \mathbf{W}^{[l]} \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]}\]

<p>where:</p>
<ul>
  <li>\(\mathbf{A}^{[l-1]}\): activation matrix (each column is one example)</li>
  <li>\(\mathbf{W}^{[l]}\): weight matrix</li>
  <li>\(\mathbf{b}^{[l]}\): bias vector (broadcasted across examples)</li>
</ul>

<h2 id="practical-implementation">Practical Implementation</h2>

<h3 id="example-simple-neural-network-in-python">Example: Simple Neural Network in Python</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">layer_sizes</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        layer_sizes: list of layer sizes including input and output
        Example: [784, 128, 64, 10] for MNIST
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_sizes</span> <span class="o">=</span> <span class="n">layer_sizes</span>
        
        <span class="c1"># Initialize weights and biases
</span>        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="c1"># He initialization for ReLU networks
</span>            <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">biases</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">exp_z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">exp_z</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        x: input of shape (input_size, num_examples)
        Returns: output of shape (output_size, num_examples)
        </span><span class="sh">"""</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="n">zs</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Forward through hidden layers
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">@</span> <span class="n">a</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">biases</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">zs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">activations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        
        <span class="c1"># Output layer (softmax)
</span>        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">@</span> <span class="n">a</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">biases</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">zs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">activations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="n">activations</span><span class="p">,</span> <span class="n">zs</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Returns class predictions</span><span class="sh">"""</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Example usage
</span><span class="n">network</span> <span class="o">=</span> <span class="nc">NeuralNetwork</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># 5 examples
</span><span class="n">output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Output shape: </span><span class="si">{</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># (10, 5)
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Predictions: </span><span class="si">{</span><span class="n">network</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="design-considerations">Design Considerations</h2>

<h3 id="number-of-layers">Number of Layers</h3>

<ul>
  <li><strong>1-2 hidden layers</strong>: Simple problems, small datasets</li>
  <li><strong>3-5 hidden layers</strong>: Moderate complexity</li>
  <li><strong>5+ hidden layers</strong>: Complex problems, large datasets, “deep” learning</li>
</ul>

<h3 id="number-of-neurons-per-layer">Number of Neurons Per Layer</h3>

<p>Rules of thumb:</p>
<ul>
  <li>Start with layers of size between input and output size</li>
  <li>Common sizes: 32, 64, 128, 256, 512</li>
  <li>More neurons = more capacity but more overfitting risk</li>
  <li>Use validation performance to guide choices</li>
</ul>

<h3 id="architecture-search">Architecture Search</h3>

<p>Finding the optimal architecture is often done through:</p>
<ul>
  <li><strong>Manual experimentation</strong>: Try different configurations</li>
  <li><strong>Grid search</strong>: Systematically try combinations</li>
  <li><strong>Random search</strong>: Often more efficient than grid search</li>
  <li><strong>Neural Architecture Search (NAS)</strong>: Automated methods (advanced topic)</li>
</ul>

<h2 id="summary">Summary</h2>

<ul>
  <li><strong>Neural networks</strong> consist of layers of neurons organized into input, hidden, and output layers</li>
  <li><strong>Feedforward networks</strong> (MLPs) are the simplest architecture where information flows in one direction</li>
  <li><strong>Forward propagation</strong> computes the output by passing inputs through successive layers</li>
  <li><strong>Network depth</strong> (number of layers) and <strong>width</strong> (neurons per layer) determine capacity</li>
  <li><strong>Universal Approximation Theorem</strong> shows networks can approximate any function, but doesn’t guarantee efficiency</li>
  <li><strong>Fully connected layers</strong> connect every neuron to every neuron in adjacent layers</li>
  <li><strong>Proper architecture design</strong> depends on the problem, data, and computational resources</li>
</ul>

<p>In the next lesson, we’ll explore activation functions in more detail and understand their critical role in learning.</p>


</div>

<!-- Back to Chapter Home Link -->

  
  
  <div style="margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #007bff;">
    <a href="/deep-learning-self-learning/contents/en/chapter02/" style="text-decoration: none; color: #007bff; font-weight: bold;">
      ← Back to Chapter Home
    </a>
  </div>













<div class="related">
  <ul class="related-posts">
    
      
    
      
        <li>
          <h2>Previous Post</h2>
          <h3>
            <a href="/deep-learning-self-learning/contents/en/chapter02/02_01_Perceptron_and_Neurons/">
              02-01 The Perceptron and Artificial Neurons
            </a>
          </h3>
        </li>
      
    
      
    
      
    
      
    
    
    
  
    
  
    
  
    
      <li>
        <h2>Next Post</h2>
        <h3>
          <a href="/deep-learning-self-learning/contents/en/chapter02/02_03_Activation_Functions/">
            02-03 Activation Functions in Detail
          </a>
        </h3>
      </li>
    
  
    
  
  </ul>
</div>



<script src="https://utteranc.es/client.js"
        repo="convex-deep-learning-for-all/convex-deep-learning-for-all.github.io"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/deep-learning-self-learning/public/js/script.js'></script>
    <script src='/deep-learning-self-learning/public/js/multilang.js'></script>
    <script src='/deep-learning-self-learning/public/js/search.js'></script>
  </body>
</html>
