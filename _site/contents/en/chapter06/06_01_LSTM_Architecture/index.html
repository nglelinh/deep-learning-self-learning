<!DOCTYPE html>
<html lang="en">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      06-01 Long Short-Term Memory Networks &middot; Deep learning in Data Science
    
  </title>

  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/poole.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/syntax.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/lanyon.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/github-markdown.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/multilang.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/search.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/content-boxes.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap">
  
  <!-- Lunr.js for search functionality -->
  <script src="https://unpkg.com/lunr/lunr.js"></script>

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="http://localhost:4000/deep-learning-self-learning/public/logo.png">
  <link rel="shortcut icon" href="http://localhost:4000/deep-learning-self-learning/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/deep-learning-self-learning/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Deep learning in Data Science</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/">Home</a>

    

    
    
    
    <!-- Hiển thị các chương có sẵn cho ngôn ngữ hiện tại -->
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter00/">
              00. Basic Mathematical Concepts
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter01/">
              01. Introduction to Deep Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter02/">
              02. Neural Networks Fundamentals
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter03/">
              03. Training Neural Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter04/">
              04. Convolutional Neural Networks (CNNs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter05/">
              05. Recurrent Neural Networks (RNNs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter06/">
              06. LSTM and GRU Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter07/">
              07. Attention Mechanisms
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter08/">
              08. Transformers
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter09/">
              09. Regularization Techniques
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter10/">
              10. Deep Learning Algorithms
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter11/">
              11. Generative Models
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter12/">
              12. Autoencoders
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter13/">
              13. Variational Autoencoders (VAE)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter14/">
              14. Generative Adversarial Networks (GANs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter15/">
              15. Transfer Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter16/">
              16. Self-Supervised Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter17/">
              17. Computer Vision Applications
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter18/">
              18. Natural Language Processing
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter19/">
              19. Speech and Audio Processing
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter20/">
              20. Reinforcement Learning Basics
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter21/">
              21. Deep Reinforcement Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter22/">
              22. Graph Neural Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter23/">
              23. Efficient Deep Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter24/">
              24. Interpretability and Explainability
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter25/">
              25. Advanced Topics and Future Directions
              
            </a>
          
        
      
    
    
    <!-- Nếu không có nội dung cho ngôn ngữ hiện tại, hiển thị thông báo -->
    
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2025. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/deep-learning-self-learning/" title="Home">Deep learning in Data Science</a>
            <small></small>
          </h3>
          <!-- Header Actions: Language Toggle and GitHub Link -->
          <div class="header-actions">
            <div class="language-toggle">
              <a href="/deep-learning-self-learning/contents/vi/chapter00/" class="language-switch" title="Switch to Vietnamese">Switch to Vietnamese</a>
            </div>
            <a class="github-logo__wrapper" target="_blank" href="https://github.com/nglelinh/deep-learning-self-learning" title="Github">
             <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
            </a>
          </div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">
    06-01 Long Short-Term Memory Networks
    
      
        <span class="lesson-badge required large">Required</span>
      
    
  </h1>
  <h1 id="long-short-term-memory-conquering-the-vanishing-gradient">Long Short-Term Memory: Conquering the Vanishing Gradient</h1>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/800px-LSTM_Cell.svg.png" alt="LSTM Cell Architecture" />
<em>Hình ảnh: Cấu trúc chi tiết của một LSTM cell với các cổng điều khiển. Nguồn: Wikimedia Commons</em></p>

<h2 id="1-concept-overview">1. Concept Overview</h2>

<p>The Long Short-Term Memory (LSTM) network represents one of the most important architectural innovations in the history of recurrent neural networks. Introduced by Hochreiter and Schmidhuber in 1997, LSTMs were specifically designed to solve the vanishing gradient problem that plagued vanilla RNNs, enabling neural networks to learn dependencies spanning hundreds or even thousands of time steps. While the architecture might seem complex at first glance with its multiple gates and cell state, each component serves a specific, carefully designed purpose in managing the flow of information through time.</p>

<p>Understanding why LSTMs were necessary requires appreciating the fundamental challenge they address. As we saw with vanilla RNNs, gradients during backpropagation through time must pass through repeated matrix multiplications by the recurrent weight matrix and activation function derivatives. When these operations consistently suppress signals (as tanh derivatives do when activations saturate), gradients vanish exponentially with the number of time steps. This makes learning long-range dependencies—understanding that a word at the beginning of a paragraph influences interpretation of a sentence at the end—nearly impossible for vanilla RNNs in practice.</p>

<p>The LSTM’s solution is elegant in its core idea, though complex in execution: create explicit pathways for information to flow unchanged through time. The key innovation is the cell state, a separate pathway that runs parallel to the hidden state. The cell state can maintain information across many time steps with only minor linear interactions, avoiding the repeated nonlinear transformations that cause vanishing gradients in vanilla RNNs. Think of the cell state as a highway for information transmission through time, while the hidden state handles moment-to-moment processing.</p>

<p>But simply having a separate cell state isn’t sufficient—we need mechanisms to control what information enters the cell state, what information is retained or forgotten, and what information is exposed to the rest of the network. This is where gates come in. LSTMs use three types of gates—forget gates, input gates, and output gates—each implemented as a sigmoid neural network layer that outputs values between 0 and 1. These gates act as learnable switches, determining how much information flows through different paths. The sigmoid is crucial here: it provides smooth gradients (unlike hard thresholds) while its output range [0,1] makes it interpretable as a probability or proportion—how much to forget, how much to add, how much to output.</p>

<p>The genius of the LSTM architecture lies in how these gates work together to create flexible, learnable memory dynamics. The forget gate can clear outdated information from the cell state. The input gate can selectively add new information. The output gate can control what parts of the cell state are exposed to the downstream network. All of this happens through learned parameters that adapt to the specific patterns in the training data. The network learns not just what to remember but when to remember, when to forget, and when to act on its memory—meta-cognitive skills that emerge from the architecture’s inductive bias.</p>

<p>LSTMs became enormously successful, dominating sequence modeling from the late 1990s through the mid-2010s. They enabled breakthroughs in machine translation, speech recognition, handwriting recognition, and many other sequential tasks. Even after Transformers emerged as the dominant architecture for NLP, LSTMs remain important: they use less memory than Transformers (\(O(n)\) vs \(O(n^2)\)), process sequences naturally online (unlike Transformers which typically process entire sequences), and for certain tasks with very long sequences or online processing requirements, still offer advantages. Understanding LSTMs deeply means understanding both a historically important architecture and ongoing principles about managing information flow in recurrent computations.</p>

<h2 id="2-mathematical-foundation">2. Mathematical Foundation</h2>

<p>The mathematical formulation of LSTMs reveals how multiple components work together to create controllable memory dynamics. At each time step \(t\), an LSTM maintains two state vectors: the hidden state \(\mathbf{h}_t\) (like vanilla RNNs) and the cell state \(\mathbf{c}_t\) (the LSTM’s innovation). Given input \(\mathbf{x}_t\) and previous states \(\mathbf{h}_{t-1}\) and \(\mathbf{c}_{t-1}\), the LSTM computes new states through a carefully orchestrated sequence of operations.</p>

<p>First, we concatenate the previous hidden state and current input into a single vector: \([\mathbf{h}_{t-1}; \mathbf{x}_t]\). This concatenation appears in all gate computations, meaning each gate considers both what’s happening now (\(\mathbf{x}_t\)) and what the network was previously thinking about (\(\mathbf{h}_{t-1}\)). This design allows gates to make contextual decisions—whether to forget might depend on both the current input and previous context.</p>

<p>The <strong>forget gate</strong> determines what information to discard from the cell state:</p>

\[\mathbf{f}_t = \sigma(\mathbf{W}_f \cdot [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_f)\]

<p>The sigmoid activation ensures \(\mathbf{f}_t \in (0,1)\), which we can interpret as “proportion to keep.” When \(f_t^{(i)} \approx 1\), we keep nearly all of cell state dimension \(i\). When \(f_t^{(i)} \approx 0\), we forget nearly everything. The network learns through backpropagation when forgetting helps—for example, when starting a new sentence, we might forget subject-verb agreement information from the previous sentence.</p>

<p>The <strong>input gate</strong> controls what new information to add to the cell state. It consists of two parts: a gate determining how much to add, and a candidate cell state determining what to add:</p>

\[\mathbf{i}_t = \sigma(\mathbf{W}_i \cdot [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_i)\]

\[\tilde{\mathbf{c}}_t = \tanh(\mathbf{W}_C \cdot [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_C)\]

<p>The candidate \(\tilde{\mathbf{c}}_t\) uses tanh to create values in \([-1, 1]\), representing potential updates to the cell state. The input gate \(\mathbf{i}_t\) then moderates how much of this candidate to actually use. This two-component design provides flexibility: the candidate can propose updates based on the input, while the gate decides whether now is the right time to update memory.</p>

<p>The cell state update combines forgetting old information and adding new information:</p>

\[\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t\]

<p>This equation is the heart of the LSTM. The Hadamard (element-wise) product \(\mathbf{f}_t \odot \mathbf{c}_{t-1}\) selectively retains information from the previous cell state. The second term \(\mathbf{i}_t \odot \tilde{\mathbf{c}}_t\) adds selectively chosen new information. Notice this is a weighted sum with minimal nonlinearity—just the element-wise multiplications by the gates. Crucially, there’s no matrix multiplication by recurrent weights here, no tanh squashing the entire state. This is why gradients can flow through the cell state more easily than through vanilla RNN hidden states.</p>

<p>The gradient flow analysis makes this explicit. When backpropagating through the cell state update:</p>

\[\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}} = \mathbf{f}_t\]

<p>The gradient is simply the forget gate values! If the forget gate is consistently near 1 (which the network can learn to do when long-term memory is needed), gradients flow backward through time nearly unchanged. This is a dramatic improvement over vanilla RNNs where:</p>

\[\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}} = \text{diag}(\tanh'(\mathbf{z}_t)) \mathbf{W}_{hh}\]

<p>involves both the weight matrix and activation derivatives, typically causing exponential decay.</p>

<p>The <strong>output gate</strong> determines what information from the cell state to expose to the rest of the network:</p>

\[\mathbf{o}_t = \sigma(\mathbf{W}_o \cdot [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_o)\]

\[\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{c}_t)\]

<p>We apply tanh to the cell state (squashing it to \([-1,1]\)) before the output gate modulates it. Why tanh here? The cell state can grow unbounded through repeated additions, so tanh normalizes it before exposure. The output gate then decides what to show—perhaps the network has information in the cell state that’s useful for future computation but not relevant to the current output.</p>

<p>Counting parameters reveals the cost of LSTM’s sophistication. With input dimension \(d_x\), hidden dimension \(d_h\), an LSTM has:</p>
<ul>
  <li>Forget gate: \((d_h + d_x) \times d_h + d_h\) parameters (matrix + bias)</li>
  <li>Input gate: \((d_h + d_x) \times d_h + d_h\) parameters</li>
  <li>Candidate: \((d_h + d_x) \times d_h + d_h\) parameters</li>
  <li>Output gate: \((d_h + d_x) \times d_h + d_h\) parameters</li>
</ul>

<p>Total: \(4[(d_h + d_x) \times d_h + d_h]\) parameters, about 4× more than vanilla RNN. This is the price of controllable memory—more parameters to learn, more computation per time step, but dramatically better ability to learn long-range dependencies.</p>

<p>The Gated Recurrent Unit (GRU), introduced by Cho et al. in 2014, simplifies LSTMs while retaining most benefits. GRUs merge the cell and hidden states, use only two gates instead of three, and have about 25% fewer parameters. The GRU equations:</p>

<p>\(\mathbf{r}_t = \sigma(\mathbf{W}_r \cdot [\mathbf{h}_{t-1}; \mathbf{x}_t])\) (reset gate: how much past to use)</p>

<p>\(\mathbf{z}_t = \sigma(\mathbf{W}_z \cdot [\mathbf{h}_{t-1}; \mathbf{x}_t])\) (update gate: how much past to keep)</p>

<p>\(\tilde{\mathbf{h}}_t = \tanh(\mathbf{W} \cdot [\mathbf{r}_t \odot \mathbf{h}_{t-1}; \mathbf{x}_t])\) (candidate hidden state)</p>

<p>\(\mathbf{h}_t = (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t\) (interpolate between old and new)</p>

<p>The update gate \(\mathbf{z}_t\) acts like a combined forget-and-input gate, deciding how much to interpolate between the previous state and the candidate. When \(z_t^{(i)} \approx 0\), dimension \(i\) keeps its old value (like forget gate ≈ 1, input gate ≈ 0 in LSTM). When \(z_t^{(i)} \approx 1\), it uses the new candidate (like forget gate ≈ 0, input gate ≈ 1). This coupling reduces parameters while maintaining the ability to control information flow.</p>

<h2 id="3-example--intuition">3. Example / Intuition</h2>

<p>To build genuine intuition for how LSTMs manage long-range dependencies, let’s trace through a concrete linguistic example: processing the sentence “The cat, which we found in the garden, was hungry” to predict whether the verb should be singular or plural (“was” vs “were”).</p>

<p>This example is challenging for vanilla RNNs because the subject “cat” (singular) appears early, followed by a relative clause “which we found in the garden” that could mislead the model with the plural “we,” and only then comes the verb that must agree with “cat.” A vanilla RNN must maintain the “cat is singular” information through processing six intervening words, during which the hidden state undergoes six transformations that might corrupt or erase this information.</p>

<p>Let’s trace what an LSTM might learn to do. When processing “cat,” the input gate allows important information (this is the subject, it’s singular) to enter the cell state. The cell state now contains something like “subject = cat, number = singular.” As we process the relative clause, the forget gate learns to keep this subject information (\(f_t \approx 1\) for dimensions encoding subject number) while the input gate allows information about “we found in the garden” to enter other dimensions of the cell state. Crucially, the subject information persists nearly unchanged through these steps because the forget gate protects it.</p>

<p>When we finally reach the position where we must generate the verb, the output gate learns to expose the subject number information from the cell state. The downstream layers can then use this information to choose “was” over “were.” The relative clause information might be gated off (\(o_t \approx 0\) for those dimensions) since it’s not relevant for verb conjugation.</p>

<p>Let’s make this concrete with simplified numbers. Suppose our cell state has just two dimensions: subject-number and clause-context. After processing “cat”:</p>

<p>\(\mathbf{c}_{\text{cat}} = \begin{bmatrix} 0.9 \\ 0.1 \end{bmatrix}\) (strongly singular, little clause context)</p>

<p>As we process “which we found” (plural), the forget gate for subject-number stays high:</p>

<p>\(\mathbf{f}_{\text{which}} = \begin{bmatrix} 0.95 \\ 0.1 \end{bmatrix}\) (keep subject info, forget old clause info)</p>

<p>The input gate allows clause information:</p>

<p>\(\mathbf{i}_{\text{which}} = \begin{bmatrix} 0.05 \\ 0.9 \end{bmatrix}\), \(\tilde{\mathbf{c}}_{\text{which}} = \begin{bmatrix} 0.2 \\ 0.7 \end{bmatrix}\)</p>

<p>After update:</p>

\[\mathbf{c}_{\text{which}} = \begin{bmatrix} 0.95 \\ 0.1 \end{bmatrix} \odot \begin{bmatrix} 0.9 \\ 0.1 \end{bmatrix} + \begin{bmatrix} 0.05 \\ 0.9 \end{bmatrix} \odot \begin{bmatrix} 0.2 \\ 0.7 \end{bmatrix} = \begin{bmatrix} 0.865 \\ 0.64 \end{bmatrix}\]

<p>The subject-number information (0.865) has degraded only slightly (from 0.9), while clause context has updated. This selective preservation is what enables long-range dependencies.</p>

<p>The three-gate design might seem overengineered, but each gate serves a distinct purpose that becomes clear when considering different linguistic phenomena. The forget gate handles context switches (new sentences, topic changes). The input gate manages relevance filtering (not all information deserves storage). The output gate controls exposure (information might be worth remembering but not worth acting on immediately). This three-way decomposition provides fine-grained control over memory dynamics that proves essential for complex sequential tasks.</p>

<p>Comparing LSTMs to biological memory systems provides another angle of intuition. Human working memory doesn’t simply accumulate all experiences—we forget what’s irrelevant (forget gate), selectively encode important new information (input gate), and retrieve different memories in different contexts (output gate). While LSTMs are far simpler than biological memory, they capture this fundamental principle that effective memory requires not just storage but selective reading, writing, and forgetting.</p>

<h2 id="2-mathematical-foundation-1">2. Mathematical Foundation</h2>

<p>Let’s build up the LSTM equations systematically, understanding each component’s role in the larger system. At time step \(t\), we have inputs \(\mathbf{x}_t \in \mathbb{R}^{d_x}\), previous hidden state \(\mathbf{h}_{t-1} \in \mathbb{R}^{d_h}\), and previous cell state \(\mathbf{c}_{t-1} \in \mathbb{R}^{d_h}\). We’ll compute new states \(\mathbf{h}_t\) and \(\mathbf{c}_t\) through the following sequence of operations.</p>

<p>First, all gates operate on the concatenation \([\mathbf{h}_{t-1}; \mathbf{x}_t] \in \mathbb{R}^{d_h + d_x}\). Let’s define this explicitly:</p>

\[\mathbf{z}_t = [\mathbf{h}_{t-1}; \mathbf{x}_t]\]

<p>Now the forget gate computation:</p>

\[\mathbf{f}_t = \sigma(\mathbf{W}_f \mathbf{z}_t + \mathbf{b}_f)\]

<p>where \(\mathbf{W}_f \in \mathbb{R}^{d_h \times (d_h + d_x)}\) and \(\mathbf{b}_f \in \mathbb{R}^{d_h}\). The sigmoid ensures each component of \(\mathbf{f}_t\) lies in \((0, 1)\). We can think of \(f_t^{(i)}\) as the probability of retaining information in cell state dimension \(i\). In practice, forget gates often learn to stay near 1 for most dimensions most of the time, occasionally dropping to near 0 when the network decides to clear memory for that dimension.</p>

<p>The input gate and candidate computation happen in parallel:</p>

\[\mathbf{i}_t = \sigma(\mathbf{W}_i \mathbf{z}_t + \mathbf{b}_i)\]

\[\tilde{\mathbf{c}}_t = \tanh(\mathbf{W}_C \mathbf{z}_t + \mathbf{b}_C)\]

<p>The candidate \(\tilde{\mathbf{c}}_t\) uses tanh, producing values in \((-1, 1)\), representing proposed updates to the cell state. These could be positive (add information) or negative (subtract, though this is less common). The input gate \(\mathbf{i}_t\) moderates how much of the candidate to actually add. When processing important information (like a sentence’s subject), the input gate opens (\(i_t^{(i)} \approx 1\)) to store it. For filler words or irrelevant information, the gate closes (\(i_t^{(i)} \approx 0\)).</p>

<p>The cell state update is where information actually flows through time:</p>

\[\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t\]

<p>Let’s analyze this equation’s gradient properties carefully. When computing \(\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}}\):</p>

\[\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}} = \mathbf{f}_t\]

<p>This is elementwise—the gradient for dimension \(i\) is simply \(f_t^{(i)}\). If the network keeps \(f_t^{(i)} = 0.99\) consistently across \(T\) time steps, the gradient for that dimension is \((0.99)^T\), which for \(T=100\) is about 0.37—substantial retention compared to vanilla RNN where it might be \(10^{-30}\). And the network can learn to keep \(f_t^{(i)}\) near 1 exactly when long-range memory is needed for dimension \(i\).</p>

<p>Contrast this with vanilla RNN where:</p>

\[\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}} = \text{diag}(\tanh'(\mathbf{z}_t)) \mathbf{W}_{hh}\]

<p>The derivative involves both the weight matrix (potentially poorly conditioned with eigenvalues far from 1) and \(\tanh'\) which is typically much less than 1 when activations are saturated. The LSTM’s direct path through the cell state, controlled by learned gates, provides much more stable gradient flow.</p>

<p>The output gate and hidden state computation:</p>

\[\mathbf{o}_t = \sigma(\mathbf{W}_o \mathbf{z}_t + \mathbf{b}_o)\]

\[\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{c}_t)\]

<p>We squash the cell state with tanh before exposing it. Why? The cell state can grow unbounded through repeated additions (imagine \(c_t = c_{t-1} + 0.1\) for 1000 steps gives \(c_{1000} = 100\)), so tanh normalizes it to \([-1,1]\) before downstream layers process it. The output gate then selectively exposes parts of this normalized cell state based on what’s relevant for current processing.</p>

<p>The complete LSTM involves four sets of weights (\(\mathbf{W}_f, \mathbf{W}_i, \mathbf{W}_C, \mathbf{W}_o\)) each of size \(d_h \times (d_h + d_x)\), plus four bias vectors, totaling \(4[d_h(d_h + d_x) + d_h]\) parameters. The computational cost per time step is \(O(d_h^2 + d_h d_x)\), about 4× that of vanilla RNN. This is the tradeoff: more computation and parameters buy better gradient flow and longer-range dependency learning.</p>

<p>GRUs simplify this by combining forget and input gates into a single update gate \(\mathbf{z}_t\), and using a reset gate \(\mathbf{r}_t\) to control how much previous hidden state influences the candidate:</p>

\[\mathbf{r}_t = \sigma(\mathbf{W}_r [\mathbf{h}_{t-1}; \mathbf{x}_t])\]

\[\mathbf{z}_t = \sigma(\mathbf{W}_z [\mathbf{h}_{t-1}; \mathbf{x}_t])\]

\[\tilde{\mathbf{h}}_t = \tanh(\mathbf{W} [\mathbf{r}_t \odot \mathbf{h}_{t-1}; \mathbf{x}_t])\]

\[\mathbf{h}_t = (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t\]

<p>The update gate \(\mathbf{z}_t\) interpolates between keeping the old hidden state and using the new candidate. When \(z_t^{(i)} = 0\), dimension \(i\) copies the previous value (like LSTM with \(f_t^{(i)} = 1, i_t^{(i)} = 0\)). When \(z_t^{(i)} = 1\), it uses the new candidate (like \(f_t^{(i)} = 0, i_t^{(i)} = 1\)). The coupling of forget and input into a single gate reduces parameters while maintaining control over memory.</p>

<p>The reset gate \(\mathbf{r}_t\) modulates how much previous hidden state influences the candidate computation. When \(r_t^{(i)} = 0\), the candidate ignores previous state for dimension \(i\), essentially “resetting” that dimension. This allows the network to forget when appropriate while computing new representations from input alone.</p>

<h2 id="4-code-snippet">4. Code Snippet</h2>

<p>Let’s implement LSTM from scratch to understand every operation, then compare with PyTorch’s optimized version:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">LSTMCell</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Single LSTM cell implementing one time step of computation.
    
    This implementation makes every operation explicit. Modern frameworks
    fuse these operations for efficiency, but our goal is understanding,
    not speed. We</span><span class="sh">'</span><span class="s">ll see exactly how gates modulate information flow.
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initialize LSTM cell with careful weight initialization.
        
        We use Xavier/Glorot initialization scaled for sigmoid and tanh.
        This initialization scheme was specifically designed to maintain
        reasonable activation and gradient scales, preventing both vanishing
        and explosion early in training.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        
        <span class="c1"># Combined input dimension (hidden + input)
</span>        <span class="n">combined_size</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">+</span> <span class="n">input_size</span>
        
        <span class="c1"># Initialize weights for four gates
</span>        <span class="c1"># Using Xavier initialization: scale by 1/√(combined_size)
</span>        <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">combined_size</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">Wf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">combined_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">combined_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
        <span class="n">self</span><span class="p">.</span><span class="n">WC</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">combined_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wo</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">combined_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
        
        <span class="c1"># Initialize biases
</span>        <span class="c1"># Forget gate bias often initialized to 1 to encourage remembering initially
</span>        <span class="c1"># This is called "forget bias trick" - start by remembering everything
</span>        <span class="n">self</span><span class="p">.</span><span class="n">bf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># Start with high forget gate!
</span>        <span class="n">self</span><span class="p">.</span><span class="n">bi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bC</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bo</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Numerically stable sigmoid</span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span>
                       <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)),</span>
                       <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        One LSTM time step.
        
        x_t: current input (input_size, 1)
        h_prev: previous hidden state (hidden_size, 1)
        c_prev: previous cell state (hidden_size, 1)
        
        Returns:
            h_t: new hidden state
            c_t: new cell state
            gates: dictionary of gate values (for analysis/debugging)
        </span><span class="sh">"""</span>
        <span class="c1"># Concatenate previous hidden state and current input
</span>        <span class="c1"># This combined vector influences all gates
</span>        <span class="n">combined</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">h_prev</span><span class="p">,</span> <span class="n">x_t</span><span class="p">])</span>
        
        <span class="c1"># Compute all gates
</span>        <span class="c1"># Each gate is a learned sigmoid function of the combined input
</span>        <span class="n">f_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Wf</span> <span class="o">@</span> <span class="n">combined</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bf</span><span class="p">)</span>  <span class="c1"># Forget gate
</span>        <span class="n">i_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Wi</span> <span class="o">@</span> <span class="n">combined</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bi</span><span class="p">)</span>  <span class="c1"># Input gate
</span>        <span class="n">C_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">WC</span> <span class="o">@</span> <span class="n">combined</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bC</span><span class="p">)</span>  <span class="c1"># Candidate values
</span>        <span class="n">o_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Wo</span> <span class="o">@</span> <span class="n">combined</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bo</span><span class="p">)</span>  <span class="c1"># Output gate
</span>        
        <span class="c1"># Update cell state: forget old, add new
</span>        <span class="c1"># This is THE key equation of LSTM
</span>        <span class="c1"># Notice: minimal nonlinearity, mostly linear combination
</span>        <span class="n">c_t</span> <span class="o">=</span> <span class="n">f_t</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">C_tilde</span>
        
        <span class="c1"># Compute new hidden state from cell state
</span>        <span class="c1"># tanh squashes unbounded cell state to [-1, 1]
</span>        <span class="c1"># output gate modulates what's exposed
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>
        
        <span class="c1"># Return states and gates (gates useful for visualization/debugging)
</span>        <span class="n">gates</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">forget</span><span class="sh">'</span><span class="p">:</span> <span class="n">f_t</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">:</span> <span class="n">i_t</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">:</span> <span class="n">o_t</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">candidate</span><span class="sh">'</span><span class="p">:</span> <span class="n">C_tilde</span>
        <span class="p">}</span>
        
        <span class="k">return</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span><span class="p">,</span> <span class="n">gates</span>

<span class="k">class</span> <span class="nc">LSTM</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Full LSTM network for sequence processing.
    
    Wraps LSTMCell to process entire sequences, maintaining states across
    time steps. This is the complete LSTM as used in practice.
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cell</span> <span class="o">=</span> <span class="nc">LSTMCell</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        
        <span class="c1"># Output projection (maps hidden state to predictions)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">Why</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="n">self</span><span class="p">.</span><span class="n">by</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">output_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Process entire sequence.
        
        inputs: list of input vectors [x_1, ..., x_T]
        return_sequences: if True, return outputs at all time steps
                         if False, return only final output
        
        Returns:
            outputs: predictions at each time step (if return_sequences=True)
                     or just final prediction (if False)
            hidden_states: all hidden states [h_0, ..., h_T]
            cell_states: all cell states [c_0, ..., c_T]
            all_gates: gate values at each time step (for analysis)
        </span><span class="sh">"""</span>
        <span class="c1"># Initialize states to zero
</span>        <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># Track evolution through time
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">]</span>
        <span class="n">cell_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span><span class="p">]</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_gates</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Process sequence step by step
</span>        <span class="k">for</span> <span class="n">x_t</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="c1"># LSTM cell update
</span>            <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">gates</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">cell</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
            
            <span class="n">hidden_states</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
            <span class="n">cell_states</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
            <span class="n">all_gates</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">gates</span><span class="p">)</span>
            
            <span class="c1"># Compute output from hidden state
</span>            <span class="n">y_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">Why</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">by</span>
            <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">return_sequences</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">cell_states</span><span class="p">,</span> <span class="n">all_gates</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For sequence classification, use only final output
</span>            <span class="k">return</span> <span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">cell_states</span><span class="p">,</span> <span class="n">all_gates</span>

<span class="c1"># Demonstrate LSTM learning long-range dependencies
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">LSTM: Learning Long-Range Dependencies</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>

<span class="c1"># Create task requiring long-term memory
# Remember first number, ignore middle numbers, predict first + last
</span><span class="k">def</span> <span class="nf">create_memory_task</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Task: Given sequence [a, x, x, x, ..., x, b], predict a + b
    
    This requires remembering </span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="s"> across seq_length-2 intervening values,
    exactly the kind of long-range dependency vanilla RNNs struggle with.
    </span><span class="sh">"""</span>
    <span class="n">inputs_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">targets_list</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="c1"># Random first and last numbers
</span>        <span class="n">first</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">*</span> <span class="mi">10</span>
        <span class="n">last</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">*</span> <span class="mi">10</span>
        
        <span class="c1"># Fill middle with noise
</span>        <span class="n">sequence</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="n">first</span><span class="p">]])]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">seq_length</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">sequence</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">*</span> <span class="mi">10</span><span class="p">]]))</span>
        <span class="n">sequence</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="n">last</span><span class="p">]]))</span>
        
        <span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="n">first</span> <span class="o">+</span> <span class="n">last</span><span class="p">]])</span>
        
        <span class="n">inputs_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
        <span class="n">targets_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">inputs_list</span><span class="p">,</span> <span class="n">targets_list</span>

<span class="c1"># Create LSTM and data
</span><span class="n">lstm</span> <span class="o">=</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_inputs</span><span class="p">,</span> <span class="n">train_targets</span> <span class="o">=</span> <span class="nf">create_memory_task</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="c1"># Simple training loop (gradient descent on small dataset)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training LSTM on long-range dependency task...</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Task: Remember first number across 13 noisy numbers, add to last number</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c1"># Process each sequence
</span>    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">train_inputs</span><span class="p">,</span> <span class="n">train_targets</span><span class="p">):</span>
        <span class="c1"># Forward pass
</span>        <span class="n">output</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">,</span> <span class="n">cells</span><span class="p">,</span> <span class="n">gates</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        
        <span class="c1"># Loss (MSE)
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        
        <span class="c1"># For demonstration, we'll skip the backward pass implementation
</span>        <span class="c1"># (BPTT for LSTM is complex - modern frameworks handle it)
</span>        <span class="c1"># In practice, use PyTorch!
</span>    
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_inputs</span><span class="p">)</span>
    <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: Average Loss = </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Note: Full LSTM backpropagation is complex - use PyTorch in practice!</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Let</span><span class="sh">'</span><span class="s">s see PyTorch</span><span class="sh">'</span><span class="s">s implementation...</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># PyTorch LSTM implementation
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="k">class</span> <span class="nc">LSTMNetwork</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    LSTM using PyTorch</span><span class="sh">'</span><span class="s">s optimized implementation.
    
    PyTorch</span><span class="sh">'</span><span class="s">s LSTM is highly optimized, using cuDNN kernels on GPU
    for maximum performance. It handles all the gate computations,
    state management, and backpropagation automatically.
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">LSTMNetwork</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># PyTorch LSTM module
</span>        <span class="c1"># num_layers &gt; 1 stacks LSTMs (output of one feeds into next)
</span>        <span class="c1"># dropout between layers helps regularize stacked LSTMs
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span>
                           <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="n">num_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="mf">0.2</span><span class="p">)</span>
        
        <span class="c1"># Output layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        x: (batch, seq_len, input_size)
        hidden: optional tuple of (h_0, c_0)
        
        Returns:
            output: (batch, seq_len, output_size) if return_sequences
                    or (batch, output_size) if not
            (h_n, c_n): final hidden and cell states
        </span><span class="sh">"""</span>
        <span class="c1"># LSTM returns:
</span>        <span class="c1"># - lstm_out: hidden states at all time steps
</span>        <span class="c1"># - (h_n, c_n): final hidden and cell states for all layers
</span>        <span class="n">lstm_out</span><span class="p">,</span> <span class="p">(</span><span class="n">h_n</span><span class="p">,</span> <span class="n">c_n</span><span class="p">)</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        
        <span class="c1"># Use final time step for sequence classification
</span>        <span class="c1"># or all time steps for sequence-to-sequence
</span>        <span class="n">final_hidden</span> <span class="o">=</span> <span class="n">lstm_out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Last time step
</span>        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">final_hidden</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">h_n</span><span class="p">,</span> <span class="n">c_n</span><span class="p">)</span>

<span class="c1"># Train on memory task
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training PyTorch LSTM on Long-Range Memory Task</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>

<span class="c1"># Convert data to tensors
</span><span class="k">def</span> <span class="nf">prepare_torch_data</span><span class="p">(</span><span class="n">inputs_list</span><span class="p">,</span> <span class="n">targets_list</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Convert list of sequences to batched tensors</span><span class="sh">"""</span>
    <span class="c1"># Find max length (for padding)
</span>    <span class="n">max_len</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">inputs_list</span><span class="p">)</span>
    
    <span class="c1"># Pad sequences and stack
</span>    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">inputs_list</span><span class="p">,</span> <span class="n">targets_list</span><span class="p">):</span>
        <span class="c1"># Convert sequence to tensor and pad
</span>        <span class="n">seq_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">inp</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">],</span> 
                                  <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">X</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">seq_tensor</span><span class="p">)</span>
        <span class="n">y</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
    
    <span class="c1"># Stack into batched tensor
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>  <span class="c1"># (n_samples, seq_len, 1)
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># (n_samples, 1)
</span>    
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="nf">prepare_torch_data</span><span class="p">(</span><span class="n">train_inputs</span><span class="p">,</span> <span class="n">train_targets</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training data shape: </span><span class="si">{</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># (200, 15, 1)
</span>
<span class="c1"># Create model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">LSTMNetwork</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Training loop
</span><span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">300</span><span class="p">):</span>
    <span class="c1"># Forward pass
</span>    <span class="n">predictions</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Backward pass
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>  <span class="c1"># BPTT through LSTM happens here automatically
</span>    
    <span class="c1"># Gradient clipping (good practice for RNNs/LSTMs)
</span>    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">60</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: Loss = </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Test on new examples
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Testing LSTM Memory Capability</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="c1"># Create test sequences
</span>    <span class="n">test_cases</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">([</span><span class="mf">5.0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span><span class="o">*</span><span class="mi">10</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">13</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">],</span> <span class="mf">8.0</span><span class="p">),</span>
        <span class="p">([</span><span class="mf">7.0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span><span class="o">*</span><span class="mi">10</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">13</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">],</span> <span class="mf">9.0</span><span class="p">),</span>
        <span class="p">([</span><span class="mf">1.0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span><span class="o">*</span><span class="mi">10</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">13</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mf">9.0</span><span class="p">],</span> <span class="mf">10.0</span><span class="p">),</span>
    <span class="p">]</span>
    
    <span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">true_sum</span> <span class="ow">in</span> <span class="n">test_cases</span><span class="p">:</span>
        <span class="n">seq_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">seq_tensor</span><span class="p">)</span>
        
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">First: </span><span class="si">{</span><span class="n">seq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">, Last: </span><span class="si">{</span><span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  True sum: </span><span class="si">{</span><span class="n">true_sum</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">, Predicted: </span><span class="si">{</span><span class="n">pred</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Error: </span><span class="si">{</span><span class="nf">abs</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">true_sum</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">LSTM successfully learned to remember first value across many steps!</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">This demonstrates its advantage over vanilla RNNs for long-range dependencies.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s also visualize gate activations to understand what LSTM learns:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Analyze gate behavior
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Analyzing LSTM Gate Activations</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>

<span class="c1"># Create a simple manual LSTM to track gates
</span><span class="n">lstm_analyze</span> <span class="o">=</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Create sequence: [5, noise, noise, ..., 3]
</span><span class="n">test_sequence</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">5.0</span><span class="p">]])]</span>
<span class="n">test_sequence</span><span class="p">.</span><span class="nf">extend</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span><span class="o">*</span><span class="mi">10</span><span class="p">]])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
<span class="n">test_sequence</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">3.0</span><span class="p">]]))</span>

<span class="c1"># Forward pass tracking all gates
</span><span class="n">output</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">,</span> <span class="n">cells</span><span class="p">,</span> <span class="n">all_gates</span> <span class="o">=</span> <span class="n">lstm_analyze</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">test_sequence</span><span class="p">,</span> 
                                                         <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Gate activations through time (showing average across hidden dimensions):</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Time | Forget | Input | Output | Cell State (avg)</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">gates</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">all_gates</span><span class="p">):</span>
    <span class="n">f_avg</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="sh">'</span><span class="s">forget</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">i_avg</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">o_avg</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">c_avg</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">cells</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>  <span class="c1"># Cell state magnitude
</span>    
    <span class="n">marker</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> &lt;-- Important input</span><span class="sh">"</span> <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">t</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">all_gates</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="sh">""</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">t</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s"> | </span><span class="si">{</span><span class="n">f_avg</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">  | </span><span class="si">{</span><span class="n">i_avg</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s"> | </span><span class="si">{</span><span class="n">o_avg</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">  | </span><span class="si">{</span><span class="n">c_avg</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}{</span><span class="n">marker</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Observations:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">- Forget gate often stays high (~0.9-1.0) to maintain memory</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">- Input gate opens for important inputs (first and last values)</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">- Output gate controls what information is exposed</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">- Cell state accumulates information, maintaining magnitude</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Now implement GRU for comparison:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GRUCell</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    GRU cell - simpler alternative to LSTM.
    
    GRU merges cell and hidden states, uses only 2 gates (vs LSTM</span><span class="sh">'</span><span class="s">s 3),
    resulting in ~25% fewer parameters. Often performs comparably to LSTM
    while being faster to train and easier to tune.
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        
        <span class="n">combined_size</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">+</span> <span class="n">input_size</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">combined_size</span><span class="p">)</span>
        
        <span class="c1"># Two gates instead of three
</span>        <span class="n">self</span><span class="p">.</span><span class="n">Wr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">combined_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>  <span class="c1"># Reset
</span>        <span class="n">self</span><span class="p">.</span><span class="n">Wz</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">combined_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>  <span class="c1"># Update
</span>        <span class="n">self</span><span class="p">.</span><span class="n">Wh</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">combined_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>  <span class="c1"># Candidate
</span>        
        <span class="n">self</span><span class="p">.</span><span class="n">br</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bz</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bh</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)),</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        GRU has no separate cell state - simpler!
        
        Returns only new hidden state (which serves as both hidden and cell state)
        </span><span class="sh">"""</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">h_prev</span><span class="p">,</span> <span class="n">x_t</span><span class="p">])</span>
        
        <span class="c1"># Reset gate: how much past to use for candidate
</span>        <span class="n">r_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Wr</span> <span class="o">@</span> <span class="n">combined</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">br</span><span class="p">)</span>
        
        <span class="c1"># Update gate: how much to interpolate old vs new
</span>        <span class="n">z_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Wz</span> <span class="o">@</span> <span class="n">combined</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bz</span><span class="p">)</span>
        
        <span class="c1"># Candidate hidden state (uses reset previous state)
</span>        <span class="n">combined_reset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">r_t</span> <span class="o">*</span> <span class="n">h_prev</span><span class="p">,</span> <span class="n">x_t</span><span class="p">])</span>
        <span class="n">h_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Wh</span> <span class="o">@</span> <span class="n">combined_reset</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bh</span><span class="p">)</span>
        
        <span class="c1"># Interpolate between old and new
</span>        <span class="c1"># When z_t ≈ 0: keep old (h_t ≈ h_prev)
</span>        <span class="c1"># When z_t ≈ 1: use new (h_t ≈ h_tilde)
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">z_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">h_prev</span> <span class="o">+</span> <span class="n">z_t</span> <span class="o">*</span> <span class="n">h_tilde</span>
        
        <span class="n">gates</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">reset</span><span class="sh">'</span><span class="p">:</span> <span class="n">r_t</span><span class="p">,</span> <span class="sh">'</span><span class="s">update</span><span class="sh">'</span><span class="p">:</span> <span class="n">z_t</span><span class="p">,</span> <span class="sh">'</span><span class="s">candidate</span><span class="sh">'</span><span class="p">:</span> <span class="n">h_tilde</span><span class="p">}</span>
        
        <span class="k">return</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">gates</span>

<span class="c1"># Compare LSTM vs GRU on same task
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Comparing LSTM vs GRU</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">LSTMModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

<span class="k">class</span> <span class="nc">GRUModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GRU</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gru</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># Train both
</span><span class="n">lstm_model</span> <span class="o">=</span> <span class="nc">LSTMModel</span><span class="p">()</span>
<span class="n">gru_model</span> <span class="o">=</span> <span class="nc">GRUModel</span><span class="p">()</span>

<span class="c1"># Count parameters
</span><span class="n">lstm_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">lstm_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
<span class="n">gru_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">gru_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">LSTM parameters: </span><span class="si">{</span><span class="n">lstm_params</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">GRU parameters: </span><span class="si">{</span><span class="n">gru_params</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">GRU has </span><span class="si">{</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gru_params</span><span class="o">/</span><span class="n">lstm_params</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">% fewer parameters</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Both can learn long-range dependencies effectively.</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">GRU: Simpler, faster. LSTM: More flexible, sometimes better on complex tasks.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="5-related-concepts">5. Related Concepts</h2>

<p>The relationship between LSTMs and vanilla RNNs exemplifies a recurring pattern in deep learning: identifying failure modes of simple architectures and designing targeted solutions through architectural innovation. Vanilla RNNs fail on long sequences due to vanishing gradients during backpropagation through time. LSTMs solve this by creating a separate information pathway (the cell state) with additive rather than multiplicative updates, and by using gates to control information flow. This isn’t just fixing a bug—it’s a fundamental architectural change motivated by understanding the mathematics of gradient flow.</p>

<p>The evolution from LSTM to GRU illustrates another important principle: simpler can be better when it preserves the essential mechanism. GRUs achieve similar performance to LSTMs for many tasks while having 25% fewer parameters and simpler dynamics (no separate cell state). The GRU’s design philosophy is minimalism—use the fewest mechanisms necessary to achieve the desired behavior. The update gate combines LSTM’s forget and input gates, reducing parameters while maintaining the crucial ability to control memory. The reset gate replaces the output gate’s functionality in a different way. For practitioners, this often means starting with GRU (simpler, faster) and only switching to LSTM if the task demonstrably benefits from its additional capacity.</p>

<p>The connection to gating mechanisms in neural architectures more broadly reveals a powerful pattern. Gates—sigmoid-activated layers that output values in (0,1) used to modulate other values—appear throughout deep learning. Highway networks use gates to control skip connections. Attention mechanisms use gates (the attention weights) to select information. Neural Turing Machines use gates to control memory read/write. The pattern is consistent: when we need learnable control over information flow, we use gates. Understanding why this works—smooth differentiability, interpretability as probabilities, effectiveness at learning conditional behavior—helps appreciate this architectural motif.</p>

<p>LSTMs and attention mechanisms have an interesting relationship. Both address long-range dependencies, but differently. LSTMs compress all past information into a fixed-size state, updated through gates. Attention allows direct access to all past states, selecting relevant ones through attention weights. This makes attention more powerful (no lossy compression) but more expensive (\(O(n^2)\) instead of \(O(n)\)). The Transformer’s success suggested that for many NLP tasks with sufficient compute, attention’s direct access outweighs LSTM’s efficiency. Yet for tasks with very long sequences or real-time constraints, LSTMs remain relevant.</p>

<p>The concept of explicit memory management in LSTMs connects to computer science more broadly—the idea of caching important information, evicting stale data, and controlling access. Database systems, operating system memory management, and CPU caches all face similar challenges of deciding what to remember and what to forget with limited capacity. LSTMs learn analogous policies from data rather than having them hand-coded. This connection helps frame what LSTMs are doing: they’re learned, differentiable memory management systems.</p>

<p>Finally, understanding LSTMs’ success and limitations informs architecture design more generally. LSTMs succeeded because they addressed a specific, well-understood problem (vanishing gradients) with a targeted solution (gated cell state). Their limitations (sequential processing, fixed-size state bottleneck) motivated further innovations (attention, Transformers). This progression from simple RNNs to complex LSTMs to attention-based Transformers shows how the field advances: identify limitations through analysis, design architectures addressing those limitations, discover new limitations, repeat. Each architecture teaches us something about the inductive biases and mechanisms needed for different types of sequential reasoning.</p>

<h2 id="6-fundamental-papers">6. Fundamental Papers</h2>

<p><strong><a href="https://www.bioinf.jku.at/publications/older/2604.pdf">“Long Short-Term Memory” (1997)</a></strong><br />
<em>Authors</em>: Sepp Hochreiter, Jürgen Schmidhuber<br />
This foundational paper introduced the LSTM architecture and rigorously analyzed why vanilla RNNs fail to learn long-range dependencies. Hochreiter and Schmidhuber showed mathematically that during backpropagation through time, gradients either vanish or explode exponentially unless the network is carefully constructed to avoid this. They proposed the LSTM with its constant error carousel (the cell state) as a solution, proving that LSTMs can in principle learn arbitrary long-range dependencies. The paper is remarkably prescient, addressing issues like memory capacity and proposing solutions that became standard (like forget gates, added in later work). While LSTMs took years to gain widespread adoption (partly due to limited computational resources and datasets at the time), this paper established the theoretical foundation and demonstrated LSTM’s advantages on carefully constructed tasks requiring long-term memory. It’s one of the most cited papers in all of deep learning and arguably enabled much of the progress in sequence modeling over the next two decades.</p>

<p><strong><a href="https://doi.org/10.1162/089976600300015015">“Learning to Forget: Continual Prediction with LSTM” (2000)</a></strong><br />
<em>Authors</em>: Felix A. Gers, Jürgen Schmidhuber, Fred Cummins<br />
The original LSTM architecture lacked a mechanism to reset the cell state—it could only add information, not remove it. This led to saturation problems on long sequences where the cell state would fill up with outdated information. This paper introduced the forget gate, allowing the network to selectively clear parts of its memory when they’re no longer needed. This seemingly simple addition—one more gate that modulates the cell state update—made LSTMs dramatically more practical for real-world tasks. The paper demonstrated improved performance on continual learning tasks where the network must process multiple sequences and reset context between them. The forget gate has become a standard part of all LSTM implementations, and the paper illustrates how architectural details that seem minor can have major practical impacts. It also demonstrates the value of ongoing refinement—the best architectures often emerge through iterative improvements addressing practical issues discovered during application.</p>

<p><strong><a href="https://arxiv.org/abs/1406.1078">“Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation” (2014)</a></strong><br />
<em>Authors</em>: Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio<br />
This paper introduced the Gated Recurrent Unit (GRU) as a simpler alternative to LSTM while also proposing the encoder-decoder architecture for neural machine translation. The GRU’s design was motivated by LSTM’s complexity—could we achieve similar performance with fewer parameters and simpler dynamics? The paper showed that GRU’s two gates (reset and update) could control information flow nearly as effectively as LSTM’s three gates, while being easier to implement and faster to train. The empirical results on machine translation demonstrated that architectural simplification doesn’t necessarily hurt performance when the essential mechanisms (gating for controlling memory) are preserved. This paper influenced architecture design philosophy: favor simpler designs when they maintain the key properties, as simplicity aids debugging, tuning, and understanding. The encoder-decoder framework introduced here became standard for sequence-to-sequence tasks, whether using RNNs, LSTMs, GRUs, or eventually Transformers.</p>

<p><strong><a href="https://arxiv.org/abs/1412.3555">“Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling” (2014)</a></strong><br />
<em>Authors</em>: Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio<br />
This paper provided the first comprehensive empirical comparison of LSTM and GRU across multiple sequence modeling tasks including music modeling, speech recognition, and language modeling. The careful experimental methodology—controlling for hyperparameters, architecture depth, and training procedures—allowed fair comparison focusing on the architectural differences. The findings were nuanced: neither architecture consistently dominated across all tasks, but GRU often matched LSTM performance while training faster due to fewer parameters. The paper established that architecture choice should depend on specific task characteristics and constraints (dataset size, sequence length, computational budget) rather than being a universal recommendation. It also demonstrated how to properly evaluate architectural innovations—not just showing one good result but systematic comparison across diverse tasks with statistical rigor. This methodology has become standard in deep learning research.</p>

<p><strong><a href="https://arxiv.org/abs/1506.02078">“Visualizing and Understanding Recurrent Networks” (2015)</a></strong><br />
<em>Authors</em>: Andrej Karpathy, Justin Johnson, Li Fei-Fei<br />
This paper investigated what LSTMs learn by analyzing their internal representations on character-level language modeling. By examining activations of individual hidden units and gates, Karpathy demonstrated that LSTMs spontaneously develop interpretable internal structure. Some cells track quote characters (activating inside quotes, deactivating outside), others track indentation levels in code, others detect line endings or comment blocks. The forget gates learn to reset at sentence boundaries. This emergent structure wasn’t explicitly programmed but arose from the training objective of predicting the next character. The paper’s methodology—systematic analysis of individual units, gate activations, and error patterns—established approaches for interpretability analysis that have since been applied to all types of neural networks. It showed that LSTMs don’t just achieve good performance through opaque computation but develop meaningful internal representations that we can understand and validate. This interpretability makes LSTMs valuable not just for their performance but for providing insight into what patterns the model has discovered in data.</p>

<h2 id="common-pitfalls-and-tricks">Common Pitfalls and Tricks</h2>

<p>The most common mistake when implementing LSTMs is initializing the forget gate bias to zero, like other biases. This causes the forget gate to start around 0.5 (from sigmoid of 0), meaning the network initially forgets half its cell state at each step. For most tasks, this aggressive forgetting early in training prevents the network from discovering that long-range dependencies matter. The solution is the “forget bias trick”: initialize \(\mathbf{b}_f = \mathbf{1}\) (a vector of ones). This makes the initial forget gate \(\sigma(0 + 1) \approx 0.73\), biasing toward retention. As training progresses, if forgetting is beneficial, the network can learn to reduce forget gate values. This simple initialization trick can mean the difference between an LSTM that trains successfully and one that never learns long-range dependencies.</p>

<p>Exploding gradients, while less problematic in LSTMs than vanilla RNNs due to the cell state dynamics, can still occur. The issue now typically comes from the gates themselves. If forget gate saturates at 1 and input gate allows large candidate values, the cell state can grow unboundedly: \(c_t = 1 \cdot c_{t-1} + 1 \cdot \tilde{c}_t\) repeated many times gives exponential growth. This manifests as parameters becoming NaN during training or loss exploding. The standard solution remains gradient clipping, but LSTM-specific solutions include:</p>
<ul>
  <li>Constraining candidate values through tanh (which LSTM already does)</li>
  <li>Using layer normalization to keep cell states in reasonable ranges</li>
  <li>Careful weight initialization to prevent gate saturation</li>
</ul>

<p>A subtle issue is the coupling between forget and input gates. In principle, these gates can learn conflicting behaviors—forget old information (\(f_t \approx 0\)) while not adding new (\(i_t \approx 0\)), causing the cell state to vanish. GRU avoids this by coupling them: \(1 - z_t\) keeps old, \(z_t\) adds new, guaranteeing at least one is substantial. Some LSTM variants also couple gates, though the standard LSTM allows them to be independent. In practice, proper initialization and sufficient training data usually allow LSTMs to learn sensible gate coordination, but when debugging LSTM training failures, checking for pathological gate behaviors (all gates near 0 or 1) can reveal issues.</p>

<p>The choice between LSTM and GRU has generated much discussion but few universal conclusions. As a practical heuristic: start with GRU because it’s simpler and faster. If performance plateaus and you have abundant data, try LSTM to see if its additional capacity helps. For very long sequences or complex temporal patterns, LSTM’s separate cell state often provides advantages. For tasks with limited data or where training time is constrained, GRU’s efficiency often makes it preferable. Always validate on your specific problem rather than assuming one architecture is universally better.</p>

<p>When stacking multiple LSTM layers, a common question is whether to apply dropout between layers. The answer: yes, but carefully. Apply dropout to the outputs (hidden states) passed between layers, not to the cell states or the recurrent connections within a layer. Typical dropout rates for LSTMs are lower than for feedforward networks—0.2 to 0.3 rather than 0.5—because LSTMs are already quite regularized through their gating mechanisms. Too much dropout can prevent LSTMs from learning the long-range dependencies they’re designed for, as the random dropping disrupts information flow through time.</p>

<p>Bidirectional LSTMs process sequences in both forward and backward directions, combining information from both at each time step: \(\mathbf{h}_t = [\overrightarrow{\mathbf{h}}_t; \overleftarrow{\mathbf{h}}_t]\). This doubles parameters and computation but provides richer representations when future context is available. However, bidirectional LSTMs can’t be used for real-time sequential prediction (where we must predict before seeing the complete sequence) or for autoregressive generation. They’re powerful for tasks like machine translation (where we have the complete source sentence) or speech recognition (where we can process the complete audio before transcribing), but inappropriate for online prediction or generation tasks.</p>

<p>A powerful technique for analysis and debugging is visualizing gate activations over time. Plot \(f_t\), \(i_t\), \(o_t\) for each dimension as the network processes a sequence. Patterns reveal what the network has learned: forget gates dropping at sentence boundaries, input gates opening for content words and closing for function words, output gates exposing information when decisions are needed. This visualization not only helps debug training issues but provides insight into what linguistic or sequential structure the network has discovered, making LSTMs more interpretable than many other deep learning architectures.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<p>Long Short-Term Memory networks solved the vanishing gradient problem that limited vanilla RNNs by introducing a cell state with gated connections that allow information to flow through time with minimal degradation. The architecture uses three gates—forget, input, and output—each implemented as sigmoid layers, to control what information is retained, added, or exposed at each time step. This gating mechanism enables learning dependencies spanning hundreds of time steps, making LSTMs successful for machine translation, speech recognition, and many other sequential tasks that require long-term memory. The cell state provides an additive update path where gradients can flow more easily than through the multiplicative, nonlinear updates of vanilla RNN hidden states. Gated Recurrent Units simplify LSTMs by using two gates instead of three and merging cell and hidden states, often achieving comparable performance with fewer parameters. The choice between LSTM and GRU depends on task complexity, data availability, and computational constraints, with GRU often being a good starting point due to its simplicity. Understanding LSTMs deeply means appreciating not just the equations but why each component exists—how gates enable learnable memory management, why the cell state uses additive updates, how these design choices enable gradient flow—and recognizing LSTMs as a solution to the specific challenge of learning long-range dependencies in sequential data through gradient-based optimization.</p>

<p>The LSTM’s success demonstrates that careful architectural design informed by understanding of gradient dynamics can overcome fundamental limitations, a lesson that has influenced neural architecture design far beyond recurrent networks.</p>

</div>

<!-- Back to Chapter Home Link -->

  
  
  <div style="margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #007bff;">
    <a href="/deep-learning-self-learning/contents/en/chapter06/" style="text-decoration: none; color: #007bff; font-weight: bold;">
      ← Back to Chapter Home
    </a>
  </div>













<div class="related">
  <ul class="related-posts">
    
      
        <li>
          <h2>Previous Post</h2>
          <h3>
            <a href="/deep-learning-self-learning/contents/en/chapter06/06_00_Introduction/">
              06 LSTM and GRU - Advanced Recurrent Architectures
            </a>
          </h3>
        </li>
      
    
      
    
    
    
  
    
  
  </ul>
</div>



<script src="https://utteranc.es/client.js"
        repo="convex-deep-learning-for-all/convex-deep-learning-for-all.github.io"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/deep-learning-self-learning/public/js/script.js'></script>
    <script src='/deep-learning-self-learning/public/js/multilang.js'></script>
    <script src='/deep-learning-self-learning/public/js/search.js'></script>
  </body>
</html>
