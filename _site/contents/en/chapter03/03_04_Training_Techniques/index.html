<!DOCTYPE html>
<html lang="en">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      03-04 Practical Training Techniques &middot; Deep learning in Data Science
    
  </title>

  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/poole.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/syntax.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/lanyon.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/github-markdown.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/multilang.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/search.css">
  <link rel="stylesheet" href="/deep-learning-self-learning/public/css/content-boxes.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap">
  
  <!-- Lunr.js for search functionality -->
  <script src="https://unpkg.com/lunr/lunr.js"></script>

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="http://localhost:4000/deep-learning-self-learning/public/logo.png">
  <link rel="shortcut icon" href="http://localhost:4000/deep-learning-self-learning/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/deep-learning-self-learning/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Deep learning in Data Science</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/">Home</a>

    

    
    
    
    <!-- Hiển thị các chương có sẵn cho ngôn ngữ hiện tại -->
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter00/">
              00. Basic Mathematical Concepts
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter01/">
              01. Introduction to Deep Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter02/">
              02. Neural Networks Fundamentals
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter03/">
              03. Training Neural Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter04/">
              04. Convolutional Neural Networks (CNNs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter05/">
              05. Recurrent Neural Networks (RNNs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter06/">
              06. LSTM and GRU Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter07/">
              07. Attention Mechanisms
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter08/">
              08. Transformers
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter09/">
              09. Regularization Techniques
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter10/">
              10. Deep Learning Algorithms
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter11/">
              11. Generative Models
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter12/">
              12. Autoencoders
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter13/">
              13. Variational Autoencoders (VAE)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter14/">
              14. Generative Adversarial Networks (GANs)
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter15/">
              15. Transfer Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter16/">
              16. Self-Supervised Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter17/">
              17. Computer Vision Applications
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter18/">
              18. Natural Language Processing
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter19/">
              19. Speech and Audio Processing
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter20/">
              20. Reinforcement Learning Basics
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter21/">
              21. Deep Reinforcement Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter22/">
              22. Graph Neural Networks
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter23/">
              23. Efficient Deep Learning
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter24/">
              24. Interpretability and Explainability
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/deep-learning-self-learning/contents/en/chapter25/">
              25. Advanced Topics and Future Directions
              
            </a>
          
        
      
    
    
    <!-- Nếu không có nội dung cho ngôn ngữ hiện tại, hiển thị thông báo -->
    
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2025. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/deep-learning-self-learning/" title="Home">Deep learning in Data Science</a>
            <small></small>
          </h3>
          <!-- Header Actions: Language Toggle and GitHub Link -->
          <div class="header-actions">
            <div class="language-toggle">
              <a href="/deep-learning-self-learning/contents/vi/chapter00/" class="language-switch" title="Switch to Vietnamese">Switch to Vietnamese</a>
            </div>
            <a class="github-logo__wrapper" target="_blank" href="https://github.com/nglelinh/deep-learning-self-learning" title="Github">
             <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
            </a>
          </div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">
    03-04 Practical Training Techniques
    
      
        <span class="lesson-badge required large">Required</span>
      
    
  </h1>
  <p>This lesson covers practical techniques and best practices for training neural networks effectively.</p>

<hr />

<h2 id="weight-initialization">Weight Initialization</h2>

<p>Proper initialization is crucial for successful training. Poor initialization can lead to vanishing/exploding gradients or slow convergence.</p>

<h3 id="bad-initialization-methods">Bad Initialization Methods</h3>

<h4 id="1-all-zeros">1. All Zeros</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_l</span><span class="p">,</span> <span class="n">n_l_prev</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>Problem</strong>: All neurons compute the same output and receive the same gradient → No learning!</p>

<h4 id="2-all-same-values">2. All Same Values</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">n_l</span><span class="p">,</span> <span class="n">n_l_prev</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.5</span>
</code></pre></div></div>

<p><strong>Problem</strong>: Same as zeros - breaks symmetry.</p>

<h3 id="good-initialization-methods">Good Initialization Methods</h3>

<h4 id="1-random-small-values">1. Random Small Values</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_l</span><span class="p">,</span> <span class="n">n_l_prev</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
</code></pre></div></div>

<p><strong>Pros</strong>: Breaks symmetry
<strong>Cons</strong>: May be too small for deep networks</p>

<h4 id="2-xavierglorot-initialization">2. Xavier/Glorot Initialization</h4>

<p>For <strong>sigmoid</strong> or <strong>tanh</strong> activations:</p>

\[\mathbf{W}^{[l]} \sim \mathcal{N}\left(0, \frac{1}{n^{[l-1]}}\right)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_l</span><span class="p">,</span> <span class="n">n_l_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_l_prev</span><span class="p">)</span>
</code></pre></div></div>

<p>or</p>

\[\mathbf{W}^{[l]} \sim \mathcal{N}\left(0, \frac{2}{n^{[l-1]} + n^{[l]}}\right)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_l</span><span class="p">,</span> <span class="n">n_l_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_l_prev</span> <span class="o">+</span> <span class="n">n_l</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>Rationale</strong>: Maintains variance of activations across layers.</p>

<h4 id="3-he-initialization">3. He Initialization</h4>

<p>For <strong>ReLU</strong> activations (most common):</p>

\[\mathbf{W}^{[l]} \sim \mathcal{N}\left(0, \frac{2}{n^{[l-1]}}\right)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_l</span><span class="p">,</span> <span class="n">n_l_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n_l_prev</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Why factor of 2?</strong> ReLU zeros out half the neurons on average.</p>

<h3 id="bias-initialization">Bias Initialization</h3>

<p>Biases can typically be initialized to zero:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_l</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="complete-initialization-function">Complete Initialization Function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">,</span> <span class="n">initialization_method</span><span class="o">=</span><span class="sh">'</span><span class="s">he</span><span class="sh">'</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Initialize network parameters
    
    layer_dims: list of layer sizes [n_x, n_h1, ..., n_y]
    initialization_method: </span><span class="sh">'</span><span class="s">zeros</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">random</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">xavier</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">he</span><span class="sh">'</span><span class="s">
    </span><span class="sh">"""</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">initialization_method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">zeros</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">W</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        
        <span class="k">elif</span> <span class="n">initialization_method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">random</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">W</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.01</span>
        
        <span class="k">elif</span> <span class="n">initialization_method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">xavier</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">W</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="k">elif</span> <span class="n">initialization_method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">he</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">W</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">b</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">parameters</span>
</code></pre></div></div>

<h2 id="data-preprocessing">Data Preprocessing</h2>

<h3 id="1-feature-scaling">1. Feature Scaling</h3>

<p>Normalize input features to similar ranges.</p>

<h4 id="standardization-z-score-normalization">Standardization (Z-score Normalization)</h4>

\[x_{\text{norm}} = \frac{x - \mu}{\sigma}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">standardize</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Standardize features to mean=0, std=1</span><span class="sh">"""</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">X_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>  <span class="c1"># Add epsilon to avoid division by zero
</span>    <span class="k">return</span> <span class="n">X_norm</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span>
</code></pre></div></div>

<h4 id="min-max-normalization">Min-Max Normalization</h4>

\[x_{\text{norm}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">min_max_normalize</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Scale features to [0, 1]</span><span class="sh">"""</span>
    <span class="n">x_min</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">x_max</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">X_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">x_min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">x_max</span> <span class="o">-</span> <span class="n">x_min</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X_norm</span><span class="p">,</span> <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span>
</code></pre></div></div>

<p><strong>When to use what:</strong></p>
<ul>
  <li><strong>Standardization</strong>: When features are normally distributed or have outliers</li>
  <li><strong>Min-Max</strong>: When you need features in a specific range (e.g., [0, 1])</li>
</ul>

<h3 id="2-data-shuffling">2. Data Shuffling</h3>

<p>Shuffle training data before each epoch to prevent learning order-dependent patterns.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">shuffle_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Shuffle training data</span><span class="sh">"""</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">permutation</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">permutation</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">X_shuffled</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">permutation</span><span class="p">]</span>
    <span class="n">Y_shuffled</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">permutation</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X_shuffled</span><span class="p">,</span> <span class="n">Y_shuffled</span>
</code></pre></div></div>

<h2 id="batch-processing">Batch Processing</h2>

<h3 id="creating-mini-batches">Creating Mini-Batches</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_mini_batches</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Create list of mini-batches
    
    X: (n_x, m)
    Y: (n_y, m)
    batch_size: size of each mini-batch
    
    Returns: list of (X_batch, Y_batch) tuples
    </span><span class="sh">"""</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">mini_batches</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Shuffle data
</span>    <span class="n">X_shuffled</span><span class="p">,</span> <span class="n">Y_shuffled</span> <span class="o">=</span> <span class="nf">shuffle_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    
    <span class="c1"># Partition
</span>    <span class="n">num_complete_batches</span> <span class="o">=</span> <span class="n">m</span> <span class="o">//</span> <span class="n">batch_size</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_complete_batches</span><span class="p">):</span>
        <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_shuffled</span><span class="p">[:,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
        <span class="n">Y_batch</span> <span class="o">=</span> <span class="n">Y_shuffled</span><span class="p">[:,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
        <span class="n">mini_batches</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">X_batch</span><span class="p">,</span> <span class="n">Y_batch</span><span class="p">))</span>
    
    <span class="c1"># Handle remaining examples (if m is not divisible by batch_size)
</span>    <span class="k">if</span> <span class="n">m</span> <span class="o">%</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_shuffled</span><span class="p">[:,</span> <span class="n">num_complete_batches</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:]</span>
        <span class="n">Y_batch</span> <span class="o">=</span> <span class="n">Y_shuffled</span><span class="p">[:,</span> <span class="n">num_complete_batches</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:]</span>
        <span class="n">mini_batches</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">X_batch</span><span class="p">,</span> <span class="n">Y_batch</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">mini_batches</span>
</code></pre></div></div>

<h2 id="trainvalidationtest-split">Train/Validation/Test Split</h2>

<h3 id="why-three-sets">Why Three Sets?</h3>

<ul>
  <li><strong>Training set</strong>: Learn parameters</li>
  <li><strong>Validation set</strong>: Tune hyperparameters, monitor overfitting</li>
  <li><strong>Test set</strong>: Final evaluation (use only once!)</li>
</ul>

<h3 id="typical-splits">Typical Splits</h3>

<p><strong>Small dataset (&lt; 10,000 examples):</strong></p>
<ul>
  <li>Train: 60%, Val: 20%, Test: 20%</li>
</ul>

<p><strong>Medium dataset (10,000 - 1,000,000):</strong></p>
<ul>
  <li>Train: 80%, Val: 10%, Test: 10%</li>
</ul>

<p><strong>Large dataset (&gt; 1,000,000):</strong></p>
<ul>
  <li>Train: 98%, Val: 1%, Test: 1%</li>
</ul>

<h3 id="implementation">Implementation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_val_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">train_ratio</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">val_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Split data into train, validation, and test sets
    </span><span class="sh">"""</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># Shuffle first
</span>    <span class="n">X_shuffled</span><span class="p">,</span> <span class="n">Y_shuffled</span> <span class="o">=</span> <span class="nf">shuffle_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    
    <span class="c1"># Calculate split indices
</span>    <span class="n">train_end</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">train_ratio</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>
    <span class="n">val_end</span> <span class="o">=</span> <span class="n">train_end</span> <span class="o">+</span> <span class="nf">int</span><span class="p">(</span><span class="n">val_ratio</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>
    
    <span class="c1"># Split
</span>    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_shuffled</span><span class="p">[:,</span> <span class="p">:</span><span class="n">train_end</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">Y_shuffled</span><span class="p">[:,</span> <span class="p">:</span><span class="n">train_end</span><span class="p">]</span>
    
    <span class="n">X_val</span> <span class="o">=</span> <span class="n">X_shuffled</span><span class="p">[:,</span> <span class="n">train_end</span><span class="p">:</span><span class="n">val_end</span><span class="p">]</span>
    <span class="n">Y_val</span> <span class="o">=</span> <span class="n">Y_shuffled</span><span class="p">[:,</span> <span class="n">train_end</span><span class="p">:</span><span class="n">val_end</span><span class="p">]</span>
    
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_shuffled</span><span class="p">[:,</span> <span class="n">val_end</span><span class="p">:]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_shuffled</span><span class="p">[:,</span> <span class="n">val_end</span><span class="p">:]</span>
    
    <span class="nf">return </span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="monitoring-training">Monitoring Training</h2>

<h3 id="metrics-to-track">Metrics to Track</h3>

<ol>
  <li><strong>Training Loss</strong>: Should decrease steadily</li>
  <li><strong>Validation Loss</strong>: Should decrease; if it increases, overfitting!</li>
  <li><strong>Training Accuracy</strong>: Should increase</li>
  <li><strong>Validation Accuracy</strong>: Should increase; gap with training accuracy indicates overfitting</li>
</ol>

<h3 id="visualization">Visualization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">plot_training_history</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">train_accs</span><span class="p">,</span> <span class="n">val_accs</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Plot training history</span><span class="sh">"""</span>
    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    
    <span class="c1"># Loss plot
</span>    <span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Training Loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">val_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Validation Loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax1</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax1</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Training and Validation Loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax1</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
    <span class="n">ax1</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Accuracy plot
</span>    <span class="n">ax2</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">train_accs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Training Accuracy</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax2</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">val_accs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Validation Accuracy</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax2</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax2</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax2</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Training and Validation Accuracy</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax2</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
    <span class="n">ax2</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="early-stopping">Early Stopping</h2>

<p>Stop training when validation loss stops improving.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EarlyStopping</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_delta</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        patience: number of epochs to wait before stopping
        min_delta: minimum change to qualify as improvement
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span>
        <span class="n">self</span><span class="p">.</span><span class="n">min_delta</span> <span class="o">=</span> <span class="n">min_delta</span>
        <span class="n">self</span><span class="p">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">early_stop</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">self</span><span class="p">.</span><span class="n">best_parameters</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">best_loss</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="n">self</span><span class="p">.</span><span class="n">best_parameters</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">val_loss</span> <span class="o">&gt;</span> <span class="n">self</span><span class="p">.</span><span class="n">best_loss</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">min_delta</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">EarlyStopping counter: </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">counter</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">patience</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">counter</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">patience</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="n">early_stop</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="n">self</span><span class="p">.</span><span class="n">best_parameters</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">early_stop</span>

<span class="c1"># Usage in training loop
</span><span class="n">early_stopping</span> <span class="o">=</span> <span class="nc">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="c1"># Training...
</span>    <span class="n">train_loss</span> <span class="o">=</span> <span class="nf">train_one_epoch</span><span class="p">(...)</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="nf">validate</span><span class="p">(...)</span>
    
    <span class="k">if</span> <span class="nf">early_stopping</span><span class="p">(</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Early stopping triggered!</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">early_stopping</span><span class="p">.</span><span class="n">best_parameters</span>
        <span class="k">break</span>
</code></pre></div></div>

<h2 id="gradient-clipping">Gradient Clipping</h2>

<p>Prevent exploding gradients by clipping gradient magnitudes.</p>

<h3 id="clip-by-value">Clip by Value</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">clip_gradients_by_value</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">5.0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Clip gradients to [-max_value, max_value]</span><span class="sh">"""</span>
    <span class="n">clipped_gradients</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
        <span class="n">clipped_gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="o">-</span><span class="n">max_value</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">clipped_gradients</span>
</code></pre></div></div>

<h3 id="clip-by-norm">Clip by Norm</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">clip_gradients_by_norm</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Clip gradients by global norm</span><span class="sh">"""</span>
    <span class="c1"># Compute global norm
</span>    <span class="n">total_norm</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">.</span><span class="nf">values</span><span class="p">():</span>
        <span class="n">total_norm</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">grad</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">total_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">total_norm</span><span class="p">)</span>
    
    <span class="c1"># Clip if necessary
</span>    <span class="n">clip_coef</span> <span class="o">=</span> <span class="n">max_norm</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_norm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">clip_coef</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">clipped_gradients</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">clipped_gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">clip_coef</span>
        <span class="k">return</span> <span class="n">clipped_gradients</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">gradients</span>
</code></pre></div></div>

<h2 id="complete-training-loop">Complete Training Loop</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span> <span class="n">layer_dims</span><span class="p">,</span> 
                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                <span class="n">initialization</span><span class="o">=</span><span class="sh">'</span><span class="s">he</span><span class="sh">'</span><span class="p">,</span> <span class="n">early_stopping_patience</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Complete training function with all best practices
    </span><span class="sh">"""</span>
    <span class="c1"># Initialize
</span>    <span class="n">parameters</span> <span class="o">=</span> <span class="nf">initialize_parameters</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">,</span> <span class="n">initialization</span><span class="p">)</span>
    <span class="n">early_stopping</span> <span class="o">=</span> <span class="nc">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="n">early_stopping_patience</span><span class="p">)</span>
    
    <span class="c1"># History
</span>    <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_accs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">val_accs</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Training loop
</span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="c1"># Create mini-batches
</span>        <span class="n">mini_batches</span> <span class="o">=</span> <span class="nf">create_mini_batches</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="c1"># Process each mini-batch
</span>        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">Y_batch</span> <span class="ow">in</span> <span class="n">mini_batches</span><span class="p">:</span>
            <span class="c1"># Forward propagation
</span>            <span class="n">AL</span><span class="p">,</span> <span class="n">caches</span> <span class="o">=</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X_batch</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
            
            <span class="c1"># Compute cost
</span>            <span class="n">batch_cost</span> <span class="o">=</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y_batch</span><span class="p">)</span>
            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">batch_cost</span>
            
            <span class="c1"># Backward propagation
</span>            <span class="n">gradients</span> <span class="o">=</span> <span class="nf">backward_propagation</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y_batch</span><span class="p">,</span> <span class="n">caches</span><span class="p">)</span>
            
            <span class="c1"># Gradient clipping (optional)
</span>            <span class="n">gradients</span> <span class="o">=</span> <span class="nf">clip_gradients_by_norm</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
            
            <span class="c1"># Update parameters
</span>            <span class="n">parameters</span> <span class="o">=</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        
        <span class="c1"># Average loss over all batches
</span>        <span class="n">epoch_loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">mini_batches</span><span class="p">)</span>
        <span class="n">train_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
        
        <span class="c1"># Compute training accuracy
</span>        <span class="n">train_predictions</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        <span class="n">train_acc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">train_predictions</span> <span class="o">==</span> <span class="n">Y_train</span><span class="p">)</span>
        <span class="n">train_accs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
        
        <span class="c1"># Validation
</span>        <span class="n">val_predictions</span><span class="p">,</span> <span class="n">val_caches</span> <span class="o">=</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">val_predictions</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)</span>
        <span class="n">val_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
        
        <span class="n">val_pred_labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">val_predictions</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">val_acc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">val_pred_labels</span> <span class="o">==</span> <span class="n">Y_val</span><span class="p">)</span>
        <span class="n">val_accs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">val_acc</span><span class="p">)</span>
        
        <span class="c1"># Print progress
</span>        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">num_epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s">: </span><span class="sh">"</span>
                  <span class="sa">f</span><span class="sh">"</span><span class="s">Train Loss: </span><span class="si">{</span><span class="n">epoch_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Train Acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, </span><span class="sh">"</span>
                  <span class="sa">f</span><span class="sh">"</span><span class="s">Val Loss: </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Val Acc: </span><span class="si">{</span><span class="n">val_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># Early stopping
</span>        <span class="k">if</span> <span class="nf">early_stopping</span><span class="p">(</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Early stopping at epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">parameters</span> <span class="o">=</span> <span class="n">early_stopping</span><span class="p">.</span><span class="n">best_parameters</span>
            <span class="k">break</span>
    
    <span class="c1"># Plot history
</span>    <span class="nf">plot_training_history</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">train_accs</span><span class="p">,</span> <span class="n">val_accs</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">parameters</span><span class="p">,</span> <span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">train_accs</span><span class="p">,</span> <span class="n">val_accs</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2>

<h3 id="key-hyperparameters">Key Hyperparameters</h3>

<ol>
  <li><strong>Learning rate</strong> (most important)</li>
  <li><strong>Batch size</strong></li>
  <li><strong>Number of layers</strong></li>
  <li><strong>Number of neurons per layer</strong></li>
  <li><strong>Activation functions</strong></li>
  <li><strong>Initialization method</strong></li>
</ol>

<h3 id="tuning-strategies">Tuning Strategies</h3>

<h4 id="1-manual-search">1. Manual Search</h4>

<p>Try different values based on intuition and experience.</p>

<h4 id="2-grid-search">2. Grid Search</h4>

<p>Try all combinations of a predefined set of values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>

<span class="n">best_val_acc</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">best_params</span> <span class="o">=</span> <span class="bp">None</span>

<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">bs</span> <span class="ow">in</span> <span class="n">batch_sizes</span><span class="p">:</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span> 
                                     <span class="n">layer_dims</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">)</span>
        <span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Last validation accuracy
</span>        
        <span class="k">if</span> <span class="n">val_acc</span> <span class="o">&gt;</span> <span class="n">best_val_acc</span><span class="p">:</span>
            <span class="n">best_val_acc</span> <span class="o">=</span> <span class="n">val_acc</span>
            <span class="n">best_params</span> <span class="o">=</span> <span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Best: LR=</span><span class="si">{</span><span class="n">best_params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">, BS=</span><span class="si">{</span><span class="n">best_params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">, Val Acc=</span><span class="si">{</span><span class="n">best_val_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="3-random-search">3. Random Search</h4>

<p>Often more efficient than grid search.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>

<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">best_val_acc</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_trials</span><span class="p">):</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Log-uniform between 0.0001 and 0.1
</span>    <span class="n">bs</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
    
    <span class="n">model</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span>
                                 <span class="n">layer_dims</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">if</span> <span class="n">val_acc</span> <span class="o">&gt;</span> <span class="n">best_val_acc</span><span class="p">:</span>
        <span class="n">best_val_acc</span> <span class="o">=</span> <span class="n">val_acc</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">New best: LR=</span><span class="si">{</span><span class="n">lr</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s">, BS=</span><span class="si">{</span><span class="n">bs</span><span class="si">}</span><span class="s">, Val Acc=</span><span class="si">{</span><span class="n">val_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="summary">Summary</h2>

<ul>
  <li><strong>Proper initialization</strong> (He for ReLU) prevents training issues</li>
  <li><strong>Data preprocessing</strong> (standardization/normalization) improves convergence</li>
  <li><strong>Mini-batch processing</strong> balances speed and stability</li>
  <li><strong>Train/Val/Test split</strong> enables proper evaluation</li>
  <li><strong>Monitoring</strong> training/validation metrics detects overfitting</li>
  <li><strong>Early stopping</strong> prevents overfitting and saves time</li>
  <li><strong>Gradient clipping</strong> prevents exploding gradients</li>
  <li><strong>Hyperparameter tuning</strong> is essential for optimal performance</li>
</ul>

<p>With these techniques, you’re equipped to train neural networks effectively. The next chapter covers Convolutional Neural Networks for computer vision tasks!</p>


</div>

<!-- Back to Chapter Home Link -->

  
  
  <div style="margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #007bff;">
    <a href="/deep-learning-self-learning/contents/en/chapter03/" style="text-decoration: none; color: #007bff; font-weight: bold;">
      ← Back to Chapter Home
    </a>
  </div>













<div class="related">
  <ul class="related-posts">
    
      
    
      
    
      
    
      
        <li>
          <h2>Previous Post</h2>
          <h3>
            <a href="/deep-learning-self-learning/contents/en/chapter03/03_03_Backpropagation/">
              03-03 Backpropagation Algorithm
            </a>
          </h3>
        </li>
      
    
      
    
    
    
  
    
  
    
  
    
  
    
  
  </ul>
</div>



<script src="https://utteranc.es/client.js"
        repo="convex-deep-learning-for-all/convex-deep-learning-for-all.github.io"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/deep-learning-self-learning/public/js/script.js'></script>
    <script src='/deep-learning-self-learning/public/js/multilang.js'></script>
    <script src='/deep-learning-self-learning/public/js/search.js'></script>
  </body>
</html>
