[
  {
    "id": "/contents/vi/chapter00/00_Introduction",
    "title": "00 Giá»›i thiá»‡u",
    "chapter": "00",
    "order": 1,
    "owner": "GitHub Copilot",
    "lesson_type": "",
    "content": "Tá»‘i Æ°u hÃ³a lÃ  trÃ¡i tim cá»§a khoa há»c dá»¯ liá»‡u. DÃ¹ báº¡n Ä‘ang huáº¥n luyá»‡n má»™t máº¡ng nÆ¡-ron, tá»‘i thiá»ƒu hÃ³a lá»—i trong cÃ¡c mÃ´ hÃ¬nh há»“i quy, hay phÃ¢n bá»• tÃ i nguyÃªn hiá»‡u quáº£ trong há»‡ thá»‘ng gá»£i Ã½, vá» báº£n cháº¥t báº¡n Ä‘ang giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n tÃ¬m kiáº¿m giáº£i phÃ¡p \"tá»‘t nháº¥t\" tá»« má»™t táº­p há»£p khá»•ng lá»“ cÃ¡c kháº£ nÄƒng. NhÆ°ng Ä‘á»ƒ lÃ m Ä‘iá»u nÃ y má»™t cÃ¡ch hiá»‡u quáº£, báº¡n cáº§n nÃ³i Ä‘Æ°á»£c ngÃ´n ngá»¯ cá»§a toÃ¡n há»c. ChÃºng ta sáº½ Ã´n láº¡i cÃ¡c Ã½ tÆ°á»Ÿng chÃ­nh tá»« Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh, lÃ½ thuyáº¿t táº­p há»£p vÃ  giáº£i tÃ­ch, Ä‘áº£m báº£o báº¡n Ä‘Æ°á»£c trang bá»‹ Ä‘á»ƒ xá»­ lÃ½ cÃ¡c gradient, ma tráº­n, rÃ ng buá»™c vÃ  sá»± báº¥t Ä‘á»‹nh phÃ¡t sinh trong cÃ¡c tÃ¡c vá»¥ tá»‘i Æ°u hÃ³a.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_Introduction/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_01_Calculus",
    "title": "00-01 Giáº£i tÃ­ch",
    "chapter": "00",
    "order": 2,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y bao gá»“m cÃ¡c khÃ¡i niá»‡m giáº£i tÃ­ch cáº§n thiáº¿t cho tá»‘i Æ°u hÃ³a, Ä‘Æ°á»£c tá»• chá»©c thÃ nh bá»‘n pháº§n chÃ­nh Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_01_Calculus/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_01_01_Continuity_and_Uniform_Continuity",
    "title": "00-01-01 TÃ­nh liÃªn tá»¥c vÃ  TÃ­nh liÃªn tá»¥c Ä‘á»u",
    "chapter": "00",
    "order": 3,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y giá»›i thiá»‡u cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n vá» tÃ­nh liÃªn tá»¥c vÃ  tÃ­nh liÃªn tá»¥c Ä‘á»u, nhá»¯ng khÃ¡i niá»‡m quan trá»ng Ä‘á»ƒ hiá»ƒu hÃ nh vi cá»§a cÃ¡c hÃ m sá»‘ trong tá»‘i Æ°u hÃ³a. --- TÃ­nh liÃªn tá»¥c vÃ  TÃ­nh liÃªn tá»¥c Ä‘á»u TÃ­nh liÃªn tá»¥c vÃ  TÃ­nh liÃªn tá»¥c Ä‘á»u lÃ  nhá»¯ng khÃ¡i niá»‡m cÆ¡ báº£n mÃ´ táº£ hÃ nh vi cá»§a cÃ¡c hÃ m sá»‘, Ä‘áº·c biá»‡t liÃªn quan Ä‘áº¿n tÃ­nh \"mÆ°á»£t mÃ \" hoáº·c \"cÃ³ thá»ƒ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c\" cá»§a chÃºng. Máº·c dÃ¹ cÃ³ liÃªn quan cháº·t cháº½, chÃºng thá»ƒ hiá»‡n cÃ¡c tÃ­nh cháº¥t khÃ¡c biá»‡t, vá»›i tÃ­nh liÃªn tá»¥c Ä‘á»u lÃ  Ä‘iá»u kiá»‡n máº¡nh hÆ¡n so vá»›i tÃ­nh liÃªn tá»¥c thÃ´ng thÆ°á»ng. Äá»‹nh nghÄ©a TÃ­nh liÃªn tá»¥c Má»™t hÃ m sá»‘ MATH Ä‘Æ°á»£c gá»i lÃ  liÃªn tá»¥c táº¡i má»™t Ä‘iá»ƒm MATH náº¿u, vá»›i má»i sá»‘ thá»±c dÆ°Æ¡ng MATH , tá»“n táº¡i má»™t sá»‘ thá»±c dÆ°Æ¡ng MATH sao cho vá»›i má»i MATH , náº¿u MATH , tá»“n táº¡i má»™t sá»‘ thá»±c dÆ°Æ¡ng MATH sao cho vá»›i má»i MATH , náº¿u MATH . 3. Kháº£ vi háº§u kháº¯p nÆ¡i : CÃ¡c hÃ m liÃªn tá»¥c Lipschitz kháº£ vi háº§u kháº¯p nÆ¡i, vÃ  táº¡i nÆ¡i Ä‘áº¡o hÃ m tá»“n táº¡i, MATH . VÃ­ dá»¥: - MATH lÃ  1-Lipschitz trÃªn MATH - MATH lÃ  1-Lipschitz trÃªn MATH vÃ¬ MATH - MATH khÃ´ng pháº£i Lipschitz trÃªn MATH nhÆ°ng lÃ  Lipschitz trÃªn báº¥t ká»³ khoáº£ng bá»‹ cháº·n nÃ o Sá»± khÃ¡c biá»‡t chÃ­nh vÃ  Thá»© báº­c Ba loáº¡i tÃ­nh liÃªn tá»¥c táº¡o thÃ nh má»™t thá»© báº­c cá»§a cÃ¡c Ä‘iá»u kiá»‡n ngÃ y cÃ ng máº¡nh: TÃ­nh liÃªn tá»¥c âŠ† TÃ­nh liÃªn tá»¥c Ä‘á»u âŠ† TÃ­nh liÃªn tá»¥c Lipschitz 1. Theo Ä‘iá»ƒm so vá»›i ToÃ n cá»¥c : - TÃ­nh liÃªn tá»¥c : TÃ­nh cháº¥t cá»¥c bá»™ kiá»ƒm tra táº¡i má»—i Ä‘iá»ƒm - TÃ­nh liÃªn tá»¥c Ä‘á»u : TÃ­nh cháº¥t toÃ n cá»¥c cá»§a toÃ n bá»™ hÃ m sá»‘ - TÃ­nh liÃªn tá»¥c Lipschitz : TÃ­nh cháº¥t toÃ n cá»¥c vá»›i cÃ¡c rÃ ng buá»™c Ä‘á»‹nh lÆ°á»£ng 2. Lá»±a chá»n MATH : - TÃ­nh liÃªn tá»¥c : MATH cÃ³ thá»ƒ phá»¥ thuá»™c vÃ o cáº£ MATH vÃ  Ä‘iá»ƒm cá»¥ thá»ƒ MATH - TÃ­nh liÃªn tá»¥c Ä‘á»u : MATH chá»‰ phá»¥ thuá»™c vÃ o MATH , hoáº¡t Ä‘á»™ng cho táº¥t cáº£ cÃ¡c Ä‘iá»ƒm Ä‘á»“ng thá»i - TÃ­nh liÃªn tá»¥c Lipschitz : MATH cung cáº¥p má»‘i quan há»‡ rÃµ rÃ ng 3. Kiá»ƒm soÃ¡t Tá»‘c Ä‘á»™ Thay Ä‘á»•i : - TÃ­nh liÃªn tá»¥c : KhÃ´ng kiá»ƒm soÃ¡t tá»‘c Ä‘á»™ thay Ä‘á»•i - TÃ­nh liÃªn tá»¥c Ä‘á»u : Äáº£m báº£o biáº¿n thiÃªn bá»‹ cháº·n trÃªn cÃ¡c khoáº£ng nhá» - TÃ­nh liÃªn tá»¥c Lipschitz : Cung cáº¥p rÃ ng buá»™c tuyáº¿n tÃ­nh rÃµ rÃ ng cho tá»‘c Ä‘á»™ thay Ä‘á»•i 4. Má»‘i quan há»‡ Äá»™ máº¡nh : - Má»i hÃ m liÃªn tá»¥c Lipschitz Ä‘á»u lÃ  liÃªn tá»¥c Ä‘á»u - Má»i hÃ m liÃªn tá»¥c Ä‘á»u Ä‘á»u lÃ  liÃªn tá»¥c - CÃ¡c má»‡nh Ä‘á» ngÆ°á»£c láº¡i nÃ³i chung khÃ´ng Ä‘Ãºng VÃ­ dá»¥ Chi tiáº¿t vÃ  So sÃ¡nh VÃ­ dá»¥ 1: MATH - TrÃªn MATH : LiÃªn tá»¥c nhÆ°ng khÃ´ng liÃªn tá»¥c Ä‘á»u tá»‘c Ä‘á»™ thay Ä‘á»•i MATH khÃ´ng bá»‹ cháº·n - TrÃªn MATH : LiÃªn tá»¥c, liÃªn tá»¥c Ä‘á»u, vÃ  Lipschitz vá»›i MATH VÃ­ dá»¥ 2: MATH - TrÃªn MATH : LiÃªn tá»¥c, liÃªn tá»¥c Ä‘á»u, vÃ  1-Lipschitz vÃ¬ MATH VÃ­ dá»¥ 3: MATH - TrÃªn MATH : LiÃªn tá»¥c, liÃªn tá»¥c Ä‘á»u, vÃ  1-Lipschitz - LÆ°u Ã½ : KhÃ´ng kháº£ vi táº¡i MATH , nhÆ°ng váº«n lÃ  Lipschitz VÃ­ dá»¥ 4: MATH - TrÃªn MATH : LiÃªn tá»¥c vÃ  liÃªn tá»¥c Ä‘á»u, nhÆ°ng khÃ´ng pháº£i Lipschitz Ä‘áº¡o hÃ m khÃ´ng bá»‹ cháº·n gáº§n MATH - TrÃªn MATH vá»›i MATH : Lipschitz vá»›i MATH",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_01_01_Continuity_and_Uniform_Continuity/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_01_02_Derivatives_and_Multivariable_Calculus",
    "title": "00-01-02 Äáº¡o hÃ m vÃ  Giáº£i tÃ­ch Ä‘a biáº¿n",
    "chapter": "00",
    "order": 4,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y bao gá»“m Ä‘áº¡o hÃ m vÃ  cÃ¡c khÃ¡i niá»‡m giáº£i tÃ­ch Ä‘a biáº¿n thiáº¿t yáº¿u táº¡o ná»n táº£ng cho lÃ½ thuyáº¿t vÃ  thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a. --- Äáº¡o hÃ m vÃ  Tá»‘c Ä‘á»™ Thay Ä‘á»•i Äáº¡o hÃ m cá»§a má»™t hÃ m má»™t biáº¿n thá»ƒ hiá»‡n tá»‘c Ä‘á»™ thay Ä‘á»•i tá»©c thá»i cá»§a nÃ³, Ä‘iá»u nÃ y ráº¥t cÆ¡ báº£n Ä‘á»ƒ hiá»ƒu cÃ¡ch cÃ¡c hÃ m sá»‘ hoáº¡t Ä‘á»™ng cá»¥c bá»™. CÃ¡c KhÃ¡i niá»‡m Äáº¡o hÃ m CÆ¡ báº£n Äá»™ dá»‘c giá»¯a hai Ä‘iá»ƒm: MATH Äáº¡o hÃ m tá»‘c Ä‘á»™ thay Ä‘á»•i tá»©c thá»i : MATH Äáº¡o hÃ m cho chÃºng ta biáº¿t hÃ m sá»‘ thay Ä‘á»•i nhanh nhÆ° tháº¿ nÃ o táº¡i báº¥t ká»³ Ä‘iá»ƒm nÃ o, Ä‘iá»u nÃ y ráº¥t quan trá»ng Ä‘á»ƒ tÃ¬m cÃ¡c Ä‘iá»ƒm tá»‘i Æ°u nÆ¡i tá»‘c Ä‘á»™ thay Ä‘á»•i báº±ng khÃ´ng. ÄÆ°á»ng má»©c cá»§a HÃ m sá»‘ ÄÆ°á»ng má»©c lÃ  má»™t khÃ¡i niá»‡m cÆ¡ báº£n trong giáº£i tÃ­ch Ä‘a biáº¿n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ trá»±c quan hÃ³a cÃ¡c hÃ m hai biáº¿n, thÆ°á»ng Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  MATH . ChÃºng cung cáº¥p cÃ¡ch biá»ƒu diá»…n má»™t bá» máº·t 3D trong máº·t pháº³ng 2D. Má»™t Ä‘Æ°á»ng má»©c cá»§a hÃ m sá»‘ MATH lÃ  táº­p há»£p táº¥t cáº£ cÃ¡c Ä‘iá»ƒm MATH trong miá»n xÃ¡c Ä‘á»‹nh cá»§a MATH nÆ¡i hÃ m sá»‘ nháº­n giÃ¡ trá»‹ háº±ng sá»‘: MATH VÃ­ dá»¥: - Vá»›i MATH , cÃ¡c Ä‘Æ°á»ng má»©c lÃ  cÃ¡c hÃ¬nh trÃ²n: MATH - Vá»›i MATH , cÃ¡c Ä‘Æ°á»ng má»©c lÃ  cÃ¡c Ä‘Æ°á»ng tháº³ng song song: MATH ÄÆ°á»ng má»©c giÃºp chÃºng ta hiá»ƒu: 1. Äá»‹a hÃ¬nh cá»§a hÃ m sá»‘ 2. HÆ°á»›ng tÄƒng vÃ  giáº£m dá»‘c nháº¥t 3. Vá»‹ trÃ­ cá»§a cÃ¡c Ä‘iá»ƒm tá»‘i Æ°u tiá»m nÄƒng --- CÃ¡c KhÃ¡i niá»‡m ChÃ­nh cá»§a Giáº£i tÃ­ch Äa biáº¿n Äáº¡o hÃ m RiÃªng Vá»›i má»™t hÃ m sá»‘ MATH , Ä‘áº¡o hÃ m riÃªng theo MATH lÃ : MATH Äiá»u nÃ y Ä‘o lÆ°á»ng cÃ¡ch MATH thay Ä‘á»•i khi chá»‰ cÃ³ MATH biáº¿n thiÃªn trong khi táº¥t cáº£ cÃ¡c biáº¿n khÃ¡c giá»¯ cá»‘ Ä‘á»‹nh. Vector Gradient Gradient lÃ  má»™t vector gá»“m táº¥t cáº£ cÃ¡c Ä‘áº¡o hÃ m riÃªng: MATH Gradient chá»‰ theo hÆ°á»›ng tÄƒng dá»‘c nháº¥t cá»§a hÃ m sá»‘ vÃ  vuÃ´ng gÃ³c vá»›i cÃ¡c Ä‘Æ°á»ng má»©c. Ma tráº­n Hessian Ma tráº­n Hessian chá»©a táº¥t cáº£ cÃ¡c Ä‘áº¡o hÃ m riÃªng báº­c hai: MATH \\nabla^2 f \\mathbf x = \\mathbf H = \\begin pmatrix \\frac \\partial^2 f \\partial x 1^2 & \\frac \\partial^2 f \\partial x 1 \\partial x 2 & \\cdots & \\frac \\partial^2 f \\partial x 1 \\partial x n \\\\ \\frac \\partial^2 f \\partial x 2 \\partial x 1 & \\frac \\partial^2 f \\partial x 2^2 & \\cdots & \\frac \\partial^2 f \\partial x 2 \\partial x n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac \\partial^2 f \\partial x n \\partial x 1 & \\frac \\partial^2 f \\partial x n \\partial x 2 & \\cdots & \\frac \\partial^2 f \\partial x n^2 \\end pmatrix MATH Hessian cung cáº¥p thÃ´ng tin vá» Ä‘á»™ cong cá»§a hÃ m sá»‘ vÃ  ráº¥t quan trá»ng cho: - XÃ¡c Ä‘á»‹nh báº£n cháº¥t cá»§a cÃ¡c Ä‘iá»ƒm tá»›i háº¡n cá»±c tiá»ƒu, cá»±c Ä‘áº¡i, hoáº·c Ä‘iá»ƒm yÃªn ngá»±a - CÃ¡c phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a báº­c hai nhÆ° phÆ°Æ¡ng phÃ¡p Newton --- Quy táº¯c DÃ¢y chuyá»n cho HÃ m Äa biáº¿n Quy táº¯c dÃ¢y chuyá»n lÃ  cÆ¡ báº£n Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m cá»§a cÃ¡c hÃ m há»£p thÃ nh, thÆ°á»ng xuáº¥t hiá»‡n trong cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a. Quy táº¯c DÃ¢y chuyá»n CÆ¡ báº£n Vá»›i hÃ m sá»‘ MATH nÆ¡i MATH vÃ  MATH : MATH Quy táº¯c DÃ¢y chuyá»n Tá»•ng quÃ¡t Vá»›i MATH nÆ¡i má»—i MATH : MATH á»¨ng dá»¥ng trong Tá»‘i Æ°u hÃ³a Quy táº¯c dÃ¢y chuyá»n ráº¥t thiáº¿t yáº¿u cho: 1. TÃ­nh toÃ¡n Gradient : TÃ­nh gradient cá»§a cÃ¡c hÃ m má»¥c tiÃªu há»£p thÃ nh 2. Xá»­ lÃ½ RÃ ng buá»™c : Xá»­ lÃ½ cÃ¡c rÃ ng buá»™c lÃ  hÃ m cá»§a cÃ¡c biáº¿n khÃ¡c 3. Triá»ƒn khai Thuáº­t toÃ¡n : Lan truyá»n ngÆ°á»£c trong máº¡ng nÆ¡-ron vÃ  vi phÃ¢n tá»± Ä‘á»™ng 4. PhÃ¢n tÃ­ch Äá»™ nháº¡y : Hiá»ƒu cÃ¡ch thay Ä‘á»•i tham sá»‘ áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡c nghiá»‡m tá»‘i Æ°u VÃ­ dá»¥: Tá»‘i Æ°u hÃ³a vá»›i RÃ ng buá»™c Xem xÃ©t viá»‡c tá»‘i thiá»ƒu hÃ³a MATH vá»›i Ä‘iá»u kiá»‡n MATH . Sá»­ dá»¥ng rÃ ng buá»™c Ä‘á»ƒ loáº¡i bá» má»™t biáº¿n: MATH , váº­y chÃºng ta tá»‘i thiá»ƒu hÃ³a: MATH Sá»­ dá»¥ng quy táº¯c dÃ¢y chuyá»n: MATH Äáº·t MATH cho MATH , váº­y Ä‘iá»ƒm tá»‘i Æ°u lÃ  MATH . Äiá»u nÃ y minh há»a cÃ¡ch cÃ¡c khÃ¡i niá»‡m giáº£i tÃ­ch Ä‘a biáº¿n lÃ m viá»‡c cÃ¹ng nhau Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a má»™t cÃ¡ch há»‡ thá»‘ng.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_01_02_Derivatives_and_Multivariable_Calculus/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_01_03_Gradient_and_Directional_Derivatives",
    "title": "00-01-03 Gradient vÃ  Äáº¡o HÃ m Theo HÆ°á»›ng",
    "chapter": "00",
    "order": 5,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson explores the gradient vector and directional derivatives, which are central concepts in deep-learning for understanding how functions change in different directions. --- Gradient Vector The gradient MATH is a vector composed of the partial derivatives of the function MATH with respect to each of its variables. It indicates the direction of the steepest ascent of the function at a given point. Definition and Computation For a function of two variables, MATH , its gradient is: MATH For a function of MATH variables, MATH : MATH Example: Computing a Gradient For MATH : MATH MATH Therefore: MATH At the point MATH : MATH --- Directional Derivatives The directional derivative measures the rate of change of MATH when we move in any chosen direction MATH . Here MATH must be a unit vector length 1 . Definition For a function MATH and unit vector MATH : MATH Geometric Interpretation The directional derivative can be written as: MATH where MATH is the angle between MATH and MATH , and MATH is the magnitude of the gradient. Example: Computing Directional Derivatives Using our previous example MATH at point MATH where MATH : Direction 1: MATH positive x-direction MATH Direction 2: MATH positive y-direction MATH Direction 3: MATH 45Â° diagonal MATH --- Maximum and Minimum Rates of Change Key Properties From the formula MATH , we can determine: 1. Maximum Rate of Change : Occurs when MATH i.e., MATH - Direction: MATH same direction as gradient - Maximum rate: MATH 2. Minimum Rate of Change : Occurs when MATH i.e., MATH - Direction: MATH opposite to gradient - Minimum rate: MATH 3. Zero Rate of Change : Occurs when MATH i.e., MATH - Direction: Any vector perpendicular to MATH Summary of Gradient Properties - The gradient MATH points in the direction of steepest increase - The direction MATH points in the direction of steepest decrease - The magnitude MATH gives the maximum rate of change - When MATH , the point is a critical point potential optimum --- Relation to Level Curves At any point on a level curve MATH , the gradient vector MATH is orthogonal perpendicular to the tangent line of the level curve at that point. Why This Matters This orthogonality property is fundamental because: 1. Level curves represent constant function values : Moving along a level curve doesn't change the function value, so the directional derivative is zero. 2. Gradient points to steepest increase : The direction that increases the function value most rapidly must be perpendicular to the direction that doesn't change it at all. 3. Deep Learning insight : To find extrema, we look for points where the gradient is zero critical points or where the gradient is perpendicular to the constraint boundary. Applications in Deep Learning Understanding gradients and directional derivatives is crucial for: 1. Gradient Descent : Moving in the direction MATH to minimize MATH 2. Gradient Ascent : Moving in the direction MATH to maximize MATH 3. Constrained Deep Learning : Using the relationship between gradients and level curves 4. Convergence Analysis : Understanding when algorithms will converge to optimal solutions 5. Step Size Selection : Determining how far to move in the gradient direction The gradient provides both the direction to move and information about how quickly the function is changing, making it the foundation for most deep-learning algorithms.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_01_03_Gradient_and_Directional_Derivatives/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_01_04_Taylor_Series",
    "title": "00-01-04 Chuá»—i Taylor",
    "chapter": "00",
    "order": 6,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y bao gá»“m viá»‡c khai triá»ƒn chuá»—i Taylor, Ä‘iá»u nÃ y ráº¥t cÆ¡ báº£n Ä‘á»ƒ xáº¥p xá»‰ cÃ¡c hÃ m vÃ  hiá»ƒu hÃ nh vi cá»¥c bá»™ cá»§a cÃ¡c hÃ m trong cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a. --- Äá»‹nh nghÄ©a Chuá»—i Taylor Chuá»—i Taylor lÃ  má»™t biá»ƒu diá»…n cá»§a má»™t hÃ m dÆ°á»›i dáº¡ng tá»•ng vÃ´ háº¡n cÃ¡c sá»‘ háº¡ng Ä‘Æ°á»£c tÃ­nh tá»« giÃ¡ trá»‹ cÃ¡c Ä‘áº¡o hÃ m cá»§a hÃ m táº¡i má»™t Ä‘iá»ƒm duy nháº¥t. NÃ³ cung cáº¥p má»™t cÃ¡ch Ä‘á»ƒ xáº¥p xá»‰ cÃ¡c hÃ m phá»©c táº¡p báº±ng cÃ¡ch sá»­ dá»¥ng Ä‘a thá»©c. Chuá»—i Taylor Má»™t Biáº¿n Chuá»—i Taylor lÃ  má»™t khai triá»ƒn chuá»—i cá»§a hÃ m MATH táº¡i Ä‘iá»ƒm MATH : MATH DÆ°á»›i dáº¡ng khai triá»ƒn: MATH Chuá»—i Maclaurin Khi khai triá»ƒn táº¡i MATH , chuá»—i Taylor Ä‘Æ°á»£c gá»i lÃ  chuá»—i Maclaurin : MATH CÃ¡c Chuá»—i Maclaurin ThÃ´ng Dá»¥ng HÃ m MÅ©: MATH HÃ m Sin: MATH HÃ m Cosin: MATH Logarit Tá»± NhiÃªn vá»›i MATH vÃ  MATH , Ä‘iá»ƒm MATH lÃ  Ä‘iá»ƒm yÃªn ngá»±a, khÃ´ng pháº£i cá»±c tiá»ƒu. --- CÃ¡c Xem xÃ©t Thá»±c táº¿ Há»™i tá»¥ vÃ  Äá»™ chÃ­nh xÃ¡c 1. BÃ¡n kÃ­nh Há»™i tá»¥ : Chuá»—i Taylor chá»‰ há»™i tá»¥ trong má»™t bÃ¡n kÃ­nh nháº¥t Ä‘á»‹nh tá»« Ä‘iá»ƒm khai triá»ƒn 2. Sai sá»‘ Cáº¯t bá» : Sá»­ dá»¥ng sá»‘ háº¡ng há»¯u háº¡n Ä‘Æ°a vÃ o sai sá»‘ xáº¥p xá»‰ 3. Chi phÃ­ TÃ­nh toÃ¡n : CÃ¡c sá»‘ háº¡ng báº­c cao yÃªu cáº§u tÃ­nh toÃ¡n nhiá»u Ä‘áº¡o hÃ m hÆ¡n Lá»±a chá»n Thuáº­t toÃ¡n Tá»‘i Æ°u hÃ³a - PhÆ°Æ¡ng phÃ¡p báº­c nháº¥t gradient descent : Chá»‰ sá»­ dá»¥ng thÃ´ng tin gradient, cháº­m hÆ¡n nhÆ°ng ráº» hÆ¡n má»—i láº§n láº·p - PhÆ°Æ¡ng phÃ¡p báº­c hai Newton : Sá»­ dá»¥ng thÃ´ng tin Hessian, há»™i tá»¥ nhanh hÆ¡n nhÆ°ng Ä‘áº¯t má»—i láº§n láº·p - PhÆ°Æ¡ng phÃ¡p quasi-Newton : Xáº¥p xá»‰ Hessian, cÃ¢n báº±ng giá»¯a tá»‘c Ä‘á»™ vÃ  chi phÃ­ tÃ­nh toÃ¡n Khai triá»ƒn chuá»—i Taylor giÃºp chÃºng ta xáº¥p xá»‰ cÃ¡c hÃ m phá»©c táº¡p vá»›i cÃ¡c hÃ m Ä‘a thá»©c Ä‘Æ¡n giáº£n hÆ¡n xung quanh má»™t Ä‘iá»ƒm cá»¥ thá»ƒ, Ä‘iá»u nÃ y ráº¥t quan trá»ng cho cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a vÃ  hiá»ƒu hÃ nh vi cá»¥c bá»™ cá»§a cÃ¡c hÃ m.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_01_04_Taylor_Series/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_02_Basic_Linear_Algebra",
    "title": "00-02 Äáº¡i sá»‘ tuyáº¿n tÃ­nh cÆ¡ báº£n",
    "chapter": "00",
    "order": 7,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y bao gá»“m cÃ¡c khÃ¡i niá»‡m Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh cáº§n thiáº¿t cho tá»‘i Æ°u hÃ³a, Ä‘Æ°á»£c tá»• chá»©c thÃ nh ba pháº§n chÃ­nh Ä‘á»ƒ há»c táº­p cÃ³ há»‡ thá»‘ng.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_02_Basic_Linear_Algebra/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_02_01_Vectors_and_Vector_Spaces",
    "title": "00-02-01 Vector vÃ  KhÃ´ng gian Vector",
    "chapter": "00",
    "order": 8,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y giá»›i thiá»‡u vector, khÃ´ng gian vector, vÃ  cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n táº¡o ná»n táº£ng Ä‘á»ƒ hiá»ƒu Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh trong ngá»¯ cáº£nh tá»‘i Æ°u hÃ³a. --- Vector vÃ  KhÃ´ng gian Vector MATH Vector lÃ  gÃ¬? - Vector: HÃ£y nghÄ© vá» vector nhÆ° má»™t mÅ©i tÃªn trong khÃ´ng gian, biá»ƒu diá»…n cáº£ hÆ°á»›ng vÃ  Ä‘á»™ lá»›n Ä‘á»™ dÃ i . Vá» máº·t toÃ¡n há»c, nÃ³ lÃ  má»™t danh sÃ¡ch cÃ³ thá»© tá»± cÃ¡c sá»‘, giá»‘ng nhÆ° tá»a Ä‘á»™. VÃ­ dá»¥, má»™t vector trong khÃ´ng gian 2D cÃ³ thá»ƒ lÃ  MATH , cÃ³ nghÄ©a lÃ  3 Ä‘Æ¡n vá»‹ dá»c theo trá»¥c x vÃ  4 Ä‘Æ¡n vá»‹ dá»c theo trá»¥c y. - GÃ³c nhÃ¬n HÃ¬nh há»c vs Äáº¡i sá»‘: - HÃ¬nh há»c: Vector lÃ  mÅ©i tÃªn cÃ³ hÆ°á»›ng vÃ  Ä‘á»™ lá»›n - Äáº¡i sá»‘: Vector lÃ  danh sÃ¡ch cÃ³ thá»© tá»± cÃ¡c sá»‘ thá»±c KhÃ´ng gian Vector - KhÃ´ng gian Vector MATH : ÄÃ¢y lÃ  táº­p há»£p táº¥t cáº£ cÃ¡c vector cÃ³ thá»ƒ cÃ³ MATH thÃ nh pháº§n sá»‘ . VÃ­ dá»¥, MATH bao gá»“m táº¥t cáº£ vector 2 thÃ nh pháº§n, biá»ƒu diá»…n táº¥t cáº£ cÃ¡c Ä‘iá»ƒm hoáº·c mÅ©i tÃªn trong máº·t pháº³ng 2D. - VÃ­ dá»¥: - MATH máº·t pháº³ng - MATH khÃ´ng gian 3D CÃ¡c PhÃ©p toÃ¡n Vector PhÃ©p Cá»™ng Vector: MATH PhÃ©p NhÃ¢n VÃ´ hÆ°á»›ng: MATH --- Äá»™c láº­p Tuyáº¿n tÃ­nh, CÆ¡ sá»Ÿ, vÃ  Chiá»u Äá»™c láº­p Tuyáº¿n tÃ­nh Má»™t táº­p há»£p vector MATH lÃ  Ä‘á»™c láº­p tuyáº¿n tÃ­nh náº¿u nghiá»‡m duy nháº¥t cá»§a: MATH lÃ  MATH . Hiá»ƒu biáº¿t Trá»±c quan: Má»™t táº­p há»£p vector \"Ä‘á»™c láº­p tuyáº¿n tÃ­nh\" náº¿u khÃ´ng cÃ³ vector nÃ o trong táº­p cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch chia tá»· lá»‡ vÃ  cá»™ng cÃ¡c vector khÃ¡c trong táº­p. Táº¥t cáº£ chÃºng Ä‘á»u chá»‰ theo cÃ¡c hÆ°á»›ng \"Ä‘á»§ khÃ¡c nhau\". VÃ­ dá»¥ trong MATH : - MATH vÃ  MATH lÃ  Ä‘á»™c láº­p tuyáº¿n tÃ­nh - MATH vÃ  MATH lÃ  phá»¥ thuá»™c tuyáº¿n tÃ­nh vÃ¬ MATH CÆ¡ sá»Ÿ Má»™t cÆ¡ sá»Ÿ cho má»™t khÃ´ng gian vector lÃ  má»™t táº­p há»£p tá»‘i thiá»ƒu cÃ¡c vector Ä‘á»™c láº­p tuyáº¿n tÃ­nh cÃ³ thá»ƒ Ä‘Æ°á»£c káº¿t há»£p chia tá»· lá»‡ vÃ  cá»™ng Ä‘á»ƒ táº¡o ra báº¥t ká»³ vector nÃ o khÃ¡c trong khÃ´ng gian Ä‘Ã³. NÃ³ giá»‘ng nhÆ° má»™t táº­p há»£p cÃ¡c khá»‘i xÃ¢y dá»±ng cÆ¡ báº£n. TÃ­nh cháº¥t cá»§a má»™t CÆ¡ sá»Ÿ: 1. CÃ¡c vector Ä‘á»™c láº­p tuyáº¿n tÃ­nh 2. ChÃºng sinh ra toÃ n bá»™ khÃ´ng gian vector 3. Má»i vector trong khÃ´ng gian cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t duy nháº¥t nhÆ° má»™t tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c vector cÆ¡ sá»Ÿ CÆ¡ sá»Ÿ Chuáº©n cho MATH : MATH Chiá»u Chiá»u cá»§a má»™t khÃ´ng gian vector Ä‘Æ¡n giáº£n lÃ  sá»‘ lÆ°á»£ng vector trong báº¥t ká»³ cÆ¡ sá»Ÿ nÃ o cá»§a nÃ³. NÃ³ cho báº¡n biáº¿t cáº§n bao nhiÃªu hÆ°á»›ng Ä‘á»™c láº­p Ä‘á»ƒ mÃ´ táº£ khÃ´ng gian. - MATH - MATH - MATH --- Chuáº©n cá»§a Vector Chuáº©n lÃ  má»™t hÃ m gÃ¡n \"Ä‘á»™ dÃ i\" hoáº·c \"kÃ­ch thÆ°á»›c\" cho má»™t vector. NÃ³ tá»•ng quÃ¡t hÃ³a khÃ¡i niá»‡m khoáº£ng cÃ¡ch tá»« gá»‘c tá»a Ä‘á»™. TÃ­nh cháº¥t cá»§a Chuáº©n Báº¥t ká»³ chuáº©n MATH nÃ o cÅ©ng pháº£i thá»a mÃ£n ba tÃ­nh cháº¥t: 1. KhÃ´ng Ã¢m: MATH , vÃ  MATH khi vÃ  chá»‰ khi MATH 2. Äá»“ng nháº¥t: MATH vá»›i báº¥t ká»³ vÃ´ hÆ°á»›ng MATH 3. Báº¥t Ä‘áº³ng thá»©c Tam giÃ¡c: MATH CÃ¡c Chuáº©n ThÃ´ng dá»¥ng Chuáº©n Euclid Chuáº©n L2 : MATH ÄÃ¢y lÃ  khoáº£ng cÃ¡ch \"thÃ´ng thÆ°á»ng\" mÃ  chÃºng ta quen thuá»™c. Chuáº©n Manhattan Chuáº©n L1 : MATH CÃ²n Ä‘Æ°á»£c gá»i lÃ  \"chuáº©n taxi\" - khoáº£ng cÃ¡ch mÃ  má»™t chiáº¿c taxi sáº½ Ä‘i trong thÃ nh phá»‘ cÃ³ bá»‘ cá»¥c dáº¡ng lÆ°á»›i. Chuáº©n Tá»‘i Ä‘a Chuáº©n Lâˆž : MATH ThÃ nh pháº§n lá»›n nháº¥t theo giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i. VÃ­ dá»¥: Vá»›i MATH : - MATH - MATH - MATH --- TÃ­ch VÃ´ hÆ°á»›ng TÃ­ch Cháº¥m TÃ­ch cháº¥m hoáº·c tÃ­ch vÃ´ hÆ°á»›ng lÃ  cÃ¡ch phá»• biáº¿n nháº¥t Ä‘á»ƒ nhÃ¢n hai vector, táº¡o ra káº¿t quáº£ vÃ´ hÆ°á»›ng. Äá»‹nh nghÄ©a Vá»›i hai vector MATH vÃ  MATH trong MATH : MATH Diá»…n giáº£i HÃ¬nh há»c MATH trong Ä‘Ã³ MATH lÃ  gÃ³c giá»¯a cÃ¡c vector. TÃ­nh cháº¥t 1. Giao hoÃ¡n: MATH 2. PhÃ¢n phá»‘i: MATH 3. Äá»“ng nháº¥t: MATH TrÆ°á»ng há»£p Äáº·c biá»‡t - Vector trá»±c giao: MATH vuÃ´ng gÃ³c - Vector song song: MATH - TÃ­ch cháº¥m báº£n thÃ¢n: MATH VÃ­ dá»¥ Vá»›i MATH vÃ  MATH : MATH --- á»¨ng dá»¥ng trong Tá»‘i Æ°u hÃ³a Hiá»ƒu vá» vector vÃ  khÃ´ng gian vector lÃ  ráº¥t quan trá»ng cho tá»‘i Æ°u hÃ³a vÃ¬: 1. Biáº¿n Quyáº¿t Ä‘á»‹nh: CÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a thÆ°á»ng liÃªn quan Ä‘áº¿n viá»‡c tÃ¬m giÃ¡ trá»‹ tá»‘t nháº¥t cho nhiá»u biáº¿n, Ä‘Æ°á»£c biá»ƒu diá»…n tá»± nhiÃªn nhÆ° vector. 2. Gradient: Gradient cá»§a má»™t hÃ m lÃ  má»™t vector chá»‰ theo hÆ°á»›ng tÄƒng dá»‘c nháº¥t. 3. RÃ ng buá»™c: RÃ ng buá»™c tuyáº¿n tÃ­nh trong tá»‘i Æ°u hÃ³a cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng tÃ­ch cháº¥m: MATH . 4. Khoáº£ng cÃ¡ch vÃ  Äá»™ tÆ°Æ¡ng tá»±: CÃ¡c chuáº©n khÃ¡c nhau cung cáº¥p cÃ¡c cÃ¡ch khÃ¡c nhau Ä‘á»ƒ Ä‘o khoáº£ng cÃ¡ch giá»¯a cÃ¡c nghiá»‡m hoáº·c kÃ­ch thÆ°á»›c cá»§a cÃ¡c thay Ä‘á»•i. 5. TÃ­nh trá»±c giao: Nhiá»u khÃ¡i niá»‡m tá»‘i Æ°u hÃ³a dá»±a vÃ o tÃ­nh vuÃ´ng gÃ³c, cháº³ng háº¡n nhÆ° má»‘i quan há»‡ giá»¯a gradient vÃ  Ä‘Æ°á»ng má»©c. 6. Tá»• há»£p Tuyáº¿n tÃ­nh: VÃ¹ng kháº£ thi thÆ°á»ng Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° tá»• há»£p tuyáº¿n tÃ­nh cá»§a vector bao lá»“i, hÃ¬nh nÃ³n, v.v. . Khung khÃ´ng gian vector cung cáº¥p ná»n táº£ng toÃ¡n há»c Ä‘á»ƒ xÃ¢y dá»±ng vÃ  giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a má»™t cÃ¡ch cÃ³ há»‡ thá»‘ng.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_02_01_Vectors_and_Vector_Spaces/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_02_02_Matrices_and_Linear_Transformations",
    "title": "00-02-02 Ma tráº­n vÃ  PhÃ©p biáº¿n Ä‘á»•i Tuyáº¿n tÃ­nh",
    "chapter": "00",
    "order": 9,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y bao gá»“m ma tráº­n, cÃ¡c phÃ©p toÃ¡n ma tráº­n, vÃ  phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh, lÃ  nhá»¯ng cÃ´ng cá»¥ cÆ¡ báº£n Ä‘á»ƒ biá»ƒu diá»…n vÃ  giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a. --- Ma tráº­n vÃ  CÃ¡c PhÃ©p toÃ¡n Ma tráº­n Ma tráº­n lÃ  gÃ¬? A matrix is a rectangular grid of numbers arranged in rows and columns. Matrices represent data, transformations, systems of equations, and relationships between variables. General Form: MATH \\mathbf A = \\begin pmatrix a 11 & a 12 & \\cdots & a 1n \\\\ a 21 & a 22 & \\cdots & a 2n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a m1 & a m2 & \\cdots & a mn \\end pmatrix MATH This is an MATH matrix MATH rows, MATH columns . Example: MATH is a MATH matrix. Matrix Addition Matrices are added by summing corresponding elements. Both matrices must have the same dimensions. MATH Example: MATH Scalar Multiplication Multiply every element of the matrix by the scalar: MATH Matrix Multiplication For matrices MATH and MATH , the product MATH is formed by taking the dot product of rows from MATH and columns from MATH : MATH Example: MATH Important: Matrix multiplication is not commutative : MATH in general. --- Linear Transformations A linear transformation is a function MATH that preserves vector addition and scalar multiplication. Every linear transformation can be represented by a matrix. Definition A transformation MATH is linear if and only if: 1. Additivity: MATH 2. Homogeneity: MATH These can be combined into: MATH Matrix-Vector Multiplication If MATH is an MATH matrix and MATH is an MATH column vector, their product MATH is an MATH column vector: MATH \\mathbf w = \\mathbf Av = \\begin pmatrix a 11 v 1 + a 12 v 2 + \\cdots + a 1n v n \\\\ a 21 v 1 + a 22 v 2 + \\cdots + a 2n v n \\\\ \\vdots \\\\ a m1 v 1 + a m2 v 2 + \\cdots + a mn v n \\end pmatrix MATH Example: MATH --- Common 2D Transformations Understanding geometric transformations helps visualize how matrices affect vectors. Scaling Scaling Matrix: MATH - Scales x-coordinates by MATH and y-coordinates by MATH - Example: MATH doubles x-values and triples y-values Rotation Rotation Matrix counter-clockwise by angle MATH : MATH - Example: 90Â° rotation: MATH - Transforms MATH Reflection Reflection across x-axis: MATH Reflection across y-axis: MATH Reflection across line MATH : MATH Shearing Horizontal Shear: MATH Transforms MATH --- Special Types of Matrices Identity Matrix The identity matrix MATH acts like the number 1 for matrix multiplication: MATH \\mathbf I n = \\begin pmatrix 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end pmatrix MATH Property: MATH for any compatible matrix MATH . Transpose The transpose MATH flips a matrix across its main diagonal: MATH Properties: - MATH - MATH - MATH Symmetric Matrices A matrix is symmetric if MATH : MATH Symmetric matrices have special properties important in deep-learning. Inverse Matrix The inverse MATH of a square matrix MATH satisfies: MATH For 2Ã—2 matrices: MATH where MATH and MATH . Note: Not all matrices have inverses. A matrix is invertible non-singular if and only if its determinant is non-zero. --- Applications in Deep Learning Matrices and linear transformations are fundamental in deep-learning for several reasons: 1. System of Linear Equations Many deep-learning problems involve solving MATH : - Unique solution: MATH when MATH is invertible - Least squares: Minimize MATH when no exact solution exists 2. Quadratic Forms Quadratic functions appear frequently in deep-learning: MATH The matrix MATH determines the curvature properties of the function. 3. Linear Programming Standard form: Minimize MATH subject to MATH , MATH 4. Constraint Representation - Equality constraints: MATH - Inequality constraints: MATH 5. Transformations of Variables Change of variables: MATH can simplify deep-learning problems. Example: Portfolio Deep Learning In finance, we might minimize portfolio risk: MATH where MATH is the vector of portfolio weights and MATH is the covariance matrix of asset returns. Understanding matrices and linear transformations provides the tools to formulate, analyze, and solve a wide variety of deep-learning problems efficiently.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_02_02_Matrices_and_Linear_Transformations/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_02_03_Eigenvalues_and_Eigenvectors",
    "title": "00-02-03 GiÃ¡ trá»‹ riÃªng vÃ  Vector riÃªng",
    "chapter": "00",
    "order": 10,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y bao gá»“m giÃ¡ trá»‹ riÃªng vÃ  vector riÃªng, ráº¥t quan trá»ng Ä‘á»ƒ hiá»ƒu hÃ nh vi cá»§a cÃ¡c phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh vÃ  hÃ m báº­c hai trong tá»‘i Æ°u hÃ³a. --- Äá»‹nh nghÄ©a vÃ  Trá»±c giÃ¡c Khi má»™t ma tráº­n biáº¿n Ä‘á»•i má»™t vector, nÃ³ thÆ°á»ng thay Ä‘á»•i cáº£ hÆ°á»›ng vÃ  Ä‘á»™ dÃ i cá»§a vector. Tuy nhiÃªn, vector riÃªng lÃ  nhá»¯ng vector Ä‘áº·c biá»‡t mÃ  khi Ä‘Æ°á»£c biáº¿n Ä‘á»•i bá»Ÿi má»™t ma tráº­n cho trÆ°á»›c, chá»‰ bá»‹ thay Ä‘á»•i tá»‰ lá»‡ nhÆ°ng khÃ´ng thay Ä‘á»•i hÆ°á»›ng. Äá»‹nh nghÄ©a ToÃ¡n há»c Vá»›i má»™t ma tráº­n vuÃ´ng MATH vÃ  má»™t vector khÃ¡c khÃ´ng MATH : - MATH lÃ  má»™t vector riÃªng cá»§a MATH - MATH lÃ  giÃ¡ trá»‹ riÃªng tÆ°Æ¡ng á»©ng náº¿u chÃºng thá»a mÃ£n phÆ°Æ¡ng trÃ¬nh giÃ¡ trá»‹ riÃªng : MATH Giáº£i thÃ­ch HÃ¬nh há»c - Vector riÃªng: CÃ¡c vector khÃ¡c khÃ´ng duy trÃ¬ hÆ°á»›ng cá»§a chÃºng dÆ°á»›i phÃ©p biáº¿n Ä‘á»•i MATH - GiÃ¡ trá»‹ riÃªng: CÃ¡c há»‡ sá»‘ vÃ´ hÆ°á»›ng mÃ  cÃ¡c vector riÃªng Ä‘Æ°á»£c nhÃ¢n vá»›i Hiá»ƒu biáº¿t Trá»±c quan: - Náº¿u MATH : Vector riÃªng bá»‹ kÃ©o dÃ i - Náº¿u MATH : Vector sáº½ tÄƒng theo cáº¥p sá»‘ nhÃ¢n explode 2. Náº¿u MATH for MATH : All eigenvalues of MATH are positive - Positive semidefinite MATH : All eigenvalues are non-negative - Negative definite MATH f \\mathbf x 0 - Local maximum: Hessian is negative definite all eigenvalues 0 MATH \\lambda 2 = 5 - \\sqrt 5 > 0 MATH , the Hessian is positive definite, confirming that the origin is a global minimum. The condition number is MATH , indicating reasonably good conditioning for deep-learning algorithms. Understanding eigenvalues and eigenvectors provides deep insights into the geometric and analytical properties of deep-learning problems, enabling better algorithm design and convergence analysis.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_02_03_Eigenvalues_and_Eigenvectors/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_03_Real_Analysis_and_Set_Theory",
    "title": "00-03 Giáº£i tÃ­ch thá»±c vÃ  LÃ½ thuyáº¿t táº­p há»£p",
    "chapter": "00",
    "order": 11,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y bao gá»“m cÃ¡c khÃ¡i niá»‡m cáº§n thiáº¿t tá»« giáº£i tÃ­ch thá»±c vÃ  lÃ½ thuyáº¿t táº­p há»£p cáº§n thiáº¿t cho tá»‘i Æ°u hÃ³a, Ä‘Æ°á»£c tá»• chá»©c thÃ nh hai pháº§n chÃ­nh Ä‘á»ƒ hiá»ƒu má»™t cÃ¡ch toÃ n diá»‡n.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_03_Real_Analysis_and_Set_Theory/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_03_01_Set_Theory_Fundamentals",
    "title": "00-03-01 CÆ¡ sá»Ÿ LÃ½ thuyáº¿t táº­p há»£p",
    "chapter": "00",
    "order": 12,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y bao gá»“m cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n tá»« lÃ½ thuyáº¿t táº­p há»£p cung cáº¥p ná»n táº£ng toÃ¡n há»c Ä‘á»ƒ hiá»ƒu cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a, rÃ ng buá»™c vÃ  vÃ¹ng kháº£ thi. --- Giá»›i thiá»‡u vá» LÃ½ thuyáº¿t táº­p há»£p LÃ½ thuyáº¿t táº­p há»£p cung cáº¥p ná»n táº£ng cho toÃ¡n há»c hiá»‡n Ä‘áº¡i vÃ  ráº¥t cáº§n thiáº¿t Ä‘á»ƒ hiá»ƒu cÃ¡c khÃ¡i niá»‡m tá»‘i Æ°u hÃ³a. Má»™t táº­p há»£p Ä‘Æ¡n giáº£n lÃ  má»™t bá»™ sÆ°u táº­p cÃ¡c Ä‘á»‘i tÆ°á»£ng riÃªng biá»‡t, Ä‘Æ°á»£c gá»i lÃ  cÃ¡c pháº§n tá»­ hoáº·c thÃ nh viÃªn. KÃ½ hiá»‡u cÆ¡ báº£n - KÃ½ hiá»‡u táº­p há»£p: MATH - Quan há»‡ thÃ nh viÃªn: MATH x thuá»™c A hoáº·c MATH x khÃ´ng thuá»™c A - Táº­p rá»—ng: MATH táº­p há»£p khÃ´ng cÃ³ pháº§n tá»­ nÃ o - KÃ½ hiá»‡u xÃ¢y dá»±ng táº­p há»£p: MATH táº­p há»£p táº¥t cáº£ x sao cho tÃ­nh cháº¥t P x Ä‘Ãºng VÃ­ dá»¥ - MATH liá»‡t kÃª tÆ°á»ng minh - MATH : MATH lá»›n hÆ¡n MATH má»™t cÃ¡ch nghiÃªm ngáº·t - MATH : MATH lá»›n hÆ¡n hoáº·c báº±ng MATH TÃ­nh cháº¥t cá»§a báº¥t Ä‘áº³ng thá»©c 1. TÃ­nh báº¯c cáº§u: Náº¿u MATH vÃ  MATH , thÃ¬ MATH 2. PhÃ©p cá»™ng: Náº¿u MATH , thÃ¬ MATH vá»›i má»i MATH 3. NhÃ¢n vá»›i sá»‘ dÆ°Æ¡ng: Náº¿u MATH vÃ  MATH , thÃ¬ MATH 4. NhÃ¢n vá»›i sá»‘ Ã¢m: Náº¿u MATH vÃ  MATH , thÃ¬ MATH báº¥t Ä‘áº³ng thá»©c Ä‘áº£o chiá»u! KÃ½ hiá»‡u khoáº£ng - Khoáº£ng má»Ÿ: MATH - Khoáº£ng Ä‘Ã³ng: MATH - Khoáº£ng ná»­a má»Ÿ: MATH , MATH - Khoáº£ng khÃ´ng bá»‹ cháº·n: MATH , MATH , MATH --- á»¨ng dá»¥ng trong Tá»‘i Æ°u hÃ³a CÃ¡c khÃ¡i niá»‡m lÃ½ thuyáº¿t táº­p há»£p lÃ  cÆ¡ báº£n cho tá»‘i Æ°u hÃ³a: 1. VÃ¹ng kháº£ thi VÃ¹ng kháº£ thi lÃ  táº­p há»£p táº¥t cáº£ cÃ¡c Ä‘iá»ƒm thá»a mÃ£n cÃ¡c rÃ ng buá»™c: MATH 2. Táº­p má»©c Vá»›i má»™t hÃ m sá»‘ MATH , táº­p má»©c táº¡i má»©c MATH lÃ : MATH 3. Äiá»u kiá»‡n rÃ ng buá»™c Hiá»ƒu biáº¿t khi nÃ o cÃ¡c táº­p rÃ ng buá»™c cÃ³ cÃ¡c tÃ­nh cháº¥t \"tá»‘t\" nhÆ° Ä‘Ã³ng hoáº·c cÃ³ pháº§n trong khÃ´ng rá»—ng áº£nh hÆ°á»Ÿng Ä‘áº¿n sá»± tá»“n táº¡i vÃ  Ä‘áº·c trÆ°ng cá»§a cÃ¡c nghiá»‡m tá»‘i Æ°u. 4. PhÃ¢n tÃ­ch há»™i tá»¥ DÃ£y sá»‘ vÃ  giá»›i háº¡n lÃ  thiáº¿t yáº¿u Ä‘á»ƒ phÃ¢n tÃ­ch liá»‡u cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a cÃ³ há»™i tá»¥ vá» nghiá»‡m tá»‘i Æ°u hay khÃ´ng. 5. CÃ¡c phÃ©p toÃ¡n táº­p há»£p trong thuáº­t toÃ¡n - Giao: TÃ¬m cÃ¡c Ä‘iá»ƒm thá»a mÃ£n nhiá»u rÃ ng buá»™c - Há»£p: Káº¿t há»£p cÃ¡c vÃ¹ng kháº£ thi tá»« cÃ¡c ká»‹ch báº£n khÃ¡c nhau - Pháº§n bÃ¹: Hiá»ƒu biáº¿t vá» cÃ¡c vÃ¹ng khÃ´ng kháº£ thi VÃ­ dá»¥: Trong quy hoáº¡ch tuyáº¿n tÃ­nh, vÃ¹ng kháº£ thi lÃ : MATH ÄÃ¢y lÃ  giao cá»§a cÃ¡c ná»­a khÃ´ng gian, minh há»a cÃ¡ch cÃ¡c phÃ©p toÃ¡n táº­p há»£p xuáº¥t hiá»‡n tá»± nhiÃªn trong viá»‡c xÃ¢y dá»±ng bÃ i toÃ¡n tá»‘i Æ°u hÃ³a. Hiá»ƒu biáº¿t vá» lÃ½ thuyáº¿t táº­p há»£p cung cáº¥p ná»n táº£ng toÃ¡n há»c cháº·t cháº½ cáº§n thiáº¿t Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  phÃ¢n tÃ­ch cÃ¡c tÃ­nh cháº¥t cá»§a chÃºng má»™t cÃ¡ch há»‡ thá»‘ng.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_03_01_Set_Theory_Fundamentals/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_03_02_Topology_in_Real_Analysis",
    "title": "00-03-02 TÃ´pÃ´ trong Giáº£i tÃ­ch thá»±c",
    "chapter": "00",
    "order": 13,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "BÃ i há»c nÃ y bao gá»“m cÃ¡c khÃ¡i niá»‡m tÃ´pÃ´ cáº§n thiáº¿t tá»« giáº£i tÃ­ch thá»±c, ráº¥t quan trá»ng Ä‘á»ƒ hiá»ƒu cáº¥u trÃºc cá»§a cÃ¡c vÃ¹ng kháº£ thi, tÃ­nh liÃªn tá»¥c vÃ  sá»± tá»“n táº¡i cá»§a nghiá»‡m tá»‘i Æ°u trong cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a. --- Giá»›i thiá»‡u vá» TÃ´pÃ´ TÃ´pÃ´ nghiÃªn cá»©u cÃ¡c tÃ­nh cháº¥t cá»§a khÃ´ng gian Ä‘Æ°á»£c báº£o toÃ n dÆ°á»›i cÃ¡c biáº¿n dáº¡ng liÃªn tá»¥c. Trong tá»‘i Æ°u hÃ³a, cÃ¡c khÃ¡i niá»‡m tÃ´pÃ´ giÃºp chÃºng ta hiá»ƒu cáº¥u trÃºc cá»§a cÃ¡c vÃ¹ng kháº£ thi vÃ  hÃ nh vi cá»§a cÃ¡c hÃ m sá»‘, Ä‘áº·c biá»‡t liÃªn quan Ä‘áº¿n sá»± tá»“n táº¡i vÃ  Ä‘áº·c trÆ°ng cá»§a nghiá»‡m tá»‘i Æ°u. KhÃ´ng gian metric vÃ  Khoáº£ng cÃ¡ch TrÆ°á»›c khi tháº£o luáº­n vá» tÃ´pÃ´, chÃºng ta cáº§n khÃ¡i niá»‡m khoáº£ng cÃ¡ch. Trong MATH , khoáº£ng cÃ¡ch Euclid chuáº©n giá»¯a cÃ¡c Ä‘iá»ƒm MATH vÃ  MATH lÃ : MATH HÃ¬nh cáº§u má»Ÿ vÃ  LÃ¢n cáº­n Má»™t hÃ¬nh cáº§u má»Ÿ cÃ³ tÃ¢m táº¡i MATH vá»›i bÃ¡n kÃ­nh MATH lÃ : MATH such that the open ball MATH is entirely contained within MATH : MATH Intuitive Understanding An open set has the property that if you're inside it, you can move a small distance in any direction and still remain inside the set. There's always some \"wiggle room\" around every point. Examples of Open Sets In MATH : - MATH first quadrant, excluding axes - MATH itself In MATH : - Any open ball MATH - MATH itself - MATH empty set - vacuously open Properties of Open Sets 1. The union of any collection of open sets is open 2. The intersection of finitely many open sets is open 3. MATH and MATH are both open --- Closed Sets A closed set is defined as a set that contains all of its boundary points. Equivalently, a set MATH is closed if its complement MATH is an open set . Formal Definition A set MATH is closed if it contains all its limit points. That is, if a sequence of points MATH from MATH converges to a point MATH , then MATH must also be in MATH : MATH Examples of Closed Sets In MATH : - MATH - MATH - MATH single point - MATH integers In MATH : - MATH closed unit disk - MATH first quadrant, including axes - MATH single point In MATH : - Any closed ball MATH - MATH itself - MATH empty set - Any finite set Properties of Closed Sets 1. The intersection of any collection of closed sets is closed 2. The union of finitely many closed sets is closed 3. MATH and MATH are both closed Important Note Sets can be: - Open but not closed: MATH - Closed but not open: MATH - Both open and closed: MATH , MATH - Neither open nor closed: MATH , MATH --- Boundary, Interior, and Closure Boundary The boundary of a set MATH , denoted MATH , consists of points that are \"on the edge\" of the set. A point MATH is a boundary point of MATH if every open ball centered at MATH intersects both MATH and its complement MATH : MATH Interior The interior of a set MATH , denoted MATH or MATH , includes all points strictly \"inside\" the set, excluding the boundary: MATH Closure The closure of a set MATH , denoted MATH or MATH , is the smallest closed set containing MATH : MATH Example Analysis For the interval MATH in MATH : - Interior: MATH - Boundary: MATH - Closure: MATH For the open disk MATH such that MATH - Closed: As defined above Examples of Compact Sets In MATH : - MATH any closed, bounded interval - MATH single point - Any finite set In MATH : - MATH closed unit disk - MATH unit square - Any finite set of points In MATH : - Any closed ball MATH - Any closed, bounded rectangle MATH Non-Compact Sets - MATH bounded but not closed - MATH closed but not bounded - MATH not bounded - MATH bounded but not closed, since 0 is a limit point not in the set --- Continuity of Functions Point-wise Continuity A function MATH is continuous at a point MATH if for every MATH , there exists MATH such that for all MATH : MATH Intuitive meaning: Small changes in input lead to small changes in output. Global Continuity MATH is continuous on MATH if it's continuous at every point in MATH . Sequential Characterization MATH is continuous at MATH if and only if for every sequence MATH in MATH converging to MATH : MATH --- Important Theorems for Deep Learning Extreme Value Theorem If MATH is continuous on a compact set MATH , then MATH attains its maximum and minimum on MATH . This is fundamental for deep-learning: it guarantees that continuous objective functions have optimal solutions on compact feasible regions. Proof idea: Compactness ensures that the supremum and infimum of MATH on MATH are actually achieved at points in MATH . Intermediate Value Theorem If MATH is continuous on MATH and MATH is between MATH and MATH , then there exists MATH such that MATH . This helps establish the existence of solutions to equations MATH . Bolzano-Weierstrass Theorem Every bounded sequence in MATH has a convergent subsequence. This is crucial for proving convergence of deep-learning algorithms. Weierstrass Approximation Theorem Every continuous function on a closed interval can be uniformly approximated by polynomials. This justifies using polynomial approximations in deep-learning algorithms. --- Applications in Deep Learning 1. Existence of Solutions Compact feasible sets guarantee optimal solutions exist: - If the feasible region MATH is compact and the objective function MATH is continuous, then the deep-learning problem MATH has a solution. 2. Constraint Qualification Understanding topological properties of constraint sets: - Regular points: Points where constraint gradients are linearly independent - Interior point methods: Require the feasible region to have non-empty interior 3. Convergence Analysis Analyzing whether deep-learning algorithms converge: - Closed sets: Ensure limit points of convergent sequences remain feasible - Compactness: Guarantees convergent subsequences exist 4. Local vs Global Optima Using neighborhoods to define optimality: - Local minimum: MATH for all MATH in some neighborhood of MATH - Global minimum: MATH for all MATH in the feasible region 5. Feasible Region Analysis Determining properties of constraint sets: - Linear constraints: Define closed sets half-spaces - Nonlinear constraints: May create sets that are neither open nor closed - Compact feasible regions: Guarantee existence of optimal solutions Example: Portfolio Deep Learning Consider minimizing portfolio risk subject to constraints: MATH \\begin align \\min \\mathbf w \\quad & \\mathbf w ^T \\mathbf \\Sigma \\mathbf w \\\\ \\text s.t. \\quad & \\mathbf 1 ^T \\mathbf w = 1 \\\\ & \\mathbf w \\geq \\mathbf 0 \\end align MATH The feasible region MATH is: - Closed: It's the intersection of closed sets - Bounded: The constraint MATH with MATH bounds the feasible region - Compact: Being closed and bounded in MATH Since the objective function MATH is continuous and MATH is compact, the Extreme Value Theorem guarantees that an optimal portfolio exists. Understanding topology and real analysis provides the rigorous foundation needed to prove that deep-learning problems have solutions and that algorithms will find them. These concepts are essential for both theoretical analysis and practical algorithm design.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_03_02_Topology_in_Real_Analysis/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_04_Probability_and_Statistics",
    "title": "00-04 XÃ¡c Suáº¥t vÃ  Thá»‘ng KÃª",
    "chapter": "00",
    "order": 13,
    "owner": "AI Assistant",
    "lesson_type": "required",
    "content": "XÃ¡c Suáº¥t vÃ  Thá»‘ng KÃª cho Tá»‘i Æ¯u HÃ³a Lá»“i XÃ¡c suáº¥t vÃ  thá»‘ng kÃª táº¡o nÃªn ná»n táº£ng quan trá»ng Ä‘á»ƒ hiá»ƒu nhiá»u bÃ i toÃ¡n tá»‘i Æ°u hÃ³a, Ä‘áº·c biá»‡t trong há»c mÃ¡y vÃ  khoa há»c dá»¯ liá»‡u. Pháº§n nÃ y giá»›i thiá»‡u cÃ¡c khÃ¡i niá»‡m xÃ¡c suáº¥t thiáº¿t yáº¿u thÆ°á»ng xuáº¥t hiá»‡n trong tá»‘i Æ°u hÃ³a lá»“i, tá»« Æ°á»›c lÆ°á»£ng há»£p lÃ½ tá»‘i Ä‘a Ä‘áº¿n tá»‘i Æ°u hÃ³a Bayes. Táº¡i Sao XÃ¡c Suáº¥t Quan Trá»ng trong Tá»‘i Æ¯u HÃ³a Nhiá»u bÃ i toÃ¡n tá»‘i Æ°u hÃ³a phÃ¡t sinh tá»« mÃ´ hÃ¬nh hÃ³a thá»‘ng kÃª: - Æ¯á»›c LÆ°á»£ng Há»£p LÃ½ Tá»‘i Äa MLE : TÃ¬m tham sá»‘ Ä‘á»ƒ tá»‘i Ä‘a hÃ³a kháº£ nÄƒng cá»§a dá»¯ liá»‡u quan sÃ¡t - Tá»‘i Æ¯u HÃ³a Bayes : Sá»­ dá»¥ng mÃ´ hÃ¬nh xÃ¡c suáº¥t Ä‘á»ƒ hÆ°á»›ng dáº«n tÃ¬m kiáº¿m giáº£i phÃ¡p tá»‘i Æ°u - Tá»‘i Æ¯u HÃ³a Ngáº«u NhiÃªn : Xá»­ lÃ½ sá»± báº¥t Ä‘á»‹nh vÃ  ngáº«u nhiÃªn trong hÃ m má»¥c tiÃªu - Regularization : ThÃªm prior xÃ¡c suáº¥t Ä‘á»ƒ ngÄƒn overfitting - Tá»‘i Thiá»ƒu HÃ³a Rá»§i Ro : Tá»‘i Æ°u hÃ³a ká»³ vá»ng loss trÃªn phÃ¢n phá»‘i xÃ¡c suáº¥t CÃ¡c Chá»§ Äá» ChÃ­nh 1. LÃ½ Thuyáº¿t XÃ¡c Suáº¥t CÆ¡ Báº£n : KhÃ´ng gian máº«u, biáº¿n cá»‘ vÃ  tiÃªn Ä‘á» xÃ¡c suáº¥t 2. CÃ¡c PhÃ¢n Phá»‘i XÃ¡c Suáº¥t ThÃ´ng Dá»¥ng : PhÃ¢n phá»‘i chuáº©n, mÅ© vÃ  cÃ¡c phÃ¢n phá»‘i quan trá»ng khÃ¡c 3. Ká»³ Vá»ng vÃ  PhÆ°Æ¡ng Sai : TÃ­nh toÃ¡n vÃ  tá»‘i Æ°u hÃ³a giÃ¡ trá»‹ ká»³ vá»ng 4. Äá»‹nh LÃ½ Bayes : Ná»n táº£ng cho tá»‘i Æ°u hÃ³a vÃ  suy luáº­n Bayes 5. Æ¯á»›c LÆ°á»£ng Thá»‘ng KÃª : Káº¿t ná»‘i lÃ½ thuyáº¿t xÃ¡c suáº¥t vá»›i bÃ i toÃ¡n tá»‘i Æ°u hÃ³a Káº¿t Ná»‘i vá»›i Tá»‘i Æ¯u HÃ³a Lá»“i Hiá»ƒu xÃ¡c suáº¥t giÃºp báº¡n: - XÃ¢y Dá»±ng BÃ i ToÃ¡n : Chuyá»ƒn Ä‘á»•i sá»± báº¥t Ä‘á»‹nh thá»±c táº¿ thÃ nh bÃ i toÃ¡n tá»‘i Æ°u hÃ³a toÃ¡n há»c - Chá»n HÃ m Má»¥c TiÃªu : Lá»±a chá»n hÃ m loss phÃ¹ há»£p dá»±a trÃªn giáº£ thuyáº¿t xÃ¡c suáº¥t - Diá»…n Giáº£i Káº¿t Quáº£ : Hiá»ƒu khoáº£ng tin cáº­y vÃ  Ã½ nghÄ©a thá»‘ng kÃª cá»§a nghiá»‡m - Xá»­ LÃ½ Nhiá»…u : Äá»‘i phÃ³ vá»›i lá»—i Ä‘o lÆ°á»ng vÃ  quÃ¡ trÃ¬nh ngáº«u nhiÃªn - Thiáº¿t Káº¿ Thuáº­t ToÃ¡n : PhÃ¡t triá»ƒn phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a bá»n vá»¯ng hoáº¡t Ä‘á»™ng dÆ°á»›i sá»± báº¥t Ä‘á»‹nh Ná»n táº£ng nÃ y sáº½ ráº¥t quan trá»ng khi chÃºng ta khÃ¡m phÃ¡ cÃ¡ch cÃ¡c mÃ´ hÃ¬nh xÃ¡c suáº¥t dáº«n Ä‘áº¿n bÃ i toÃ¡n tá»‘i Æ°u hÃ³a lá»“i trong há»c mÃ¡y, thá»‘ng kÃª vÃ  cÃ¡c á»©ng dá»¥ng ká»¹ thuáº­t. ðŸ’¡ Lá»™ TrÃ¬nh Há»c: Báº¯t Ä‘áº§u vá»›i cÃ¡c khÃ¡i niá»‡m xÃ¡c suáº¥t cÆ¡ báº£n, sau Ä‘Ã³ khÃ¡m phÃ¡ cÃ¡ch chÃºng káº¿t ná»‘i vá»›i tá»‘i Æ°u hÃ³a thÃ´ng qua Æ°á»›c lÆ°á»£ng há»£p lÃ½ tá»‘i Ä‘a vÃ  phÆ°Æ¡ng phÃ¡p Bayes. Má»—i bÃ i há»c xÃ¢y dá»±ng hÆ°á»›ng tá»›i viá»‡c hiá»ƒu cÃ¡ch sá»± báº¥t Ä‘á»‹nh vÃ  ngáº«u nhiÃªn táº¡o ra cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_04_Probability_and_Statistics/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_04_01_Basic_Probability_Theory",
    "title": "00-04-01 LÃ½ Thuyáº¿t XÃ¡c Suáº¥t CÆ¡ Báº£n",
    "chapter": "00",
    "order": 14,
    "owner": "AI Assistant",
    "lesson_type": "required",
    "content": "LÃ½ Thuyáº¿t XÃ¡c Suáº¥t CÆ¡ Báº£n LÃ½ thuyáº¿t xÃ¡c suáº¥t cung cáº¥p khung toÃ¡n há»c Ä‘á»ƒ lÃ½ luáº­n vá» sá»± báº¥t Ä‘á»‹nh, Ä‘iá»u nÃ y lÃ  ná»n táº£ng cho nhiá»u bÃ i toÃ¡n tá»‘i Æ°u hÃ³a trong há»c mÃ¡y vÃ  khoa há»c dá»¯ liá»‡u. 1. KhÃ´ng Gian Máº«u vÃ  Biáº¿n Cá»‘ KhÃ´ng Gian Máº«u Î© : Táº­p há»£p táº¥t cáº£ cÃ¡c káº¿t quáº£ cÃ³ thá»ƒ cÃ³ cá»§a má»™t thÃ­ nghiá»‡m. Biáº¿n Cá»‘ A : Má»™t táº­p con cá»§a khÃ´ng gian máº«u Ä‘áº¡i diá»‡n cho má»™t táº­p há»£p cÃ¡c káº¿t quáº£. VÃ­ dá»¥: - Tung Ä‘á»“ng xu: Î© = S, N - Tung xÃºc xáº¯c: Î© = 1, 2, 3, 4, 5, 6 - LiÃªn tá»¥c: Î© = 0, 1 cho biáº¿n ngáº«u nhiÃªn Ä‘á»u Trá»±c Quan HÃ³a KhÃ´ng Gian Máº«u TÆ°Æ¡ng TÃ¡c Trá»±c quan hÃ³a: Nháº¥p Ä‘á»ƒ táº¡o máº«u ngáº«u nhiÃªn. CÃ¡c mÃ u khÃ¡c nhau Ä‘áº¡i diá»‡n cho cÃ¡c biáº¿n cá»‘ khÃ¡c nhau. Loáº¡i ThÃ­ Nghiá»‡m Tung Äá»“ng Xu Tung XÃºc Xáº¯c Äá»u 0,1 Táº¡o Máº«u XÃ³a Thá»‘ng KÃª: Tá»•ng máº«u: 0 Biáº¿n cá»‘ A: 0 Biáº¿n cá»‘ B: 0 P A â‰ˆ 0.000 P B â‰ˆ 0.000 2. TiÃªn Äá» XÃ¡c Suáº¥t TiÃªn Äá» Kolmogorov Vá»›i báº¥t ká»³ Ä‘á»™ Ä‘o xÃ¡c suáº¥t P nÃ o, cÃ¡c tiÃªn Ä‘á» sau pháº£i Ä‘Æ°á»£c thá»a mÃ£n: TiÃªn Äá» 1: TÃ­nh KhÃ´ng Ã‚m MATH TiÃªn Äá» 2: Chuáº©n HÃ³a MATH TiÃªn Äá» 3: TÃ­nh Cá»™ng Äáº¿m ÄÆ°á»£c Vá»›i cÃ¡c biáº¿n cá»‘ xung kháº¯c MATH : MATH 3. TÃ­nh Cháº¥t vÃ  Quy Táº¯c CÆ¡ Báº£n Quy Táº¯c Pháº§n BÃ¹ MATH Quy Táº¯c Cá»™ng Vá»›i hai biáº¿n cá»‘ A vÃ  B báº¥t ká»³: MATH Quy Táº¯c NhÃ¢n MATH 4. XÃ¡c Suáº¥t CÃ³ Äiá»u Kiá»‡n XÃ¡c suáº¥t cá»§a biáº¿n cá»‘ A khi biáº¿t ráº±ng biáº¿n cá»‘ B Ä‘Ã£ xáº£y ra: MATH Diá»…n giáº£i : XÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n cáº­p nháº­t niá»m tin cá»§a chÃºng ta vá» A khi cÃ³ thÃ´ng tin vá» B. Trá»±c Quan HÃ³a XÃ¡c Suáº¥t CÃ³ Äiá»u Kiá»‡n Biá»ƒu Äá»“ Venn: HÃ¬nh trÃ²n xanh lÃ  biáº¿n cá»‘ A, hÃ¬nh trÃ²n Ä‘á» lÃ  biáº¿n cá»‘ B. Giao mÃ u tÃ­m thá»ƒ hiá»‡n A âˆ© B. Äiá»u Chá»‰nh XÃ¡c Suáº¥t P A : 0.4 P B : 0.5 Giao: 0.2 XÃ¡c Suáº¥t: P A = 0.400 P B = 0.500 P A âˆ© B = 0.200 P A âˆª B = 0.700 CÃ³ Äiá»u Kiá»‡n: P A|B = 0.400 P B|A = 0.500 5. TÃ­nh Äá»™c Láº­p Hai biáº¿n cá»‘ A vÃ  B Ä‘á»™c láº­p náº¿u: MATH TÆ°Æ¡ng Ä‘Æ°Æ¡ng: MATH Diá»…n giáº£i : Kiáº¿n thá»©c vá» má»™t biáº¿n cá»‘ khÃ´ng thay Ä‘á»•i xÃ¡c suáº¥t cá»§a biáº¿n cá»‘ kia. 6. Biáº¿n Ngáº«u NhiÃªn Biáº¿n ngáº«u nhiÃªn X lÃ  má»™t hÃ m gÃ¡n má»™t sá»‘ thá»±c cho má»—i káº¿t quáº£ trong khÃ´ng gian máº«u: MATH CÃ¡c Loáº¡i Biáº¿n Ngáº«u NhiÃªn: Rá»i ráº¡c : Nháº­n cÃ¡c giÃ¡ trá»‹ Ä‘áº¿m Ä‘Æ°á»£c vÃ­ dá»¥: sá»‘ láº§n xuáº¥t hiá»‡n máº·t sáº¥p - HÃ m Khá»‘i XÃ¡c Suáº¥t PMF : MATH LiÃªn tá»¥c : Nháº­n cÃ¡c giÃ¡ trá»‹ khÃ´ng Ä‘áº¿m Ä‘Æ°á»£c vÃ­ dá»¥: chiá»u cao, cÃ¢n náº·ng - HÃ m Máº­t Äá»™ XÃ¡c Suáº¥t PDF : MATH - MATH 7. Káº¿t Ná»‘i vá»›i Tá»‘i Æ¯u HÃ³a LÃ½ thuyáº¿t xÃ¡c suáº¥t káº¿t ná»‘i vá»›i tá»‘i Æ°u hÃ³a theo nhiá»u cÃ¡ch: Æ¯á»›c LÆ°á»£ng Há»£p LÃ½ Tá»‘i Äa TÃ¬m tham sá»‘ Î¸ Ä‘á»ƒ tá»‘i Ä‘a hÃ³a likelihood: MATH Tá»‘i Æ¯u HÃ³a GiÃ¡ Trá»‹ Ká»³ Vá»ng Tá»‘i thiá»ƒu hÃ³a ká»³ vá»ng loss: MATH Tá»‘i Æ¯u HÃ³a Bayes Sá»­ dá»¥ng phÃ¢n phá»‘i xÃ¡c suáº¥t Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a sá»± báº¥t Ä‘á»‹nh trong hÃ m má»¥c tiÃªu vÃ  hÆ°á»›ng dáº«n tÃ¬m kiáº¿m nghiá»‡m tá»‘i Æ°u. VÃ­ Dá»¥ XÃ¡c Suáº¥t trong Tá»‘i Æ¯u HÃ³a VÃ­ Dá»¥ MLE: TÃ¬m tham sá»‘ Î¼ Ä‘á»ƒ tá»‘i Ä‘a hÃ³a likelihood cá»§a dá»¯ liá»‡u quan sÃ¡t tá»« Normal Î¼, 1 . Demo MLE Î¼ Thá»±c: 2.0 KÃ­ch ThÆ°á»›c Máº«u: 20 Táº¡o Dá»¯ Liá»‡u & TÃ¬m MLE Káº¿t Quáº£: Î¼ Thá»±c: 2.000 Trung bÃ¬nh máº«u: -- Æ¯á»›c lÆ°á»£ng MLE: -- Sai sá»‘: -- Nhá»¯ng Äiá»ƒm ChÃ­nh 1. Ná»n Táº£ng : TiÃªn Ä‘á» xÃ¡c suáº¥t cung cáº¥p ná»n táº£ng toÃ¡n há»c Ä‘á»ƒ lÃ½ luáº­n vá» sá»± báº¥t Ä‘á»‹nh 2. XÃ¡c Suáº¥t CÃ³ Äiá»u Kiá»‡n : Thiáº¿t yáº¿u Ä‘á»ƒ cáº­p nháº­t niá»m tin vá»›i thÃ´ng tin má»›i 3. TÃ­nh Äá»™c Láº­p : ÄÆ¡n giáº£n hÃ³a tÃ­nh toÃ¡n vÃ  giáº£ thuyáº¿t mÃ´ hÃ¬nh hÃ³a 4. Biáº¿n Ngáº«u NhiÃªn : Cáº§u ná»‘i giá»¯a xÃ¡c suáº¥t trá»«u tÆ°á»£ng vÃ  á»©ng dá»¥ng cá»¥ thá»ƒ 5. Káº¿t Ná»‘i Tá»‘i Æ¯u HÃ³a : Nhiá»u bÃ i toÃ¡n tá»‘i Æ°u hÃ³a phÃ¡t sinh tá»« mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t Hiá»ƒu nhá»¯ng khÃ¡i niá»‡m cÆ¡ báº£n nÃ y chuáº©n bá»‹ cho báº¡n cÃ¡c chá»§ Ä‘á» nÃ¢ng cao hÆ¡n nhÆ° suy luáº­n Bayes, Æ°á»›c lÆ°á»£ng há»£p lÃ½ tá»‘i Ä‘a vÃ  tá»‘i Æ°u hÃ³a ngáº«u nhiÃªn - nhá»¯ng yáº¿u tá»‘ trung tÃ¢m cá»§a há»c mÃ¡y vÃ  khoa há»c dá»¯ liá»‡u hiá»‡n Ä‘áº¡i. // Sample Space Visualization class SampleSpaceDemo constructor this.canvas = document.getElementById 'sampleSpaceCanvas' ; this.ctx = this.canvas.getContext '2d' ; this.width = this.canvas.width; this.height = this.canvas.height; this.samples = ; this.experimentType = 'coin'; this.setupControls ; this.draw ; setupControls const radios = document.querySelectorAll 'input name=\"experiment\" ' ; const generateBtn = document.getElementById 'generate-sample' ; const clearBtn = document.getElementById 'clear-samples' ; radios.forEach radio => radio.addEventListener 'change', e => this.experimentType = e.target.value; this.samples = ; this.updateStats ; this.draw ; ; ; generateBtn.addEventListener 'click', => this.generateSample ; clearBtn.addEventListener 'click', => this.samples = ; this.updateStats ; this.draw ; ; this.canvas.addEventListener 'click', => this.generateSample ; generateSample let sample; switch this.experimentType case 'coin': sample = value: Math.random = 4, // Biáº¿n cá»‘ A: 4, 5, hoáº·c 6 eventB: diceValue % 2 === 0 // Biáº¿n cá»‘ B: Cháºµn ; break; case 'uniform': const uniformValue = Math.random ; sample = value: uniformValue.toFixed 3 , x: uniformValue this.width - 40 + 20, y: Math.random this.height - 40 + 20, eventA: uniformValue > 0.5, // Biáº¿n cá»‘ A: > 0.5 eventB: uniformValue s.eventA .length; const eventBCount = this.samples.filter s => s.eventB .length; document.getElementById 'total-samples' .textContent = total; document.getElementById 'event-a-count' .textContent = eventACount; document.getElementById 'event-b-count' .textContent = eventBCount; document.getElementById 'prob-a' .textContent = total > 0 ? eventACount / total .toFixed 3 : '0.000'; document.getElementById 'prob-b' .textContent = total > 0 ? eventBCount / total .toFixed 3 : '0.000'; draw this.ctx.clearRect 0, 0, this.width, this.height ; // Draw background this.ctx.fillStyle = ' f8f9fa'; this.ctx.fillRect 0, 0, this.width, this.height ; // Draw samples this.samples.forEach sample => // Determine color based on events let color = ' 666'; if sample.eventA && sample.eventB color = ' 9c27b0'; // Cáº£ hai biáº¿n cá»‘ else if sample.eventA color = ' 2196f3'; // Chá»‰ biáº¿n cá»‘ A else if sample.eventB color = ' f44336'; // Chá»‰ biáº¿n cá»‘ B this.ctx.fillStyle = color; this.ctx.beginPath ; this.ctx.arc sample.x, sample.y, 5, 0, 2 Math.PI ; this.ctx.fill ; // Draw value this.ctx.fillStyle = ' 000'; this.ctx.font = '10px Arial'; this.ctx.textAlign = 'center'; this.ctx.fillText sample.value, sample.x, sample.y - 8 ; ; // Draw legend this.ctx.fillStyle = ' 000'; this.ctx.font = '12px Arial'; this.ctx.textAlign = 'left'; this.ctx.fillText 'ChÃº thÃ­ch:', 10, 20 ; this.ctx.fillStyle = ' 2196f3'; this.ctx.beginPath ; this.ctx.arc 20, 35, 4, 0, 2 Math.PI ; this.ctx.fill ; this.ctx.fillStyle = ' 000'; this.ctx.fillText 'Chá»‰ biáº¿n cá»‘ A', 30, 38 ; this.ctx.fillStyle = ' f44336'; this.ctx.beginPath ; this.ctx.arc 20, 50, 4, 0, 2 Math.PI ; this.ctx.fill ; this.ctx.fillStyle = ' 000'; this.ctx.fillText 'Chá»‰ biáº¿n cá»‘ B', 30, 53 ; this.ctx.fillStyle = ' 9c27b0'; this.ctx.beginPath ; this.ctx.arc 20, 65, 4, 0, 2 Math.PI ; this.ctx.fill ; this.ctx.fillStyle = ' 000'; this.ctx.fillText 'Cáº£ A vÃ  B', 30, 68 ; // Conditional Probability Visualization class ConditionalProbDemo constructor this.canvas = document.getElementById 'conditionalCanvas' ; this.ctx = this.canvas.getContext '2d' ; this.width = this.canvas.width; this.height = this.canvas.height; this.probA = 0.4; this.probB = 0.5; this.overlap = 0.2; this.setupControls ; this.draw ; setupControls const probASlider = document.getElementById 'prob-a-slider' ; const probBSlider = document.getElementById 'prob-b-slider' ; const overlapSlider = document.getElementById 'overlap-slider' ; probASlider.addEventListener 'input', e => this.probA = parseFloat e.target.value ; document.getElementById 'prob-a-value' .textContent = this.probA.toFixed 1 ; this.updateCalculations ; this.draw ; ; probBSlider.addEventListener 'input', e => this.probB = parseFloat e.target.value ; document.getElementById 'prob-b-value' .textContent = this.probB.toFixed 1 ; this.updateCalculations ; this.draw ; ; overlapSlider.addEventListener 'input', e => this.overlap = parseFloat e.target.value ; document.getElementById 'overlap-value' .textContent = this.overlap.toFixed 1 ; // Ensure overlap doesn't exceed min probA, probB const maxOverlap = Math.min this.probA, this.probB ; if this.overlap > maxOverlap this.overlap = maxOverlap; overlapSlider.value = this.overlap; document.getElementById 'overlap-value' .textContent = this.overlap.toFixed 1 ; this.updateCalculations ; this.draw ; ; this.updateCalculations ; updateCalculations const probUnion = this.probA + this.probB - this.overlap; const probAGivenB = this.probB > 0 ? this.overlap / this.probB : 0; const probBGivenA = this.probA > 0 ? this.overlap / this.probA : 0; document.getElementById 'display-prob-a' .textContent = this.probA.toFixed 3 ; document.getElementById 'display-prob-b' .textContent = this.probB.toFixed 3 ; document.getElementById 'display-prob-ab' .textContent = this.overlap.toFixed 3 ; document.getElementById 'display-prob-union' .textContent = probUnion.toFixed 3 ; document.getElementById 'display-prob-a-given-b' .textContent = probAGivenB.toFixed 3 ; document.getElementById 'display-prob-b-given-a' .textContent = probBGivenA.toFixed 3 ; draw this.ctx.clearRect 0, 0, this.width, this.height ; // Draw universe rectangle this.ctx.strokeStyle = ' 000'; this.ctx.lineWidth = 2; this.ctx.strokeRect 50, 50, 300, 200 ; this.ctx.fillStyle = ' 000'; this.ctx.font = '14px Arial'; this.ctx.fillText 'Î© KhÃ´ng gian máº«u ', 55, 45 ; // Calculate circle parameters const centerAX = 150; const centerAY = 150; const centerBX = 250; const centerBY = 150; // Calculate radii based on probabilities area proportional to probability const radiusA = Math.sqrt this.probA 10000 / Math.PI ; const radiusB = Math.sqrt this.probB 10000 / Math.PI ; // Draw circle A this.ctx.globalAlpha = 0.3; this.ctx.fillStyle = ' 2196f3'; this.ctx.beginPath ; this.ctx.arc centerAX, centerAY, radiusA, 0, 2 Math.PI ; this.ctx.fill ; // Draw circle B this.ctx.fillStyle = ' f44336'; this.ctx.beginPath ; this.ctx.arc centerBX, centerBY, radiusB, 0, 2 Math.PI ; this.ctx.fill ; // Draw intersection approximate if this.overlap > 0 this.ctx.fillStyle = ' 9c27b0'; const overlapRadius = Math.sqrt this.overlap 5000 / Math.PI ; this.ctx.beginPath ; this.ctx.arc centerAX + centerBX / 2, centerAY + centerBY / 2, overlapRadius, 0, 2 Math.PI ; this.ctx.fill ; this.ctx.globalAlpha = 1.0; // Draw circle outlines this.ctx.strokeStyle = ' 2196f3'; this.ctx.lineWidth = 2; this.ctx.beginPath ; this.ctx.arc centerAX, centerAY, radiusA, 0, 2 Math.PI ; this.ctx.stroke ; this.ctx.strokeStyle = ' f44336'; this.ctx.beginPath ; this.ctx.arc centerBX, centerBY, radiusB, 0, 2 Math.PI ; this.ctx.stroke ; // Labels this.ctx.fillStyle = ' 000'; this.ctx.font = '16px Arial'; this.ctx.fillText 'A', centerAX - 40, centerAY ; this.ctx.fillText 'B', centerBX + 30, centerBY ; if this.overlap > 0 this.ctx.fillText 'Aâˆ©B', centerAX + centerBX / 2 - 15, centerAY + centerBY / 2 + 5 ; // MLE Deep Learning Demo class MLEDemo constructor this.canvas = document.getElementById 'deep-learningCanvas' ; this.ctx = this.canvas.getContext '2d' ; this.width = this.canvas.width; this.height = this.canvas.height; this.trueMu = 2.0; this.sampleSize = 20; this.data = ; this.setupControls ; this.draw ; setupControls const trueMuSlider = document.getElementById 'true-mu-slider' ; const sampleSizeSlider = document.getElementById 'sample-size-slider' ; const generateBtn = document.getElementById 'generate-mle-data' ; trueMuSlider.addEventListener 'input', e => this.trueMu = parseFloat e.target.value ; document.getElementById 'true-mu-value' .textContent = this.trueMu.toFixed 1 ; document.getElementById 'display-true-mu' .textContent = this.trueMu.toFixed 3 ; ; sampleSizeSlider.addEventListener 'input', e => this.sampleSize = parseInt e.target.value ; document.getElementById 'sample-size-value' .textContent = this.sampleSize; ; generateBtn.addEventListener 'click', => this.generateDataAndFindMLE ; generateDataAndFindMLE // Generate data from Normal trueMu, 1 this.data = ; for let i = 0; i sum + x, 0 / this.data.length; const error = Math.abs sampleMean - this.trueMu ; // Update display document.getElementById 'sample-mean' .textContent = sampleMean.toFixed 3 ; document.getElementById 'mle-estimate' .textContent = sampleMean.toFixed 3 ; document.getElementById 'mle-error' .textContent = error.toFixed 3 ; this.draw ; draw this.ctx.clearRect 0, 0, this.width, this.height ; if this.data.length === 0 this.ctx.fillStyle = ' 666'; this.ctx.font = '16px Arial'; this.ctx.textAlign = 'center'; this.ctx.fillText 'Nháº¥p \"Táº¡o Dá»¯ Liá»‡u & TÃ¬m MLE\" Ä‘á»ƒ báº¯t Ä‘áº§u', this.width / 2, this.height / 2 ; return; // Draw axes this.ctx.strokeStyle = ' ddd'; this.ctx.lineWidth = 1; const marginX = 50; const marginY = 50; const plotWidth = this.width - 2 marginX; const plotHeight = this.height - 2 marginY; // X-axis this.ctx.beginPath ; this.ctx.moveTo marginX, this.height - marginY ; this.ctx.lineTo this.width - marginX, this.height - marginY ; this.ctx.stroke ; // Y-axis this.ctx.beginPath ; this.ctx.moveTo marginX, marginY ; this.ctx.lineTo marginX, this.height - marginY ; this.ctx.stroke ; // Find data range const minX = Math.min ...this.data - 1; const maxX = Math.max ...this.data + 1; // Draw likelihood function this.ctx.strokeStyle = ' 2196f3'; this.ctx.lineWidth = 2; this.ctx.beginPath ; for let i = 0; i sum + x, 0 / this.data.length; const mleX = marginX + sampleMean - minX / maxX - minX plotWidth; this.ctx.strokeStyle = ' f44336'; this.ctx.lineWidth = 2; this.ctx.beginPath ; this.ctx.moveTo mleX, marginY ; this.ctx.lineTo mleX, this.height - marginY ; this.ctx.stroke ; // Mark true value const trueX = marginX + this.trueMu - minX / maxX - minX plotWidth; this.ctx.strokeStyle = ' 4caf50'; this.ctx.lineWidth = 2; this.ctx.setLineDash 5, 5 ; this.ctx.beginPath ; this.ctx.moveTo trueX, marginY ; this.ctx.lineTo trueX, this.height - marginY ; this.ctx.stroke ; this.ctx.setLineDash ; // Draw data points this.ctx.fillStyle = ' 666'; for const x of this.data const pointX = marginX + x - minX / maxX - minX plotWidth; this.ctx.beginPath ; this.ctx.arc pointX, this.height - marginY + 10, 2, 0, 2 Math.PI ; this.ctx.fill ; // Labels this.ctx.fillStyle = ' 000'; this.ctx.font = '12px Arial'; this.ctx.textAlign = 'center'; this.ctx.fillText 'Î¼', this.width / 2, this.height - 10 ; this.ctx.save ; this.ctx.translate 15, this.height / 2 ; this.ctx.rotate -Math.PI / 2 ; this.ctx.fillText 'Log-Likelihood', 0, 0 ; this.ctx.restore ; // Legend this.ctx.textAlign = 'left'; this.ctx.fillText 'â€” Likelihood', 10, 20 ; this.ctx.fillStyle = ' f44336'; this.ctx.fillText 'â€” MLE', 10, 35 ; this.ctx.fillStyle = ' 4caf50'; this.ctx.fillText '--- Î¼ Thá»±c', 10, 50 ; // Initialize when DOM is loaded document.addEventListener 'DOMContentLoaded', function new SampleSpaceDemo ; new ConditionalProbDemo ; new MLEDemo ; ; input type=\"range\" -webkit-appearance: none; appearance: none; height: 5px; background: ddd; outline: none; border-radius: 5px; input type=\"range\" ::-webkit-slider-thumb -webkit-appearance: none; appearance: none; width: 15px; height: 15px; background: 007bff; cursor: pointer; border-radius: 50%; input type=\"range\" ::-moz-range-thumb width: 15px; height: 15px; background: 007bff; cursor: pointer; border-radius: 50%; border: none; canvas border-radius: 5px; .demo-container margin: 20px 0;",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_04_01_Basic_Probability_Theory/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_05_Quiz_Basic_Concepts",
    "title": "00-05 BÃ i táº­p tráº¯c nghiá»‡m - KhÃ¡i niá»‡m cÆ¡ báº£n",
    "chapter": "00",
    "order": 15,
    "owner": "GitHub Copilot",
    "lesson_type": "quiz",
    "content": "BÃ i táº­p tráº¯c nghiá»‡m nÃ y kiá»ƒm tra hiá»ƒu biáº¿t cá»§a báº¡n vá» cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n trong giáº£i tÃ­ch vÃ  Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh, lÃ  ná»n táº£ng cho tá»‘i Æ°u hÃ³a. --- ðŸ“š Ã”n táº­p lÃ½ thuyáº¿t TrÆ°á»›c khi lÃ m bÃ i táº­p, hÃ£y Ã´n láº¡i cÃ¡c khÃ¡i niá»‡m chÃ­nh trong chÆ°Æ¡ng nÃ y: ðŸ”¢ Giáº£i tÃ­ch cÆ¡ báº£n Äáº¡o hÃ m vÃ  Ä‘áº¡o hÃ m riÃªng: - Äáº¡o hÃ m: MATH - Äáº¡o hÃ m riÃªng: MATH - Ä‘áº¡o hÃ m theo MATH khi cÃ¡c biáº¿n khÃ¡c cá»‘ Ä‘á»‹nh Gradient vÃ  Ä‘áº¡o hÃ m cÃ³ hÆ°á»›ng: - Gradient: MATH - vector chá»‰ hÆ°á»›ng tÄƒng dá»‘c nháº¥t - Äáº¡o hÃ m cÃ³ hÆ°á»›ng: MATH Chuá»—i Taylor: - Báº­c 1: MATH - Báº­c 2: MATH ðŸ“ Äáº¡i sá»‘ tuyáº¿n tÃ­nh Vector vÃ  khÃ´ng gian vector: - Vector: danh sÃ¡ch cÃ³ thá»© tá»± cÃ¡c sá»‘ MATH - KhÃ´ng gian MATH : táº­p táº¥t cáº£ vector cÃ³ MATH thÃ nh pháº§n CÃ¡c phÃ©p toÃ¡n vector: - Cá»™ng vector: MATH - NhÃ¢n vÃ´ hÆ°á»›ng: MATH - TÃ­ch vÃ´ hÆ°á»›ng: MATH Chuáº©n vector: - Chuáº©n L2 Euclid : MATH - Chuáº©n L1 Manhattan : MATH - Chuáº©n Lâˆž Max : MATH Ma tráº­n: - PhÃ©p nhÃ¢n ma tráº­n: MATH - Ma tráº­n nghá»‹ch Ä‘áº£o: MATH náº¿u tá»“n táº¡i - Äá»‹nh thá»©c: MATH - Ä‘o \"thá»ƒ tÃ­ch\" biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh Eigenvalue vÃ  eigenvector: - MATH vá»›i MATH - MATH : eigenvalue, MATH : eigenvector tÆ°Æ¡ng á»©ng ðŸ“Š LÃ½ thuyáº¿t táº­p há»£p vÃ  giáº£i tÃ­ch thá»±c Táº­p há»£p cÆ¡ báº£n: - Táº­p má»Ÿ: má»i Ä‘iá»ƒm Ä‘á»u cÃ³ lÃ¢n cáº­n náº±m trong táº­p - Táº­p Ä‘Ã³ng: chá»©a táº¥t cáº£ Ä‘iá»ƒm biÃªn - Táº­p compact: Ä‘Ã³ng vÃ  bá»‹ cháº·n trong MATH TÃ­nh liÃªn tá»¥c: - HÃ m liÃªn tá»¥c táº¡i MATH : MATH - LiÃªn tá»¥c Ä‘á»u: MATH sao cho MATH \\lvert x-y \\rvert BÃ i táº­p tráº¯c nghiá»‡m: KhÃ¡i niá»‡m cÆ¡ báº£n Chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng nháº¥t cho má»—i cÃ¢u há»i. Báº¡n sáº½ nháº­n Ä‘Æ°á»£c káº¿t quáº£ ngay sau khi hoÃ n thÃ nh. CÃ¢u há»i 1/25 CÃ¢u 1: Äáº¡o hÃ m cá»§a hÃ m sá»‘ MATH táº¡i MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: B MATH MATH MATH CÃ¢u 2: Cho MATH . Äáº¡o hÃ m riÃªng MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH Khi tÃ­nh Ä‘áº¡o hÃ m riÃªng theo MATH , ta coi MATH lÃ  háº±ng sá»‘: MATH CÃ¢u 3: Gradient cá»§a hÃ m MATH táº¡i Ä‘iá»ƒm MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH , MATH Táº¡i MATH : MATH CÃ¢u 4: Cho gradient MATH vÃ  vector Ä‘Æ¡n vá»‹ MATH . Äáº¡o hÃ m cÃ³ hÆ°á»›ng MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH CÃ¢u 5: HÆ°á»›ng tÄƒng dá»‘c nháº¥t cá»§a hÃ m sá»‘ luÃ´n lÃ : A HÆ°á»›ng cá»§a vector gradient B HÆ°á»›ng ngÆ°á»£c vá»›i vector gradient C HÆ°á»›ng vuÃ´ng gÃ³c vá»›i vector gradient D HÆ°á»›ng cá»§a trá»¥c x ÄÃ¡p Ã¡n Ä‘Ãºng: A HÆ°á»›ng cá»§a vector gradient Gradient luÃ´n chá»‰ theo hÆ°á»›ng tÄƒng dá»‘c nháº¥t cá»§a hÃ m sá»‘. ÄÃ¢y lÃ  má»™t tÃ­nh cháº¥t cÆ¡ báº£n cá»§a gradient. CÃ¢u 6: Cho MATH vÃ  MATH . Vector MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH CÃ¢u 7: TÃ­ch vÃ´ hÆ°á»›ng cá»§a MATH vÃ  MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH CÃ¢u 8: Chuáº©n Euclid cá»§a vector MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH CÃ¢u 9: Hai vector MATH vÃ  MATH Ä‘Æ°á»£c gá»i lÃ  trá»±c giao khi: A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH Hai vector trá»±c giao vuÃ´ng gÃ³c khi vÃ  chá»‰ khi tÃ­ch vÃ´ hÆ°á»›ng cá»§a chÃºng báº±ng 0. CÃ¢u 10: TÃ­ch cá»§a hai ma tráº­n MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH Pháº§n tá»­ MATH : MATH Pháº§n tá»­ MATH : MATH Pháº§n tá»­ MATH : MATH Pháº§n tá»­ MATH : MATH CÃ¢u 11: Ma tráº­n Ä‘Æ¡n vá»‹ MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH Ma tráº­n Ä‘Æ¡n vá»‹ cÃ³ cÃ¡c pháº§n tá»­ trÃªn Ä‘Æ°á»ng chÃ©o chÃ­nh báº±ng 1 vÃ  cÃ¡c pháº§n tá»­ khÃ¡c báº±ng 0. CÃ¢u 12: Chuyá»ƒn vá»‹ cá»§a ma tráº­n MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH Chuyá»ƒn vá»‹ cá»§a ma tráº­n Ä‘Æ°á»£c táº¡o báº±ng cÃ¡ch hoÃ¡n Ä‘á»•i hÃ ng vÃ  cá»™t. CÃ¢u 13: Äá»‹nh thá»©c cá»§a ma tráº­n MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH CÃ¢u 14: Hai vector MATH vÃ  MATH lÃ : A Äá»™c láº­p tuyáº¿n tÃ­nh B Phá»¥ thuá»™c tuyáº¿n tÃ­nh C Trá»±c giao D KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh ÄÃ¡p Ã¡n Ä‘Ãºng: B Phá»¥ thuá»™c tuyáº¿n tÃ­nh MATH , do Ä‘Ã³ hai vector nÃ y phá»¥ thuá»™c tuyáº¿n tÃ­nh. CÃ¢u 15: Chuáº©n Manhattan L1 cá»§a vector MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH CÃ¢u 16: Cho MATH vá»›i MATH vÃ  MATH . Äáº¡o hÃ m MATH táº¡i MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH Táº¡i MATH : MATH , nÃªn MATH CÃ¢u 17: Äiá»ƒm tá»›i háº¡n cá»§a hÃ m sá»‘ lÃ  Ä‘iá»ƒm mÃ : A Gradient báº±ng vector khÃ´ng B HÃ m sá»‘ Ä‘áº¡t giÃ¡ trá»‹ lá»›n nháº¥t C HÃ m sá»‘ Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t D HÃ m sá»‘ khÃ´ng xÃ¡c Ä‘á»‹nh ÄÃ¡p Ã¡n Ä‘Ãºng: A Gradient báº±ng vector khÃ´ng Äiá»ƒm tá»›i háº¡n lÃ  Ä‘iá»ƒm mÃ  gradient cá»§a hÃ m sá»‘ báº±ng vector khÃ´ng, tá»©c lÃ  MATH . CÃ¢u 18: Ma tráº­n MATH cÃ³ nghá»‹ch Ä‘áº£o lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH MATH CÃ¢u 19: Gradient cá»§a hÃ m sá»‘ táº¡i má»™t Ä‘iá»ƒm luÃ´n: A VuÃ´ng gÃ³c vá»›i Ä‘Æ°á»ng má»©c táº¡i Ä‘iá»ƒm Ä‘Ã³ B Song song vá»›i Ä‘Æ°á»ng má»©c táº¡i Ä‘iá»ƒm Ä‘Ã³ C Táº¡o gÃ³c 45Â° vá»›i Ä‘Æ°á»ng má»©c D KhÃ´ng cÃ³ má»‘i quan há»‡ vá»›i Ä‘Æ°á»ng má»©c ÄÃ¡p Ã¡n Ä‘Ãºng: A VuÃ´ng gÃ³c vá»›i Ä‘Æ°á»ng má»©c táº¡i Ä‘iá»ƒm Ä‘Ã³ Gradient luÃ´n vuÃ´ng gÃ³c vá»›i Ä‘Æ°á»ng má»©c vÃ¬ Ä‘Æ°á»ng má»©c lÃ  táº­p há»£p cÃ¡c Ä‘iá»ƒm cÃ³ cÃ¹ng giÃ¡ trá»‹ hÃ m sá»‘. CÃ¢u 20: Trong thuáº­t toÃ¡n gradient descent, chÃºng ta di chuyá»ƒn theo hÆ°á»›ng: A NgÆ°á»£c vá»›i hÆ°á»›ng gradient B CÃ¹ng hÆ°á»›ng vá»›i gradient C VuÃ´ng gÃ³c vá»›i gradient D HÆ°á»›ng ngáº«u nhiÃªn ÄÃ¡p Ã¡n Ä‘Ãºng: A NgÆ°á»£c vá»›i hÆ°á»›ng gradient Gradient descent di chuyá»ƒn theo hÆ°á»›ng MATH Ä‘á»ƒ tÃ¬m Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a hÃ m sá»‘. CÃ¢u 21: Äáº¡o hÃ m báº­c hai cá»§a hÃ m MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH MATH CÃ¢u 22: Cho MATH . Ma tráº­n Hessian táº¡i Ä‘iá»ƒm MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH , MATH , MATH Ma tráº­n Hessian khÃ´ng phá»¥ thuá»™c vÃ o Ä‘iá»ƒm cá»¥ thá»ƒ trong trÆ°á»ng há»£p nÃ y. CÃ¢u 23: Khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm MATH Ä‘áº¿n Ä‘iá»ƒm MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH Khoáº£ng cÃ¡ch = MATH CÃ¢u 24: Ma tráº­n MATH cÃ³ eigenvalue lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH Äá»‘i vá»›i ma tráº­n tam giÃ¡c trÃªn, eigenvalue chÃ­nh lÃ  cÃ¡c pháº§n tá»­ trÃªn Ä‘Æ°á»ng chÃ©o chÃ­nh. CÃ¢u 25: Äiá»ƒm cá»±c tiá»ƒu cá»§a hÃ m MATH lÃ : A MATH B MATH C MATH D MATH ÄÃ¡p Ã¡n Ä‘Ãºng: A MATH MATH MATH , nÃªn MATH lÃ  Ä‘iá»ƒm cá»±c tiá»ƒu. CÃ¢u trÆ°á»›c CÃ¢u tiáº¿p Ná»™p bÃ i Káº¿t quáº£ bÃ i táº­p LÃ m láº¡i quiz-container max-width: 800px; margin: 0 auto; padding: 20px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; quiz-header text-align: center; margin-bottom: 30px; progress-bar width: 100%; height: 10px; background-color: e0e0e0; border-radius: 5px; margin: 20px 0; overflow: hidden; progress-fill height: 100%; background: linear-gradient 90deg, 4CAF50, 45a049 ; width: 5%; transition: width 0.3s ease; .question background: f9f9f9; border: 1px solid ddd; border-radius: 8px; padding: 25px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba 0,0,0,0.1 ; .question h3 color: 333; margin-bottom: 20px; font-size: 1.1em; line-height: 1.4; .options display: flex; flex-direction: column; gap: 12px; .options label display: flex; align-items: center; padding: 12px; background: white; border: 2px solid e0e0e0; border-radius: 6px; cursor: pointer; transition: all 0.2s ease; font-size: 1em; .options label:hover border-color: 4CAF50; background-color: f0f8f0; .options input type=\"radio\" margin-right: 12px; transform: scale 1.2 ; .options label.selected border-color: 4CAF50; background-color: e8f5e8; .explanation margin-top: 20px; padding: 15px; background-color: e3f2fd; border-left: 4px solid 2196F3; border-radius: 4px; font-size: 0.95em; line-height: 1.5; .explanation strong color: 1976D2; quiz-navigation text-align: center; margin: 30px 0; display: flex; justify-content: space-between; align-items: center; quiz-navigation button background: 4CAF50; color: white; border: none; padding: 12px 24px; border-radius: 6px; cursor: pointer; font-size: 1em; transition: background-color 0.2s ease; quiz-navigation button:hover:not :disabled background: 45a049; quiz-navigation button:disabled background: cccccc; cursor: not-allowed; quiz-results text-align: center; padding: 30px; background: f0f8f0; border-radius: 8px; border: 2px solid 4CAF50; score-display font-size: 1.5em; font-weight: bold; margin: 20px 0; color: 2E7D32; detailed-results text-align: left; margin: 20px 0; max-height: 400px; overflow-y: auto; .result-item padding: 10px; margin: 5px 0; border-radius: 4px; border-left: 4px solid; .result-item.correct background-color: e8f5e8; border-left-color: 4CAF50; .result-item.incorrect background-color: ffebee; border-left-color: f44336; @media max-width: 600px quiz-container padding: 10px; .question padding: 15px; quiz-navigation flex-direction: column; gap: 10px; quiz-navigation button width: 100%; let currentQuestion = 1; const totalQuestions = 25; let userAnswers = ; let quizSubmitted = false; // ÄÃ¡p Ã¡n Ä‘Ãºng cho tá»«ng cÃ¢u há»i const correctAnswers = q1: 'b', q2: 'a', q3: 'a', q4: 'a', q5: 'a', q6: 'a', q7: 'a', q8: 'a', q9: 'a', q10: 'a', q11: 'a', q12: 'a', q13: 'a', q14: 'b', q15: 'a', q16: 'a', q17: 'a', q18: 'a', q19: 'a', q20: 'a', q21: 'a', q22: 'a', q23: 'a', q24: 'a', q25: 'a' ; / Cáº­p nháº­t thanh tiáº¿n trÃ¬nh vÃ  hiá»ƒn thá»‹ cÃ¢u há»i hiá»‡n táº¡i / function updateProgress const progressFill = document.getElementById 'progress-fill' ; const progressText = document.getElementById 'progress-text' ; const percentage = currentQuestion / totalQuestions 100; progressFill.style.width = percentage + '%'; progressText.textContent = CÃ¢u há»i MATH totalQuestions ; / Hiá»ƒn thá»‹ cÃ¢u há»i theo sá»‘ thá»© tá»± / function showQuestion questionNum // áº¨n táº¥t cáº£ cÃ¢u há»i for let i = 1; i 1 currentQuestion--; showQuestion currentQuestion ; / LÆ°u Ä‘Ã¡p Ã¡n cá»§a ngÆ°á»i dÃ¹ng / function saveAnswer questionId, answer userAnswers questionId = answer; // Cáº­p nháº­t giao diá»‡n Ä‘á»ƒ hiá»ƒn thá»‹ Ä‘Ã¡p Ã¡n Ä‘Ã£ chá»n const labels = document.querySelectorAll $ questionId .options label ; labels.forEach label => label.classList.remove 'selected' ; if label.querySelector 'input' .value === answer label.classList.add 'selected' ; ; / Ná»™p bÃ i vÃ  hiá»ƒn thá»‹ káº¿t quáº£ / function submitQuiz if quizSubmitted return; quizSubmitted = true; let correctCount = 0; let detailedResults = ''; // TÃ­nh Ä‘iá»ƒm vÃ  táº¡o bÃ¡o cÃ¡o chi tiáº¿t for let i = 1; i CÃ¢u MATH isCorrect ? 'ÄÃºng' : 'Sai' MATH userAnswer || 'ChÆ°a tráº£ lá»i' | ÄÃ¡p Ã¡n Ä‘Ãºng: $ correctAnswer : '' ; // Hiá»ƒn thá»‹ káº¿t quáº£ const scorePercentage = Math.round correctCount / totalQuestions 100 ; document.getElementById 'score-display' .innerHTML = Äiá»ƒm sá»‘: MATH totalQuestions $ scorePercentage % $ scorePercentage >= 80 ? 'ðŸŽ‰ Xuáº¥t sáº¯c!' : scorePercentage >= 60 ? 'ðŸ‘ KhÃ¡ tá»‘t!' : scorePercentage >= 40 ? 'ðŸ“š Cáº§n Ã´n táº­p thÃªm' : 'ðŸ’ª HÃ£y cá»‘ gáº¯ng hÆ¡n!' ; document.getElementById 'detailed-results' .innerHTML = detailedResults; document.getElementById 'quiz-results' .style.display = 'block'; document.getElementById 'quiz-navigation' .style.display = 'none'; // Cuá»™n Ä‘áº¿n káº¿t quáº£ document.getElementById 'quiz-results' .scrollIntoView behavior: 'smooth' ; / Khá»Ÿi Ä‘á»™ng láº¡i bÃ i táº­p / function restartQuiz currentQuestion = 1; userAnswers = ; quizSubmitted = false; // áº¨n káº¿t quáº£ vÃ  hiá»ƒn thá»‹ láº¡i Ä‘iá»u hÆ°á»›ng document.getElementById 'quiz-results' .style.display = 'none'; document.getElementById 'quiz-navigation' .style.display = 'flex'; // áº¨n táº¥t cáº£ giáº£i thÃ­ch document.querySelectorAll '.explanation' .forEach exp => exp.style.display = 'none'; ; // XÃ³a táº¥t cáº£ lá»±a chá»n document.querySelectorAll 'input type=\"radio\" ' .forEach input => input.checked = false; ; document.querySelectorAll '.options label' .forEach label => label.classList.remove 'selected' ; ; // Hiá»ƒn thá»‹ cÃ¢u há»i Ä‘áº§u tiÃªn showQuestion 1 ; // Cuá»™n lÃªn Ä‘áº§u document.getElementById 'quiz-header' .scrollIntoView behavior: 'smooth' ; // Khá»Ÿi táº¡o bÃ i táº­p khi trang Ä‘Æ°á»£c táº£i document.addEventListener 'DOMContentLoaded', function showQuestion 1 ; // ThÃªm event listener cho táº¥t cáº£ radio button document.querySelectorAll 'input type=\"radio\" ' .forEach input => input.addEventListener 'change', function const questionId = this.name; const answer = this.value; saveAnswer questionId, answer ; ; ; // Render MathJax sau khi DOM Ä‘Æ°á»£c táº£i if window.MathJax MathJax.typesetPromise ; ; // Xá»­ lÃ½ phÃ­m táº¯t document.addEventListener 'keydown', function event if quizSubmitted return; switch event.key case 'ArrowLeft': if currentQuestion > 1 previousQuestion ; break; case 'ArrowRight': if currentQuestion",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_05_Quiz_Basic_Concepts/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_04_02_Common_Probability_Distributions",
    "title": "00-04-02 CÃ¡c PhÃ¢n Phá»‘i XÃ¡c Suáº¥t ThÃ´ng Dá»¥ng",
    "chapter": "00",
    "order": 15,
    "owner": "AI Assistant",
    "lesson_type": "required",
    "content": "CÃ¡c PhÃ¢n Phá»‘i XÃ¡c Suáº¥t ThÃ´ng Dá»¥ng Hiá»ƒu cÃ¡c phÃ¢n phá»‘i xÃ¡c suáº¥t chÃ­nh lÃ  Ä‘iá»u thiáº¿t yáº¿u cho cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a trong há»c mÃ¡y vÃ  thá»‘ng kÃª. CÃ¡c phÃ¢n phá»‘i nÃ y thÆ°á»ng xuáº¥t hiá»‡n nhÆ° giáº£ thuyáº¿t trong mÃ´ hÃ¬nh, prior trong phÆ°Æ¡ng phÃ¡p Bayes, vÃ  mÃ´ hÃ¬nh lá»—i trong há»“i quy. 1. PhÃ¢n Phá»‘i Rá»i Ráº¡c PhÃ¢n Phá»‘i Bernoulli MÃ´ hÃ¬nh má»™t thÃ­ nghiá»‡m Ä‘Æ¡n vá»›i hai káº¿t quáº£ thÃ nh cÃ´ng/tháº¥t báº¡i . Tham sá»‘ : MATH xÃ¡c suáº¥t thÃ nh cÃ´ng PMF : MATH vá»›i MATH Ká»³ vá»ng : MATH PhÆ°Æ¡ng sai : MATH á»¨ng dá»¥ng : PhÃ¢n loáº¡i nhá»‹ phÃ¢n, tung Ä‘á»“ng xu, kiá»ƒm Ä‘á»‹nh A/B PhÃ¢n Phá»‘i Nhá»‹ Thá»©c MÃ´ hÃ¬nh sá»‘ láº§n thÃ nh cÃ´ng trong MATH thÃ­ nghiá»‡m Bernoulli Ä‘á»™c láº­p. Tham sá»‘ : MATH sá»‘ thÃ­ nghiá»‡m , MATH xÃ¡c suáº¥t thÃ nh cÃ´ng PMF : MATH vá»›i MATH Ká»³ vá»ng : MATH PhÆ°Æ¡ng sai : MATH PhÃ¢n Phá»‘i Poisson MÃ´ hÃ¬nh sá»‘ sá»± kiá»‡n trong má»™t khoáº£ng thá»i gian cá»‘ Ä‘á»‹nh khi cÃ¡c sá»± kiá»‡n xáº£y ra Ä‘á»™c láº­p vá»›i tá»‘c Ä‘á»™ khÃ´ng Ä‘á»•i. Tham sá»‘ : MATH tham sá»‘ tá»‘c Ä‘á»™ PMF : MATH vá»›i MATH Ká»³ vá»ng : MATH PhÆ°Æ¡ng sai : MATH á»¨ng dá»¥ng : Dá»¯ liá»‡u Ä‘áº¿m, sá»± kiá»‡n hiáº¿m, lÃ½ thuyáº¿t hÃ ng Ä‘á»£i PhÃ¢n Phá»‘i Rá»i Ráº¡c TÆ°Æ¡ng TÃ¡c Trá»±c quan hÃ³a: HÃ m khá»‘i xÃ¡c suáº¥t cá»§a cÃ¡c phÃ¢n phá»‘i rá»i ráº¡c. Loáº¡i PhÃ¢n Phá»‘i Bernoulli Nhá»‹ Thá»©c Poisson p: 0.5 n: 10 Î»: 3.0 Thá»‘ng KÃª: Ká»³ vá»ng: 0.500 PhÆ°Æ¡ng sai: 0.250 Mode: 0 hoáº·c 1 2. PhÃ¢n Phá»‘i LiÃªn Tá»¥c PhÃ¢n Phá»‘i Äá»u Táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ trong má»™t khoáº£ng Ä‘á»u cÃ³ kháº£ nÄƒng xáº£y ra nhÆ° nhau. Tham sá»‘ : MATH vá»›i MATH phÆ°Æ¡ng sai PDF : MATH Ká»³ vá»ng : MATH PhÆ°Æ¡ng sai : MATH TÃ­nh cháº¥t : - Äá»‘i xá»©ng quanh MATH - Quy táº¯c 68-95-99.7 - Äá»‹nh lÃ½ giá»›i háº¡n trung tÃ¢m - Entropy tá»‘i Ä‘a vá»›i ká»³ vá»ng vÃ  phÆ°Æ¡ng sai cho trÆ°á»›c PhÃ¢n Phá»‘i MÅ© MÃ´ hÃ¬nh thá»i gian chá» giá»¯a cÃ¡c sá»± kiá»‡n trong quÃ¡ trÃ¬nh Poisson. Tham sá»‘ : MATH tham sá»‘ tá»‘c Ä‘á»™ PDF : MATH vá»›i MATH Ká»³ vá»ng : MATH PhÆ°Æ¡ng sai : MATH TÃ­nh cháº¥t : TÃ­nh cháº¥t khÃ´ng nhá»› PhÃ¢n Phá»‘i Beta PhÃ¢n phá»‘i linh hoáº¡t trÃªn MATH , thÆ°á»ng dÃ¹ng Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t. Tham sá»‘ : MATH tham sá»‘ hÃ¬nh dáº¡ng PDF : MATH vá»›i MATH Ká»³ vá»ng : MATH PhÆ°Æ¡ng sai : MATH PhÃ¢n Phá»‘i LiÃªn Tá»¥c TÆ°Æ¡ng TÃ¡c Trá»±c quan hÃ³a: HÃ m máº­t Ä‘á»™ xÃ¡c suáº¥t cá»§a cÃ¡c phÃ¢n phá»‘i liÃªn tá»¥c. Loáº¡i PhÃ¢n Phá»‘i Äá»u Chuáº©n MÅ© Beta a: 0 b: 1 Î¼: 0 Ïƒ: 1.0 Î»: 1.0 Î±: 2 Î²: 2 Thá»‘ng KÃª: Ká»³ vá»ng: 0.500 PhÆ°Æ¡ng sai: 0.083 Miá»n xÃ¡c Ä‘á»‹nh: 0, 1 3. PhÃ¢n Phá»‘i Äa Biáº¿n PhÃ¢n Phá»‘i Chuáº©n Äa Biáº¿n Má»Ÿ rá»™ng cá»§a phÃ¢n phá»‘i chuáº©n cho nhiá»u chiá»u. Tham sá»‘ : MATH vector ká»³ vá»ng , MATH ma tráº­n hiá»‡p phÆ°Æ¡ng sai, xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng PDF : MATH TÃ­nh cháº¥t : - PhÃ¢n phá»‘i biÃªn lÃ  chuáº©n - Tá»• há»£p tuyáº¿n tÃ­nh lÃ  chuáº©n - PhÃ¢n phá»‘i cÃ³ Ä‘iá»u kiá»‡n lÃ  chuáº©n PhÃ¢n Phá»‘i Chuáº©n Äa Biáº¿n Trá»±c Quan 2D: Äá»“ thá»‹ Ä‘Æ°á»ng Ä‘á»“ng má»©c cá»§a phÃ¢n phá»‘i chuáº©n hai biáº¿n. CÃ¡c máº«u hiá»ƒn thá»‹ dÆ°á»›i dáº¡ng cháº¥m. Tham Sá»‘ Î¼â‚: 0.0 Î¼â‚‚: 0.0 Ïƒâ‚: 1.0 Ïƒâ‚‚: 1.0 Ï tÆ°Æ¡ng quan : 0.0 Táº¡o Máº«u Ma Tráº­n Hiá»‡p PhÆ°Æ¡ng Sai: Î£â‚â‚: 1.000 Î£â‚â‚‚: 0.000 Î£â‚‚â‚‚: 1.000 Det Î£ : 1.000 4. á»¨ng Dá»¥ng trong Tá»‘i Æ¯u HÃ³a Æ¯á»›c LÆ°á»£ng Há»£p LÃ½ Tá»‘i Äa Nhiá»u bÃ i toÃ¡n tá»‘i Æ°u hÃ³a liÃªn quan Ä‘áº¿n viá»‡c tÃ¬m tham sá»‘ Ä‘á»ƒ tá»‘i Ä‘a hÃ³a likelihood cá»§a dá»¯ liá»‡u quan sÃ¡t dÆ°á»›i má»™t phÃ¢n phá»‘i cá»¥ thá»ƒ: MATH Tá»‘i Æ¯u HÃ³a Bayes PhÃ¢n phá»‘i prior mÃ£ hÃ³a niá»m tin vá» tham sá»‘ trÆ°á»›c khi tháº¥y dá»¯ liá»‡u: MATH Regularization PhÃ¢n phá»‘i cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° prior Ä‘á»ƒ regularize bÃ i toÃ¡n tá»‘i Æ°u hÃ³a: - L2 regularization â†” Prior Gaussian - L1 regularization â†” Prior Laplace Tá»‘i Æ¯u HÃ³a Ngáº«u NhiÃªn PhÃ¢n phá»‘i mÃ´ hÃ¬nh nhiá»…u vÃ  sá»± báº¥t Ä‘á»‹nh trong hÃ m má»¥c tiÃªu vÃ  rÃ ng buá»™c. Nhá»¯ng Hiá»ƒu Biáº¿t Quan Trá»ng cho Tá»‘i Æ¯u HÃ³a 1. Lá»±a Chá»n MÃ´ HÃ¬nh : Chá»n phÃ¢n phá»‘i phÃ¹ há»£p vá»›i Ä‘áº·c Ä‘iá»ƒm dá»¯ liá»‡u cá»§a báº¡n 2. Æ¯á»›c LÆ°á»£ng Tham Sá»‘ : Sá»­ dá»¥ng MLE hoáº·c phÆ°Æ¡ng phÃ¡p Bayes Ä‘á»ƒ Æ°á»›c lÆ°á»£ng tham sá»‘ phÃ¢n phá»‘i 3. Äá»‹nh LÆ°á»£ng Sá»± Báº¥t Äá»‹nh : PhÃ¢n phá»‘i cung cáº¥p cÃ¡ch tá»± nhiÃªn Ä‘á»ƒ Ä‘á»‹nh lÆ°á»£ng sá»± báº¥t Ä‘á»‹nh 4. Regularization : PhÃ¢n phá»‘i prior cÃ³ thá»ƒ ngÄƒn overfitting 5. Hiá»‡u Quáº£ TÃ­nh ToÃ¡n : Má»™t sá»‘ phÃ¢n phá»‘i cÃ³ nghiá»‡m dáº¡ng Ä‘Ã³ng cho cÃ¡c phÃ©p toÃ¡n thÃ´ng dá»¥ng Hiá»ƒu cÃ¡c phÃ¢n phá»‘i nÃ y vÃ  tÃ­nh cháº¥t cá»§a chÃºng lÃ  quan trá»ng Ä‘á»ƒ xÃ¢y dá»±ng vÃ  giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a trong há»c mÃ¡y, thá»‘ng kÃª vÃ  cÃ¡c á»©ng dá»¥ng ká»¹ thuáº­t. // Discrete Distributions Demo class DiscreteDistributionsDemo constructor this.canvas = document.getElementById 'discreteCanvas' ; this.ctx = this.canvas.getContext '2d' ; this.width = this.canvas.width; this.height = this.canvas.height; this.distType = 'bernoulli'; this.params = p: 0.5, n: 10, lambda: 3.0 ; this.setupControls ; this.draw ; setupControls const radios = document.querySelectorAll 'input name=\"discrete-dist\" ' ; const pSlider = document.getElementById 'p-slider' ; const nSlider = document.getElementById 'n-slider' ; const lambdaSlider = document.getElementById 'lambda-slider' ; radios.forEach radio => radio.addEventListener 'change', e => this.distType = e.target.value; this.updateParameterVisibility ; this.updateStats ; this.draw ; ; ; pSlider.addEventListener 'input', e => this.params.p = parseFloat e.target.value ; document.getElementById 'p-value' .textContent = this.params.p.toFixed 1 ; this.updateStats ; this.draw ; ; nSlider.addEventListener 'input', e => this.params.n = parseInt e.target.value ; document.getElementById 'n-value' .textContent = this.params.n; this.updateStats ; this.draw ; ; lambdaSlider.addEventListener 'input', e => this.params.lambda = parseFloat e.target.value ; document.getElementById 'lambda-value' .textContent = this.params.lambda.toFixed 1 ; this.updateStats ; this.draw ; ; this.updateParameterVisibility ; this.updateStats ; updateParameterVisibility document.getElementById 'p-param' .style.display = this.distType === 'bernoulli' || this.distType === 'binomial' ? 'block' : 'none'; document.getElementById 'n-param' .style.display = this.distType === 'binomial' ? 'block' : 'none'; document.getElementById 'lambda-param' .style.display = this.distType === 'poisson' ? 'block' : 'none'; updateStats let mean, variance, mode; switch this.distType case 'bernoulli': mean = this.params.p; variance = this.params.p 1 - this.params.p ; mode = this.params.p > 0.5 ? '1' : this.params.p n return 0; return this.factorial n / this.factorial k this.factorial n - k ; getProbability k switch this.distType case 'bernoulli': return k === 0 ? 1 - this.params.p : k === 1 ? this.params.p : 0 ; case 'binomial': if k this.params.n return 0; return this.binomialCoeff this.params.n, k Math.pow this.params.p, k Math.pow 1 - this.params.p, this.params.n - k ; case 'poisson': if k radio.addEventListener 'change', e => this.distType = e.target.value; this.updateParameterVisibility ; this.updateStats ; this.draw ; ; ; // Setup all sliders const sliders = 'a', 'b', 'mu', 'sigma', 'exp-lambda', 'alpha', 'beta' ; sliders.forEach slider => const element = document.getElementById slider + '-slider' ; if element element.addEventListener 'input', e => const value = parseFloat e.target.value ; const param = slider === 'exp-lambda' ? 'lambda' : slider; this.params param = value; const valueSpan = document.getElementById slider + '-value' ; if valueSpan valueSpan.textContent = value.toFixed 1 ; this.updateStats ; this.draw ; ; ; this.updateParameterVisibility ; this.updateStats ; updateParameterVisibility document.getElementById 'uniform-params' .style.display = this.distType === 'uniform' ? 'block' : 'none'; document.getElementById 'normal-params' .style.display = this.distType === 'normal' ? 'block' : 'none'; document.getElementById 'exponential-params' .style.display = this.distType === 'exponential' ? 'block' : 'none'; document.getElementById 'beta-params' .style.display = this.distType === 'beta' ? 'block' : 'none'; updateStats let mean, variance, support; switch this.distType case 'uniform': mean = this.params.a + this.params.b / 2; variance = Math.pow this.params.b - this.params.a, 2 / 12; support = MATH this.params.b ; break; case 'normal': mean = this.params.mu; variance = this.params.sigma this.params.sigma; support = ' -âˆž, âˆž '; break; case 'exponential': mean = 1 / this.params.lambda; variance = 1 / this.params.lambda this.params.lambda ; support = ' 0, âˆž '; break; case 'beta': mean = this.params.alpha / this.params.alpha + this.params.beta ; variance = this.params.alpha this.params.beta / Math.pow this.params.alpha + this.params.beta, 2 this.params.alpha + this.params.beta + 1 ; support = ' 0, 1 '; break; document.getElementById 'continuous-mean' .textContent = mean.toFixed 3 ; document.getElementById 'continuous-variance' .textContent = variance.toFixed 3 ; document.getElementById 'continuous-support' .textContent = support; gamma z // Stirling's approximation for gamma function if z = this.params.a && x = 0 ? this.params.lambda Math.exp -this.params.lambda x : 0; case 'beta': if x 1 return 0; const B = this.gamma this.params.alpha this.gamma this.params.beta / this.gamma this.params.alpha + this.params.beta ; return Math.pow x, this.params.alpha - 1 Math.pow 1 - x, this.params.beta - 1 / B; getRange switch this.distType case 'uniform': return this.params.a - 0.5, this.params.b + 0.5 ; case 'normal': return this.params.mu - 4 this.params.sigma, this.params.mu + 4 this.params.sigma ; case 'exponential': return 0, 5 / this.params.lambda ; case 'beta': return 0, 1 ; draw this.ctx.clearRect 0, 0, this.width, this.height ; const marginX = 50; const marginY = 50; const plotWidth = this.width - 2 marginX; const plotHeight = this.height - 2 marginY; // Draw axes this.ctx.strokeStyle = ' ddd'; this.ctx.lineWidth = 1; this.ctx.beginPath ; this.ctx.moveTo marginX, this.height - marginY ; this.ctx.lineTo this.width - marginX, this.height - marginY ; this.ctx.moveTo marginX, marginY ; this.ctx.lineTo marginX, this.height - marginY ; this.ctx.stroke ; const minX, maxX = this.getRange ; // Find max PDF for scaling let maxPDF = 0; for let i = 0; i const element = document.getElementById slider + '-slider' ; element.addEventListener 'input', e => this.params slider = parseFloat e.target.value ; document.getElementById slider + '-value' .textContent = this.params slider .toFixed 1 ; this.updateStats ; this.draw ; ; ; document.getElementById 'generate-samples' .addEventListener 'click', => this.generateSamples ; this.draw ; ; this.updateStats ; updateStats const cov11 = this.params.sigma1 this.params.sigma1; const cov12 = this.params.rho this.params.sigma1 this.params.sigma2; const cov22 = this.params.sigma2 this.params.sigma2; const det = cov11 cov22 - cov12 cov12; document.getElementById 'cov11' .textContent = cov11.toFixed 3 ; document.getElementById 'cov12' .textContent = cov12.toFixed 3 ; document.getElementById 'cov22' .textContent = cov22.toFixed 3 ; document.getElementById 'det-cov' .textContent = det.toFixed 3 ; generateSamples this.samples = ; const n = 100; for let i = 0; i this.ctx.strokeStyle = colors idx ; this.ctx.lineWidth = 1; this.ctx.beginPath ; const a = level this.params.sigma1; const b = level this.params.sigma2; const angle = 0.5 Math.atan2 2 this.params.rho this.params.sigma1 this.params.sigma2, this.params.sigma1 this.params.sigma1 - this.params.sigma2 this.params.sigma2 ; for let i = 0; i 0 this.ctx.fillStyle = ' 2196f3'; this.samples.forEach x1, x2 => const plotX = marginX + x1 + 4 / 8 plotWidth; const plotY = this.height - marginY - x2 + 4 / 8 plotHeight; if plotX >= marginX && plotX = marginY && plotY input type=\"range\" -webkit-appearance: none; appearance: none; height: 5px; background: ddd; outline: none; border-radius: 5px; input type=\"range\" ::-webkit-slider-thumb -webkit-appearance: none; appearance: none; width: 15px; height: 15px; background: 007bff; cursor: pointer; border-radius: 50%; input type=\"range\" ::-moz-range-thumb width: 15px; height: 15px; background: 007bff; cursor: pointer; border-radius: 50%; border: none; canvas border-radius: 5px; .demo-container margin: 20px 0;",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_04_02_Common_Probability_Distributions/",
    "lang": "vi"
  },
  {
    "id": "/contents/vi/chapter00/00_04_03_Likelihood_and_Maximum_Likelihood_Estimation",
    "title": "00-04-03 Likelihood vÃ  Æ¯á»›c LÆ°á»£ng Há»£p LÃ½ Tá»‘i Äa (MLE)",
    "chapter": "00",
    "order": 16,
    "owner": "AI Assistant",
    "lesson_type": "required",
    "content": "Likelihood vÃ  Æ¯á»›c LÆ°á»£ng Há»£p LÃ½ Tá»‘i Äa Trong xÃ¡c suáº¥t thá»‘ng kÃª, likelihood táº¡m dá»‹ch: kháº£ nÄƒng xáº£y ra hoáº·c Ä‘á»™ há»£p lÃ½ lÃ  má»™t khÃ¡i niá»‡m quan trá»ng, biá»ƒu thá»‹ xÃ¡c suáº¥t quan sÃ¡t Ä‘Æ°á»£c dá»¯ liá»‡u cá»¥ thá»ƒ khi giáº£ Ä‘á»‹nh má»™t mÃ´ hÃ¬nh hoáº·c má»™t táº­p há»£p cÃ¡c tham sá»‘ nháº¥t Ä‘á»‹nh lÃ  Ä‘Ãºng. Tuy nhiÃªn, likelihood khÃ´ng pháº£i lÃ  xÃ¡c suáº¥t thÃ´ng thÆ°á»ng mÃ  lÃ  má»™t hÃ m Ä‘o lÆ°á»ng má»©c Ä‘á»™ phÃ¹ há»£p cá»§a mÃ´ hÃ¬nh vá»›i dá»¯ liá»‡u. Äá»‹nh NghÄ©a Likelihood Likelihood cá»§a má»™t tham sá»‘ MATH hoáº·c má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  xÃ¡c suáº¥t cá»§a dá»¯ liá»‡u quan sÃ¡t Ä‘Æ°á»£c, giáº£ sá»­ tham sá»‘ MATH lÃ  Ä‘Ãºng. NÃ³ thÆ°á»ng Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  MATH , trong Ä‘Ã³ MATH lÃ  dá»¯ liá»‡u quan sÃ¡t Ä‘Æ°á»£c. CÃ´ng thá»©c: MATH á»ž Ä‘Ã¢y, MATH lÃ  xÃ¡c suáº¥t xáº£y ra dá»¯ liá»‡u MATH khi tham sá»‘ MATH Ä‘Æ°á»£c cho trÆ°á»›c. Äiá»ƒm KhÃ¡c Biá»‡t vá»›i XÃ¡c Suáº¥t - XÃ¡c suáº¥t mÃ´ táº£ kháº£ nÄƒng xáº£y ra cá»§a má»™t sá»± kiá»‡n trong tÆ°Æ¡ng lai, vá»›i cÃ¡c tham sá»‘ Ä‘Ã£ biáº¿t. - Likelihood ngÆ°á»£c láº¡i, Ä‘o lÆ°á»ng má»©c Ä‘á»™ phÃ¹ há»£p cá»§a cÃ¡c tham sá»‘ khÃ¡c nhau Ä‘á»‘i vá»›i dá»¯ liá»‡u Ä‘Ã£ quan sÃ¡t. âš ï¸ LÆ°u Ã½ quan trá»ng: Likelihood khÃ´ng pháº£i lÃ  xÃ¡c suáº¥t cá»§a tham sá»‘ Î¸, vÃ¬ nÃ³ khÃ´ng Ä‘Æ°á»£c chuáº©n hÃ³a nhÆ° má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t tá»•ng tÃ­ch phÃ¢n cá»§a likelihood khÃ´ng nháº¥t thiáº¿t báº±ng 1 . VÃ­ Dá»¥ Minh Há»a: Tung Äá»“ng Xu Giáº£ sá»­ báº¡n tung má»™t Ä‘á»“ng xu vÃ  quan sÃ¡t Ä‘Æ°á»£c káº¿t quáº£ lÃ  3 láº§n \"máº·t sáº¥p\" S vÃ  2 láº§n \"máº·t ngá»­a\" N trong 5 láº§n tung. Báº¡n muá»‘n biáº¿t xÃ¡c suáº¥t tung Ä‘Æ°á»£c máº·t sáº¥p lÃ  MATH . Likelihood cá»§a MATH lÃ  xÃ¡c suáº¥t quan sÃ¡t Ä‘Æ°á»£c dá»¯ liá»‡u 3 sáº¥p, 2 ngá»­a khi MATH Ä‘Æ°á»£c cho trÆ°á»›c: MATH Trong Ä‘Ã³: - MATH lÃ  há»‡ sá»‘ nhá»‹ thá»©c - MATH lÃ  xÃ¡c suáº¥t cho 3 láº§n sáº¥p - MATH lÃ  xÃ¡c suáº¥t cho 2 láº§n ngá»­a Thá»­ nghiá»‡m vá»›i cÃ¡c giÃ¡ trá»‹ khÃ¡c nhau: Vá»›i MATH : MATH Vá»›i MATH : MATH Likelihood cao hÆ¡n vá»›i MATH cho tháº¥y giÃ¡ trá»‹ nÃ y phÃ¹ há»£p hÆ¡n vá»›i dá»¯ liá»‡u quan sÃ¡t Ä‘Æ°á»£c so vá»›i MATH . VÃ­ Dá»¥ TÆ°Æ¡ng TÃ¡c: Tung Äá»“ng Xu ThÃ­ Nghiá»‡m Tung Äá»“ng Xu XÃ¡c suáº¥t máº·t sáº¥p p : 0.5 Sá»‘ láº§n sáº¥p: Sá»‘ láº§n ngá»­a: Likelihood hiá»‡n táº¡i: 0.3125 Log-likelihood: -1.16 document.addEventListener 'DOMContentLoaded', function const pSlider = document.getElementById 'p-slider' ; const pValue = document.getElementById 'p-value' ; const headsInput = document.getElementById 'heads-input' ; const tailsInput = document.getElementById 'tails-input' ; const canvas = document.getElementById 'likelihood-plot' ; const ctx = canvas.getContext '2d' ; const likelihoodResult = document.getElementById 'current-likelihood' ; const logLikelihoodResult = document.getElementById 'current-log-likelihood' ; function binomialCoeff n, k if k > n return 0; if k === 0 || k === n return 1; let result = 1; for let i = 0; i Maximum Likelihood Estimation MLE Æ¯á»›c lÆ°á»£ng há»£p lÃ½ tá»‘i Ä‘a lÃ  phÆ°Æ¡ng phÃ¡p tÃ¬m giÃ¡ trá»‹ cá»§a tham sá»‘ MATH sao cho hÃ m likelihood MATH Ä‘áº¡t giÃ¡ trá»‹ lá»›n nháº¥t. CÃ´ng thá»©c MLE: MATH ThÆ°á»ng thÃ¬ viá»‡c tá»‘i Ä‘a hÃ³a log-likelihood dá»… dÃ ng hÆ¡n: MATH VÃ­ Dá»¥ MLE: PhÃ¢n Phá»‘i Chuáº©n Giáº£ sá»­ chÃºng ta cÃ³ MATH quan sÃ¡t Ä‘á»™c láº­p MATH tá»« phÃ¢n phá»‘i chuáº©n MATH . HÃ m likelihood: MATH Log-likelihood: MATH Æ¯á»›c lÆ°á»£ng MLE: Äá»ƒ tÃ¬m MATH , láº¥y Ä‘áº¡o hÃ m theo MATH vÃ  Ä‘áº·t báº±ng 0: MATH Giáº£i Ä‘Æ°á»£c: MATH TÆ°Æ¡ng tá»± cho MATH : MATH VÃ­ Dá»¥ TÆ°Æ¡ng TÃ¡c: MLE cho PhÃ¢n Phá»‘i Chuáº©n MLE cho PhÃ¢n Phá»‘i Chuáº©n Táº¡o Dá»¯ Liá»‡u Má»›i Sá»‘ máº«u: Æ¯á»›c lÆ°á»£ng Î¼: 0 Æ¯á»›c lÆ°á»£ng Ïƒ: 1 Dá»¯ liá»‡u thá»±c: Î¼ = 1.0 , Ïƒ = 1.5 MLE Æ°á»›c lÆ°á»£ng: Î¼Ì‚ = 0.95 , ÏƒÌ‚ = 1.48 Æ¯á»›c lÆ°á»£ng hiá»‡n táº¡i: Î¼ = 0 , Ïƒ = 1 Log-likelihood hiá»‡n táº¡i: -75.2 Log-likelihood tá»‘i Ä‘a: -72.1 document.addEventListener 'DOMContentLoaded', function const generateBtn = document.getElementById 'generate-data' ; const sampleSizeInput = document.getElementById 'sample-size' ; const muGuess = document.getElementById 'mu-guess' ; const sigmaGuess = document.getElementById 'sigma-guess' ; const muGuessValue = document.getElementById 'mu-guess-value' ; const sigmaGuessValue = document.getElementById 'sigma-guess-value' ; const canvas = document.getElementById 'normal-plot' ; const ctx = canvas.getContext '2d' ; let data = ; let trueMu = 1.0; let trueSigma = 1.5; function normalRandom mu, sigma let u = 0, v = 0; while u === 0 u = Math.random ; while v === 0 v = Math.random ; return Math.sqrt -2.0 Math.log u Math.cos 2.0 Math.PI v sigma + mu; function generateData const n = parseInt sampleSizeInput.value ; trueMu = Math.random - 0.5 4; // -2 to 2 trueSigma = 0.5 + Math.random 2; // 0.5 to 2.5 data = ; for let i = 0; i sum + x, 0 / data.length; const mleSigma = Math.sqrt data.reduce sum, x => sum + x - mleMu 2, 0 / data.length ; document.getElementById 'true-mu' .textContent = trueMu.toFixed 2 ; document.getElementById 'true-sigma' .textContent = trueSigma.toFixed 2 ; document.getElementById 'mle-mu' .textContent = mleMu.toFixed 2 ; document.getElementById 'mle-sigma' .textContent = mleSigma.toFixed 2 ; plotData ; function logLikelihood mu, sigma, data const n = data.length; const sumSquares = data.reduce sum, x => sum + x - mu 2, 0 ; return -n/2 Math.log 2 Math.PI - n Math.log sigma - sumSquares / 2 sigma 2 ; function normalPDF x, mu, sigma return 1 / sigma Math.sqrt 2 Math.PI Math.exp -0.5 x - mu / sigma 2 ; function plotData if data.length === 0 return; ctx.clearRect 0, 0, canvas.width, canvas.height ; const currentMu = parseFloat muGuess.value ; const currentSigma = parseFloat sigmaGuess.value ; // TÃ¬m pháº¡m vi dá»¯ liá»‡u const minX = Math.min ...data - 1; const maxX = Math.max ...data + 1; const range = maxX - minX; // Váº½ histogram const bins = 20; const binWidth = range / bins; const binCounts = new Array bins .fill 0 ; data.forEach x => const binIndex = Math.floor x - minX / binWidth ; if binIndex >= 0 && binIndex sum + x, 0 / data.length; const mleSigma = Math.sqrt data.reduce sum, x => sum + x - mleMu 2, 0 / data.length ; const maxLL = logLikelihood mleMu, mleSigma, data ; document.getElementById 'current-mu' .textContent = currentMu.toFixed 2 ; document.getElementById 'current-sigma' .textContent = currentSigma.toFixed 2 ; document.getElementById 'current-ll' .textContent = currentLL.toFixed 2 ; document.getElementById 'max-ll' .textContent = maxLL.toFixed 2 ; // ThÃªm chÃº thÃ­ch ctx.fillStyle = ' 4CAF50'; ctx.fillRect 450, 60, 15, 3 ; ctx.fillStyle = ' 333'; ctx.font = '12px Arial'; ctx.fillText 'PhÃ¢n phá»‘i thá»±c', 470, 65 ; ctx.strokeStyle = ' f44336'; ctx.setLineDash 5, 5 ; ctx.beginPath ; ctx.moveTo 450, 80 ; ctx.lineTo 465, 80 ; ctx.stroke ; ctx.setLineDash ; ctx.fillText 'Æ¯á»›c lÆ°á»£ng hiá»‡n táº¡i', 470, 85 ; function updateDisplay muGuessValue.textContent = muGuess.value; sigmaGuessValue.textContent = sigmaGuess.value; plotData ; generateBtn.addEventListener 'click', generateData ; muGuess.addEventListener 'input', updateDisplay ; sigmaGuess.addEventListener 'input', updateDisplay ; sampleSizeInput.addEventListener 'change', generateData ; // Khá»Ÿi táº¡o generateData ; ; TÃ­nh Cháº¥t cá»§a MLE 1. TÃ­nh Nháº¥t QuÃ¡n Consistency : Khi kÃ­ch thÆ°á»›c máº«u tÄƒng, Æ°á»›c lÆ°á»£ng MLE há»™i tá»¥ vá» giÃ¡ trá»‹ thá»±c cá»§a tham sá»‘. 2. TÃ­nh Tiá»‡m Cáº­n Chuáº©n Asymptotic Normality : Vá»›i máº«u lá»›n, phÃ¢n phá»‘i cá»§a Æ°á»›c lÆ°á»£ng MLE xáº¥p xá»‰ phÃ¢n phá»‘i chuáº©n. 3. TÃ­nh Hiá»‡u Quáº£ Tiá»‡m Cáº­n Asymptotic Efficiency : MLE Ä‘áº¡t Ä‘Æ°á»£c cáº­n CramÃ©r-Rao, nghÄ©a lÃ  cÃ³ phÆ°Æ¡ng sai nhá» nháº¥t trong cÃ¡c Æ°á»›c lÆ°á»£ng khÃ´ng thiÃªn lá»‡ch. 4. Báº¥t Biáº¿n Invariance : Náº¿u MATH lÃ  MLE cá»§a MATH , thÃ¬ MATH lÃ  MLE cá»§a MATH vá»›i MATH lÃ  hÃ m kháº£ nghá»‹ch. á»¨ng Dá»¥ng cá»§a MLE 1. Æ¯á»›c LÆ°á»£ng Tham Sá»‘ MLE Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ Æ°á»›c lÆ°á»£ng tham sá»‘ trong cÃ¡c mÃ´ hÃ¬nh thá»‘ng kÃª: - PhÃ¢n phá»‘i chuáº©n: Æ°á»›c lÆ°á»£ng trung bÃ¬nh vÃ  phÆ°Æ¡ng sai - PhÃ¢n phá»‘i Poisson: Æ°á»›c lÆ°á»£ng tham sá»‘ Î» - Há»“i quy tuyáº¿n tÃ­nh: Æ°á»›c lÆ°á»£ng há»‡ sá»‘ há»“i quy 2. So SÃ¡nh MÃ´ HÃ¬nh Likelihood Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh thá»‘ng kÃª khÃ¡c nhau: - Likelihood Ratio Test : So sÃ¡nh hai mÃ´ hÃ¬nh lá»“ng nhau - AIC/BIC : TiÃªu chÃ­ lá»±a chá»n mÃ´ hÃ¬nh dá»±a trÃªn likelihood 3. Thá»‘ng KÃª Bayes Trong thá»‘ng kÃª Bayes, likelihood káº¿t há»£p vá»›i phÃ¢n phá»‘i tiÃªn nghiá»‡m prior Ä‘á»ƒ tÃ­nh phÃ¢n phá»‘i háº­u nghiá»‡m posterior : MATH Káº¿t Ná»‘i vá»›i Tá»‘i Æ¯u HÃ³a MLE táº¡o ra má»™t bÃ i toÃ¡n tá»‘i Æ°u hÃ³a: MATH Äiá»u nÃ y thÆ°á»ng dáº«n Ä‘áº¿n: - Tá»‘i Æ°u hÃ³a khÃ´ng rÃ ng buá»™c : Khi khÃ´ng cÃ³ rÃ ng buá»™c trÃªn tham sá»‘ - Tá»‘i Æ°u hÃ³a cÃ³ rÃ ng buá»™c : Khi tham sá»‘ pháº£i thá»a mÃ£n Ä‘iá»u kiá»‡n nháº¥t Ä‘á»‹nh - BÃ i toÃ¡n lá»“i : Nhiá»u hÃ m log-likelihood lÃ  lá»“i, Ä‘áº£m báº£o nghiá»‡m tá»‘i Æ°u toÃ n cá»¥c ðŸ’¡ Káº¿t Ná»‘i vá»›i Tá»‘i Æ¯u HÃ³a Lá»“i: Nhiá»u bÃ i toÃ¡n MLE dáº«n Ä‘áº¿n tá»‘i Æ°u hÃ³a lá»“i, Ä‘áº·c biá»‡t trong há» phÃ¢n phá»‘i mÅ©. Äiá»u nÃ y Ä‘áº£m báº£o ráº±ng nghiá»‡m tÃ¬m Ä‘Æ°á»£c lÃ  tá»‘i Æ°u toÃ n cá»¥c vÃ  cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a sáº½ há»™i tá»¥ Ä‘áº¿n nghiá»‡m Ä‘Ãºng. VÃ­ Dá»¥ Thá»±c Táº¿: Há»“i Quy Logistic Trong há»“i quy logistic, chÃºng ta mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t cá»§a biáº¿n nhá»‹ phÃ¢n: MATH Log-likelihood: MATH ÄÃ¢y lÃ  má»™t bÃ i toÃ¡n tá»‘i Æ°u hÃ³a lá»“i, cÃ³ thá»ƒ giáº£i báº±ng gradient descent hoáº·c Newton's method. TÃ³m Táº¯t - Likelihood Ä‘o lÆ°á»ng má»©c Ä‘á»™ phÃ¹ há»£p cá»§a tham sá»‘ vá»›i dá»¯ liá»‡u quan sÃ¡t - MLE tÃ¬m tham sá»‘ tá»‘i Ä‘a hÃ³a likelihood - MLE cÃ³ nhiá»u tÃ­nh cháº¥t thá»‘ng kÃª tá»‘t: nháº¥t quÃ¡n, hiá»‡u quáº£, tiá»‡m cáº­n chuáº©n - MLE táº¡o ra cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u hÃ³a, nhiá»u trong sá»‘ Ä‘Ã³ lÃ  lá»“i - á»¨ng dá»¥ng rá»™ng rÃ£i trong thá»‘ng kÃª, há»c mÃ¡y vÃ  khoa há»c dá»¯ liá»‡u ðŸŽ¯ Äiá»ƒm Quan Trá»ng: MLE lÃ  cáº§u ná»‘i quan trá»ng giá»¯a thá»‘ng kÃª vÃ  tá»‘i Æ°u hÃ³a. Hiá»ƒu rÃµ MLE giÃºp báº¡n náº¯m vá»¯ng cÃ¡ch cÃ¡c mÃ´ hÃ¬nh thá»‘ng kÃª Ä‘Æ°á»£c Æ°á»›c lÆ°á»£ng thÃ´ng qua cÃ¡c phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a, Ä‘áº·c biá»‡t lÃ  tá»‘i Æ°u hÃ³a lá»“i.",
    "url": "/deep-learning-self-learning/vi/contents/vi/chapter00/00_04_03_Likelihood_and_Maximum_Likelihood_Estimation/",
    "lang": "vi"
  }
]